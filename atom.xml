<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Gai&#39;s Blog</title>
  
  <subtitle>A Zone for Knowledge</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://bluesmilery.github.io/"/>
  <updated>2018-07-14T10:41:45.914Z</updated>
  <id>https://bluesmilery.github.io/</id>
  
  <author>
    <name>Gai</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>腾讯云GPU服务器搭建TensorFlow开发环境</title>
    <link href="https://bluesmilery.github.io/blogs/77f7532c/"/>
    <id>https://bluesmilery.github.io/blogs/77f7532c/</id>
    <published>2018-05-14T08:12:56.000Z</published>
    <updated>2018-07-14T10:41:45.914Z</updated>
    
    <content type="html"><![CDATA[<p>日前，我司开始使用腾讯云的GPU服务器，那自然需要在其上配置TF的开发环境。之前写过在CentOS6上进行源码编译安装TF（<a href="https://bluesmilery.github.io/blogs/9ef0e127/">传送门</a>），所以这篇也是在以前的基础上修改而来，不过因为系统版本换为CentOS7，许多步骤都可以省略了，方便不少。不过仍有部分操作或问题与之前不一致，在此也会对其说明</p><p>最终配置的环境为CUDA-8.0 + cuDNN-6.0 + TensorFlow-1.3.0</p><a id="more"></a><h1 id="总览"><a href="#总览" class="headerlink" title="总览"></a>总览</h1><p>目前总共配置过两种腾讯云GPU服务器，以下内容主要为第一种的安装过程。第二种在此基础上，降低了系统内核版本以便安装显卡驱动</p><p>第一种：CentOS Linux release 7.4.1708 (Core)</p><p>第二种：CentOS Linux release 7.5.1804 (Core)</p><p>目前内核版本均为：3.10.0-693.21.1.el7.x86_64（第二种默认为3.10.0-862.el7.x86_64，已降级）</p><p>配置过程主要分为以下几个部分：</p><ul><li>Python相关</li><li>安装NVIDIA Driver、CUDA、cuDNN</li><li>安装TensorFlow</li></ul><p>因为本次能够通过pip来安装TensorFlow，并且gcc版本符合安装要求，所以省略了之前安装Java8、Bazel，升级gcc的步骤</p><p>但安装过程中仍有部分操作或者问题与之前的不一致，不同之处主要在于：</p><ul><li>编译安装Python时需指定UCS编码方式</li><li>pip需要使用9.0.1版本</li><li>使用yum需要额外修改/usr/libexec/urlgrabber-ext-down文件</li><li>下载显卡驱动时需下载.run格式文件</li></ul><p>之后对安装过程进行简述，所有环境变量添加在/etc/profile.d/path.sh文件中（没有可创建）</p><h1 id="安装过程"><a href="#安装过程" class="headerlink" title="安装过程"></a>安装过程</h1><h2 id="1、Python更新"><a href="#1、Python更新" class="headerlink" title="1、Python更新"></a>1、Python更新</h2><p>通过 <code>python -V</code> 可以看到服务器上python的版本为2.7.5，已经符合TensorFlow安装要求。如果没有特殊需求，不建议再安装更(四声)新的2.7版本，例如2.7.14，因为父版本相同的情况下差别很小，只是一些Bug修复和性能改进，而安装2.7.14所要付出的代价（繁琐程度）是蛮大的，因为父版本相同会产生一些问题。但是如果需要安装Python3的话，那完全没问题</p><p>以下为安装Python2.7.14的过程。Python3类似</p><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><p>安装一些系统依赖</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">yum groupinstall -y <span class="string">'development tools'</span></span><br><span class="line">yum install -y zlib-devel bzip2-devel openssl-devel xz-libs wget</span><br><span class="line">yum install readline* rlwrap</span><br><span class="line">yum install sqlite-devel tk-devel</span><br></pre></td></tr></table></figure><p>下载Python2.7源码<br><a href="https://www.python.org/ftp/python/2.7.14/Python-2.7.14.tar.xz" target="_blank" rel="noopener">https://www.python.org/ftp/python/2.7.14/Python-2.7.14.tar.xz</a></p><p>解压</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xz -d Python-2.7.14.tar.xz</span><br><span class="line">tar -xvf Python-2.7.14.tar</span><br></pre></td></tr></table></figure><p>进入目录<code>cd Python-2.7.14</code>，接下来的操作都在这个目录下进行</p><h3 id="配置编译安装"><a href="#配置编译安装" class="headerlink" title="配置编译安装"></a>配置编译安装</h3><p>配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./configure --prefix=/usr/<span class="built_in">local</span> --<span class="built_in">enable</span>-unicode=ucs4</span><br></pre></td></tr></table></figure><p>关于–enable-unicode=ucs4参数后续会说明其作用</p><p>编译，用时几分钟。然后安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make &amp;&amp; make install</span><br></pre></td></tr></table></figure><h3 id="使新版本生效"><a href="#使新版本生效" class="headerlink" title="使新版本生效"></a>使新版本生效</h3><p>两种方式：将路径加入PATH环境变量、软连接</p><ul><li>加入PATH</li></ul><p>将以下代码加入/etc/profile.d/path.sh文件中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export PATH=/usr/local/bin:$PATH</span><br></pre></td></tr></table></figure><ul><li>软连接</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mv /usr/bin/python /usr/bin/python2.7.5</span><br><span class="line">ln -s /usr/<span class="built_in">local</span>/bin/python2.7 /usr/bin/python</span><br></pre></td></tr></table></figure><p>此时通过 <code>python -V</code> 可以查看Python版本已经为2.7.14。如果还想使用Python2.7.5，那么执行<code>python2.7.5</code>即可</p><h3 id="解决yum失效问题"><a href="#解决yum失效问题" class="headerlink" title="解决yum失效问题"></a>解决yum失效问题</h3><p>因为yum依赖的是原来的Python版本，所以做以下修改</p><p>将/usr/bin/yum以及/usr/libexec/urlgrabber-ext-down的第一行均改为</p><p>#!/usr/bin/python2.7.5</p><p>修改前者是为了升级Python后能够运行yum，修改后者的原因是使用yum安装软件会报以下错误</p><p>ImportError:No module nameed urlgrabber.grabber</p><p>修改后错误消失</p><h3 id="更新pip"><a href="#更新pip" class="headerlink" title="更新pip"></a>更新pip</h3><p>从<a href="https://bootstrap.pypa.io/get-pip.py" target="_blank" rel="noopener">该地址</a>﻿下载get-pip.py文件</p><p>然后执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python get-pip.py</span><br></pre></td></tr></table></figure><p>该文件会安装pip以及setuptools等工具</p><h3 id="一些错误"><a href="#一些错误" class="headerlink" title="一些错误"></a>一些错误</h3><ul><li>pkg_resources.DistributionNotFound: The ‘pip==9.0.1’ distribution was not found and is required by the application</li></ul><p>如果遇到该错误，那么就安装指定的pip版本，下载地址为：<a href="https://github.com/pypa/pip/releases" target="_blank" rel="noopener">https://github.com/pypa/pip/releases</a> ，下载指定的版本。这里下载的是pip-9.0.1版本，然后安装问题解决</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">unzip 9.0.1.zip</span><br><span class="line"><span class="built_in">cd</span> pip-9.0.1</span><br><span class="line">python setup.py install</span><br></pre></td></tr></table></figure><h2 id="2、NVIDIA-Driver"><a href="#2、NVIDIA-Driver" class="headerlink" title="2、NVIDIA Driver"></a>2、NVIDIA Driver</h2><h3 id="准备工作-1"><a href="#准备工作-1" class="headerlink" title="准备工作"></a>准备工作</h3><p>安装一些系统依赖</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install kernel-devel-xxx</span><br></pre></td></tr></table></figure><p>xxx是内核版本号，可以通过 uname -r 查看</p><h3 id="下载驱动程序"><a href="#下载驱动程序" class="headerlink" title="下载驱动程序"></a>下载驱动程序</h3><p>去 <a href="http://www.nvidia.cn/Download/index.aspx" target="_blank" rel="noopener">http://www.nvidia.cn/Download/index.aspx</a> 这里寻找对应的显卡驱动即可，这里选择：</p><ul><li>Product Type: Tesla</li><li>Product Series: M-Series</li><li>Product: M40</li><li>Operating System: Linux 64-bit</li><li>CUDA Toolkit: 8.0</li><li>Language: English(US)</li></ul><p>这里下载的文件名是：NVIDIA-Linux-x86_64-384.66.run</p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>添加可执行权限，安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod +x NVIDIA-Linux-x86_64-384.66.run</span><br><span class="line">./NVIDIA-Linux-x86_64-384.66.run</span><br></pre></td></tr></table></figure><p>进入安装界面后一路同意就可以，是否安装32位的库，我选择的同意。对于最后出现的warning我选择了忽略</p><p>安装完成后执行<code>nvidia-smi</code>后能看到一些显卡信息，若运行错误可重启服务器<code>reboot</code></p><h3 id="一些错误-1"><a href="#一些错误-1" class="headerlink" title="一些错误"></a>一些错误</h3><ul><li>NVIDIA-SMI has failed because it couldn’t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.</li></ul><p>如果安装完驱动后执行<code>nvidia-smi</code>后可能会出现该错误，重启后消失</p><p>如果在下载驱动的时候操作系统选择Linux 64-bit RHEL7，则会下载.rpm格式的安装包，安装后重启仍会出现该错误检测不到显卡驱动。所以操作系统需要选择Linux 64-bit 以下载.run格式安装包</p><ul><li>ERROR: An error occurred while performing the step: “Building kernel modules”. See /var/log/nvidia-installer.log for details.</li></ul><p>这个错误出现的原因之一是系统内核版本过高（或者过低）与驱动程序不匹配。问题发生在给第二种服务器安装时出现的，其系统版本升级为CentOS 7.5，默认内核版本为3.10.0-862.el7.x86_64（对比第一种为3.10.0-693.21.1.el7.x86_64），遂考虑切换内核版本</p><p>使用<code>cat /boot/grub2/grub.cfg | grep menuentry</code>查看目前有哪些内核版本，使用<code>grub2-set-default</code>命令切换，使用<code>grub2-editenv list</code>验证是否配置成功。在这里需要设置的内核版本为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grub2-set-default <span class="string">"CentOS Linux (3.10.0-693.21.1.el7.x86_64) 7 (Core)"</span></span><br></pre></td></tr></table></figure><p>如果切换完成后仍报该错误可以重启服务器</p><ul><li>WARNING: nvidia-installer was forced to guess the X library path ‘/usr/lib64’ and X module path ‘/usr/lib64/xorg/modules’; these paths were not queryable from the system. If X fails to find the NVIDIA X driver module, please install the pkg-config utility and the X.Org SDK/development package for your distribution and reinstall the driver.</li></ul><p>这是安装完后出现的warning，目前没发现有什么问题</p><h2 id="3、CUDA-8-0-amp-cuDNN-6-0"><a href="#3、CUDA-8-0-amp-cuDNN-6-0" class="headerlink" title="3、CUDA-8.0 &amp; cuDNN-6.0"></a>3、CUDA-8.0 &amp; cuDNN-6.0</h2><h3 id="CUDA下载"><a href="#CUDA下载" class="headerlink" title="CUDA下载"></a>CUDA下载</h3><p>去 <a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-downloads</a> 这里寻找对应平台的文件下载即可。这里有一份详尽官方的 <a href="http://docs.nvidia.com/cuda/cuda-installation-guide-linux/" target="_blank" rel="noopener">说明文档</a></p><p>这里一些选项的选择为：</p><ul><li>Operating System: Linux</li><li>Architecture: x86_64</li><li>Distribution: CentOS</li><li>Version: 7</li><li>Installer Type: runfile(local)</li></ul><p>下面会显示两个安装文件，一个 Base Installer ，一个Patch。安装完Base后再安装Patch即可</p><h3 id="CUDA安装"><a href="#CUDA安装" class="headerlink" title="CUDA安装"></a>CUDA安装</h3><p>添加可执行权限，安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod +x cuda_8.0.61_375.26_linux.run</span><br><span class="line">./cuda_8.0.61_375.26_linux.run</span><br></pre></td></tr></table></figure><p>接下来会有一系列提示需要确认，其中在询问是否要安装显卡驱动时选 n ，因为我们之前已经安装了最新版本的驱动。其他的一路同意即可</p><p>最后再打一下补丁即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod +x cuda_8.0.61.2_linux.run</span><br><span class="line">./cuda_8.0.61.2_linux.run</span><br></pre></td></tr></table></figure><h3 id="CUDA测试"><a href="#CUDA测试" class="headerlink" title="CUDA测试"></a>CUDA测试</h3><p>进入samples目录，选择第一个例子进行测试</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/cuda/samples/1_Utilities/deviceQuery</span><br><span class="line">make</span><br></pre></td></tr></table></figure><p>编译完成后执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./deviceQuery</span><br></pre></td></tr></table></figure><p>会看到一系列显卡参数信息，只要最后显示 Result = PASS 即说明CUDA安装成功</p><h3 id="添加环境变量"><a href="#添加环境变量" class="headerlink" title="添加环境变量"></a>添加环境变量</h3><p>编辑 /etc/profile.d/path.sh 文件，添加以下内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># cuda-8.0</span><br><span class="line">export CUDA_HOME=/usr/local/cuda</span><br><span class="line">export PATH=/usr/local/cuda-8.0/bin:$PATH</span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64:$LD_LIBRARY_PATH</span><br></pre></td></tr></table></figure><h3 id="cuDNN下载"><a href="#cuDNN下载" class="headerlink" title="cuDNN下载"></a>cuDNN下载</h3><p>去 <a href="https://developer.nvidia.com/rdp/cudnn-download" target="_blank" rel="noopener">https://developer.nvidia.com/rdp/cudnn-download</a> 选择对应的版本下载即可。不过需要先注册开发者账号后才可以下载</p><p>之前安装的CUDA是8.0，所以我们选择</p><ul><li>Download cuDNN v6.0 (April 27, 2017), for CUDA 8.0</li><li>cuDNN v6.0 Library for Linux</li></ul><h3 id="cuDNN安装"><a href="#cuDNN安装" class="headerlink" title="cuDNN安装"></a>cuDNN安装</h3><p>执行解压操作</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf cudnn-8.0-linux-x64-v6.0.tgz</span><br></pre></td></tr></table></figure><p>解压后的文件夹是cuda。执行以下操作把文件复制到相应的位置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cp cuda/include/cudnn.h /usr/<span class="built_in">local</span>/cuda/include/</span><br><span class="line">cp cuda/lib64/libcudnn* /usr/<span class="built_in">local</span>/cuda/lib64/</span><br><span class="line">chmod a+r /usr/<span class="built_in">local</span>/cuda/include/cudnn.h</span><br><span class="line">chmod a+r /usr/<span class="built_in">local</span>/cuda/lib64/libcudnn*</span><br></pre></td></tr></table></figure><h3 id="一些错误-2"><a href="#一些错误-2" class="headerlink" title="一些错误"></a>一些错误</h3><ul><li>g++: No such file or directory</li></ul><p>CUDA编译上述例子时报以上错误，这是因为没有安装g++，使用命令 <code>yum install gcc-c++</code> 来安装g++即可解决</p><h2 id="4、TensorFlow-1-3-0"><a href="#4、TensorFlow-1-3-0" class="headerlink" title="4、TensorFlow-1.3.0"></a>4、TensorFlow-1.3.0</h2><h3 id="准备工作-2"><a href="#准备工作-2" class="headerlink" title="准备工作"></a>准备工作</h3><p>安装一些系统依赖</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install python-devel</span><br></pre></td></tr></table></figure><h3 id="pip安装TensorFlow"><a href="#pip安装TensorFlow" class="headerlink" title="pip安装TensorFlow"></a>pip安装TensorFlow</h3><p>有两种方法</p><p>a）通过pip安装指定版本（1.3.0），注意要安装GPU版本</p><p>pip install tensorflow-gpu==1.3.0</p><p>b）若提示找不到对应版本（第一天安装是这样，第二天测试了下第一种方法又好使了），可执行以下命令</p><p>pip install –upgrade tfBinaryURL</p><p>其中，tfBinaryURL替换为<a href="https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.3.0-cp27-none-linux_x86_64.whl" target="_blank" rel="noopener">https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.3.0-cp27-none-linux_x86_64.whl</a></p><p>如果不指定版本的话那默认安装的是最新版本的，而最新版本的TensorFlow已经不支持CUDA8.0了，可以去<a href="https://www.tensorflow.org/install/install_sources" target="_blank" rel="noopener">https://www.tensorflow.org/install/install_sources</a> 查看TF与CUDA版本的对应关系，在最下面。如果该链接打不开（被墙），可以访问<a href="https://tensorflow.google.cn/install/install_sources" target="_blank" rel="noopener">https://tensorflow.google.cn/install/install_sources</a> 。这是TF给中国大陆单独开的域名</p><p>安装其他版本CUDA与上述步骤类似</p><h3 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h3><p>进入python，进行 <code>import tensorflow</code>，没有错误那就代表安装成功</p><h3 id="一些错误-3"><a href="#一些错误-3" class="headerlink" title="一些错误"></a>一些错误</h3><ul><li>ImportError: /usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: PyUnicodeUCS4_AsUTF8String</li></ul><p>出现这个错误的原因是Python和某个你用的库编译时指定的UCS编码方式不对。编译Python时，可以通过指定–enable-unicode=ucs2或者ucs4来选择使用UCS2或者UCS4</p><p>如果你的错误是undefined symbol: PyUnicodeUCS2_AsUTF8String，说明你的Python编译时使用的是UCS4，反之亦然</p><p>解决方案就是重新编译Python或者重新编译库，但这里我们使用的是pip安装库（不想编译TensorFlow），所以选择重新编译Python</p><p>因为报错是PyUnicodeUCS4_AsUTF8String，说明TensorFlow是用UCS4编译的，而Python是UCS2编译的，所以重新编译Python时设置unicode为ucs4</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./configure --prefix=/usr/<span class="built_in">local</span> --<span class="built_in">enable</span>-unicode=ucs4</span><br></pre></td></tr></table></figure><ul><li>ImportError: Importing the multiarray numpy extension module failed. Most likely you are trying to import a failed build of numpy. If you’re working with a numpy git repo, try <code>git clean -xdf</code> (removes all files not under version control). Otherwise reinstall numpy. </li></ul><p>这是由于之前的旧版本Python安装的numpy与新编译的Python不兼容的缘故，卸载numpy重新安装即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip uninstall numpy</span><br><span class="line">pip install numpy</span><br></pre></td></tr></table></figure><ul><li>Cannot uninstall ‘Markdown’. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.</li></ul><p>如果先把pip从9升级到10后再安装TensorFlow会出现该问题。该问题的具体解释可见<a href="https://github.com/pypa/pip/issues/5247" target="_blank" rel="noopener">链接</a>。解决的方法为手动删除该包相关文件，在这里主要删除了一个文件夹以及一个info文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rm -rf /usr/lib/python2.7/site-packages/markdown</span><br><span class="line">rm Markdown-2.4.1-py2.7.egg-info</span><br></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>官方安装指南</p><ul><li><a href="https://www.tensorflow.org/install/install_sources" target="_blank" rel="noopener">https://www.tensorflow.org/install/install_sources</a></li><li><a href="https://www.tensorflow.org/install/install_linux" target="_blank" rel="noopener">https://www.tensorflow.org/install/install_linux</a></li></ul><p>腾讯云官方文档</p><ul><li><p>安装 NVIDIA 驱动指引<a href="https://cloud.tencent.com/document/product/560/8048" target="_blank" rel="noopener">https://cloud.tencent.com/document/product/560/8048</a></p></li><li><p>安装 CUDA 驱动指引<a href="https://cloud.tencent.com/document/product/560/8064" target="_blank" rel="noopener">https://cloud.tencent.com/document/product/560/8064</a></p></li></ul><p>参考的Blog</p><ul><li><a href="https://blog.csdn.net/qq708986022/article/details/77896791" target="_blank" rel="noopener">https://blog.csdn.net/qq708986022/article/details/77896791</a></li><li><a href="https://www.jianshu.com/p/b0b8ab2be12c" target="_blank" rel="noopener">https://www.jianshu.com/p/b0b8ab2be12c</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;日前，我司开始使用腾讯云的GPU服务器，那自然需要在其上配置TF的开发环境。之前写过在CentOS6上进行源码编译安装TF（&lt;a href=&quot;https://bluesmilery.github.io/blogs/9ef0e127/&quot;&gt;传送门&lt;/a&gt;），所以这篇也是在以前的基础上修改而来，不过因为系统版本换为CentOS7，许多步骤都可以省略了，方便不少。不过仍有部分操作或问题与之前不一致，在此也会对其说明&lt;/p&gt;
&lt;p&gt;最终配置的环境为CUDA-8.0 + cuDNN-6.0 + TensorFlow-1.3.0&lt;/p&gt;
    
    </summary>
    
      <category term="技术" scheme="https://bluesmilery.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="CentOS" scheme="https://bluesmilery.github.io/tags/CentOS/"/>
    
      <category term="TensorFlow" scheme="https://bluesmilery.github.io/tags/TensorFlow/"/>
    
      <category term="腾讯云" scheme="https://bluesmilery.github.io/tags/%E8%85%BE%E8%AE%AF%E4%BA%91/"/>
    
  </entry>
  
  <entry>
    <title>CentOS6源码编译安装TensorFlow</title>
    <link href="https://bluesmilery.github.io/blogs/9ef0e127/"/>
    <id>https://bluesmilery.github.io/blogs/9ef0e127/</id>
    <published>2017-09-30T12:59:17.000Z</published>
    <updated>2018-01-31T03:45:37.756Z</updated>
    
    <content type="html"><![CDATA[<p>文章首发于微信公众号：链家产品技术团队，欢迎搜索关注~</p><p>公司前一阵搞了两台GPU服务器，终于有“玩具”可以玩了～用的卡是最新的P100（好吧，真正最新的是V100，不过还没铺货）。本着爱折腾的精神，自然就开始了折腾它们的征程（结果是我被折腾了。。。）</p><p>以前自己也搭建过TensorFlow的开发环境（<a href="https://bluesmilery.github.io/blogs/9a018dfc/">见链接</a>），所以一开始以为这次也不会难，结果。。。咳咳，还是要正视自己的水平的。因为服务器上装的是系统是CentOS，以前自己捣鼓的时候用的是Ubuntu，差别还是不少的，所以特此记录自己踩过的坑，也给其他人一些经验帮助。</p><p>这次折腾总共分为两个阶段：最初打算直接通过pip下载TensorFlow安装包安装，结果完全失败，不过在此期间摸清了许多限制条件，也为第二个阶段——通过编译的方式安装TensorFlow打下了基础。</p><p>系统版本：CentOS release 6.8 (Final)    64位</p><a id="more"></a><h1 id="追根溯源"><a href="#追根溯源" class="headerlink" title="追根溯源"></a>追根溯源</h1><p>首先，目标是安装TensorFlow，所以查阅TensorFlow官网的源码安装教程</p><p>发现需要使用Bazel来进行编译TensorFlow源码，进而生成pip安装包，通过pip来安装</p><p>那下一步就是安装Bazel。去Bazel官网查看，发现主要提供了Ubuntu、MacOS、Windows的安装方式，对于其他平台来说，需要通过源码编译的方式来安装。所需要的环境条件主要是JDK8、Python、跟C相关的编译工具等</p><p>CentOS 6.8自带的gcc版本为4.4.7，不支持C++11，而TensorFlow和Bazel的编译需要C++11的支持，所以要将gcc升级为支持C++11的版本，经过网上查找，gcc版本跨度不是很大的情况下，可以使用低版本的来编译安装高版本的gcc。gcc4.9.4可以通过4.4.7编译安装</p><p>CentOS 6.8自带的Python版本为2.6.6，而TensorFlow和Bazel的编译所需要的Python版本最低为2.7</p><p>因为要使用GPU，所以关于TensorFlow部分还需要一些额外的条件，在编译之前需要具备显卡驱动、Cuda Toolkit以及cuDNN</p><p>所以整个安装流程如下：</p><p>Java8、gcc4.9.4、Python2.7、pip、Bazel、NVIDIA Driver、CUDA、cuDNN、TensorFlow</p><h1 id="安装过程"><a href="#安装过程" class="headerlink" title="安装过程"></a>安装过程</h1><h2 id="1、Java-8"><a href="#1、Java-8" class="headerlink" title="1、Java-8"></a>1、Java-8</h2><p>去Oracle官网下载最新的Java8版本即可（8u144）</p><p><a href="http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html" target="_blank" rel="noopener">http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html</a></p><p>下载.tar.gz格式的压缩包后，解压缩，将bin目录所在的路径添加到PATH当中即可</p><h2 id="2、gcc-4-9-4"><a href="#2、gcc-4-9-4" class="headerlink" title="2、gcc-4.9.4"></a>2、gcc-4.9.4</h2><h3 id="2-1-准备工作"><a href="#2-1-准备工作" class="headerlink" title="2.1 准备工作"></a>2.1 准备工作</h3><p>先检查系统中是否安装了g++，如果没有的话在编译安装gcc时会报错</p><blockquote><p>make[1]: *** [stage1-bubble] Error 2</p></blockquote><p>使用yum安装g++</p><p><code>yum install gcc-c++</code></p><h3 id="2-2-下载gcc源码"><a href="#2-2-下载gcc源码" class="headerlink" title="2.2 下载gcc源码"></a>2.2 下载gcc源码</h3><p><a href="http://ftp.gnu.org/gnu/gcc/gcc-4.9.4/gcc-4.9.4.tar.bz2" target="_blank" rel="noopener">http://ftp.gnu.org/gnu/gcc/gcc-4.9.4/gcc-4.9.4.tar.bz2</a></p><p>解压</p><p><code>tar -jxvf gcc-4.9.4.tar.bz2</code></p><p>进入目录，接下来的操作都在这个目录下进行</p><p><code>cd gcc-4.9.4</code></p><h3 id="2-3-下载所需要的依赖库"><a href="#2-3-下载所需要的依赖库" class="headerlink" title="2.3 下载所需要的依赖库"></a>2.3 下载所需要的依赖库</h3><p>正常只需要执行</p><p><code>./contrib/download_prerequisites</code></p><p>但是因为服务器无法访问外网（后来就算可以访问了，但是连接依旧有限，比如这步自动下载一些依赖包的操作，就无法正常进行），所以我们采用先把所需要的依赖包下载下来后，放到所需要的目录（后来发现是ftp服务器无法请求= =）</p><p>查看下载操作的脚本</p><p><code>vim contrib/download_prerequisites</code></p><p>可以发现脚本通过wget下载了五个依赖包，分别是</p><ul><li><a href="http://ftp.gnu.org/gnu/mpfr/mpfr-2.4.2.tar.bz2" target="_blank" rel="noopener">mpfr-2.4.2</a></li><li><a href="http://ftp.gnu.org/gnu/gmp/gmp-4.3.2.tar.bz2" target="_blank" rel="noopener">gmp-4.3.2</a></li><li><a href="http://www.multiprecision.org/mpc/download/mpc-0.8.1.tar.gz" target="_blank" rel="noopener">mpc-0.8.1</a></li><li><a href="http://isl.gforge.inria.fr/isl-0.12.2.tar.bz2" target="_blank" rel="noopener">isl-0.12.2</a></li><li><a href="https://www.bastoul.net/cloog/pages/download/cloog-0.18.1.tar.gz" target="_blank" rel="noopener">cloog-0.18.1</a></li></ul><p>也可以去 <a href="ftp://gcc.gnu.org/pub/gcc/infrastructure/" target="_blank" rel="noopener">ftp://gcc.gnu.org/pub/gcc/infrastructure/</a> 下载</p><p>下载完成后将它们放到gcc-4.9.4目录下即可，然后再执行</p><p><code>./contrib/download_prerequisites</code></p><h3 id="2-4-配置编译安装"><a href="#2-4-配置编译安装" class="headerlink" title="2.4 配置编译安装"></a>2.4 配置编译安装</h3><p>官方建议新建一个目录用于编译，所以</p><p><code>mkdir build</code></p><p><code>cd build</code></p><p>配置</p><p><code>../gcc-4.9.4/configure --prefix=/opt/gcc-4.9.4/ --enable-checking=release --enable-languages=c,c++ --disable-multilib</code></p><p>关于具体的参数设置可以参照 <a href="https://gcc.gnu.org/install/configure.html" target="_blank" rel="noopener">https://gcc.gnu.org/install/configure.html</a></p><p>编译</p><p><code>make -j4</code></p><p>-j4指的是使用四个线程，服务器的CPU大约使用了不到20分钟。不过有人不建议使用多线程编译，说是可能会失败。</p><p>安装</p><p><code>make install</code></p><p>添加路径，打开 .bashrc，添加如下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">export PATH=/opt/gcc-4.9.4/bin:$PATH  </span><br><span class="line">export CXX=/opt/gcc-4.9.4/bin/c++  </span><br><span class="line">export CC=/opt/gcc-4.9.4/bin/gcc  </span><br><span class="line">export LDFLAGS=&quot;-L/opt/gcc-4.9.4/lib -L/opt/gcc-4.9.4/lib64&quot;  </span><br><span class="line">export CXXFLAGS=&quot;-L/opt/gcc-4.9.4/lib -L/opt/gcc-4.9.4/lib64&quot;  </span><br><span class="line">export C_INCLUDE_PATH=/opt/gcc-4.9.4/include  </span><br><span class="line">export CXX_INCLUDE_PATH=$C_INCLUDE_PATH  </span><br><span class="line">export LD_RUN_PATH=/opt/gcc-4.9.4/lib/:/opt/gcc-4.9.4/lib64/  </span><br><span class="line">export LD_LIBRARY_PATH=/opt/gcc-4.9.4/lib/:/opt/gcc-4.9.4/lib64/:$LD_LIBRARY_PATH</span><br></pre></td></tr></table></figure><p>一般添加第一行就可以，但是在Bazel的编译过程中会出现一些错误，加上后面的部分可以解决这些错误</p><p>最后在source一下 .bashrc 就可以了～</p><p>通过 <code>gcc -v</code> 和 <code>g++ -v</code> 可以看到版本已经变成了4.9.4：gcc version 4.9.4 (GCC)</p><h2 id="3、Python-2-7"><a href="#3、Python-2-7" class="headerlink" title="3、Python-2.7"></a>3、Python-2.7</h2><p>Python也需要通过编译源码的方式安装，使用刚才升级过的gcc来编译。之所以这样做是因为可以在后续操作中避免一些错误</p><h3 id="3-1-准备工作"><a href="#3-1-准备工作" class="headerlink" title="3.1 准备工作"></a>3.1 准备工作</h3><p>安装一些系统依赖</p><p><code>yum groupinstall -y &#39;development tools&#39;</code><br><code>yum install -y zlib-devel bzip2-devel openssl-devel xz-libs wget</code></p><h3 id="3-2-下载Python2-7源码"><a href="#3-2-下载Python2-7源码" class="headerlink" title="3.2 下载Python2.7源码"></a>3.2 下载Python2.7源码</h3><p><a href="https://www.python.org/ftp/python/2.7.14/Python-2.7.14.tar.xz" target="_blank" rel="noopener">https://www.python.org/ftp/python/2.7.14/Python-2.7.14.tar.xz</a></p><p>解压</p><p><code>xz -d Python-2.7.14.tar.xz</code><br><code>tar -xvf Python-2.7.14.tar</code></p><p>进入目录，接下来的操作都在这个目录下进行</p><p><code>cd Python-2.7.14</code></p><h3 id="3-3-配置编译安装"><a href="#3-3-配置编译安装" class="headerlink" title="3.3 配置编译安装"></a>3.3 配置编译安装</h3><p>配置</p><p><code>./configure --prefix=/usr/local</code></p><p>编译，用时几分钟</p><p><code>make</code></p><p>安装</p><p><code>make install</code></p><h3 id="3-4-使新版本生效"><a href="#3-4-使新版本生效" class="headerlink" title="3.4 使新版本生效"></a>3.4 使新版本生效</h3><p>两种方式：加入PATH、软连接</p><p>加入PATH</p><p><code>export PATH=/usr/local/bin:$PATH</code></p><p>软连接</p><p><code>mv /usr/bin/python /usr/bin/python2.6</code><br><code>ln -s /usr/local/bin/python2.7 /usr/bin/python</code></p><p>此时通过 <code>python -V</code> 可以查看Python版本已经为2.7</p><h3 id="3-5-解决yum失效问题"><a href="#3-5-解决yum失效问题" class="headerlink" title="3.5 解决yum失效问题"></a>3.5 解决yum失效问题</h3><p>因为yum依赖的是原来Python2.6版本，所以做以下修改</p><p><code>vim /usr/bin/yum</code></p><p>将第一行 <code>#!/usr/bin/python</code> 改为 <code>#!/usr/bin/python2.6</code></p><h3 id="3-6-更新setuptools和pip"><a href="#3-6-更新setuptools和pip" class="headerlink" title="3.6 更新setuptools和pip"></a>3.6 更新setuptools和pip</h3><p>分别下载 <a href="https://pypi.python.org/pypi/setuptools" target="_blank" rel="noopener">setuptools</a> 和 <a href="https://pypi.python.org/pypi/pip" target="_blank" rel="noopener">pip</a> 的安装包</p><p>解压后进到相应的目录，执行</p><p><code>python setup.py install</code></p><p>此时通过 <code>pip -V</code> 可以查看pip的版本，已经是对应于Python2.7的了</p><p>顺手更新pip源，毕竟还是国内的快</p><p><code>vim ~/.pip/pip.conf</code></p><p>然后写入如下内容并保存</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[global]  </span><br><span class="line">trusted-host = mirrors.aliyun.com  </span><br><span class="line">index-url = http://mirrors.aliyun.com/pypi/simple</span><br></pre></td></tr></table></figure><h2 id="4、Bazel-0-5-3"><a href="#4、Bazel-0-5-3" class="headerlink" title="4、Bazel-0.5.3"></a>4、Bazel-0.5.3</h2><p>接下来是编译安装Bazel，主要参照 <a href="https://docs.bazel.build/versions/master/install-compile-source.html" target="_blank" rel="noopener">官方教程</a> 即可</p><h3 id="4-1-下载Bazel源码"><a href="#4-1-下载Bazel源码" class="headerlink" title="4.1 下载Bazel源码"></a>4.1 下载Bazel源码</h3><p>去GitHub上下载 bazel-\&lt;VERSION>-dist.zip 格式的源码，下载最新版或者特定版本均可。此处下载的是0.5.3版本（最新的为0.5.4）</p><p><a href="https://github.com/bazelbuild/bazel/releases/download/0.5.3/bazel-0.5.3-dist.zip" target="_blank" rel="noopener">https://github.com/bazelbuild/bazel/releases/download/0.5.3/bazel-0.5.3-dist.zip</a></p><p>解压，最好指定目录，因为Bazel所有文件都放在根目录</p><p><code>unzip bazel-0.5.3-dist.zip -d bazel-0.5.3</code></p><p>进入目录，接下来的操作都在这个目录下进行</p><p><code>cd bazel-0.5.3</code></p><h3 id="4-2-编译"><a href="#4-2-编译" class="headerlink" title="4.2 编译"></a>4.2 编译</h3><p>运行</p><p><code>./compile.sh</code></p><p>编译出的结果放在 <code>output/bazel</code> 当中，将其复制到PATH路径下即可</p><p><code>cp output/bazel /usr/local/bin</code></p><p>或者</p><p><code>cp output/bazel /usr/bin</code></p><p>可以执行 <code>bazel</code> 验证是否安装成功</p><h3 id="4-3-一些错误"><a href="#4-3-一些错误" class="headerlink" title="4.3 一些错误"></a>4.3 一些错误</h3><ul><li>第一个</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ERROR: /root/gg/bazel/third_party/BUILD:106:1: Extracting interface //third_party:apache_commons_collections failed (Exit 1): ijar failed: error executing command</span><br></pre></td></tr></table></figure><p>参照这个 <a href="https://github.com/bazelbuild/bazel/issues/760" target="_blank" rel="noopener">issue</a> 解决：</p><p>升级gcc后，需要添加CXX, CC, LDFLAGS and CXXFLAGS等环境变量</p><ul><li>第二个 </li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Executing genrule //src:embedded_tools failed (Exit 1): bash failed: error executing command</span><br></pre></td></tr></table></figure><p>参照这两个 <a href="https://github.com/bazelbuild/bazel/issues/2738" target="_blank" rel="noopener">issue</a>、<a href="https://github.com/bazelbuild/bazel/issues/673" target="_blank" rel="noopener">issue</a> 解决：</p><p><code>mkdir /root/tmp</code><br><code>export TMPDIR=/root/tmp</code> </p><ul><li>第三个</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">AttributeError: ZipFile instance has no attribute &apos;__exit__&apos;</span><br></pre></td></tr></table></figure><p>当初升级gcc后直接编译Bazel报的错误，经查找是Python版本的问题（当时Python还是2.6），使用gcc4.9.4编译安装Python2.7后问题解决</p><h2 id="5、NVIDIA-Driver"><a href="#5、NVIDIA-Driver" class="headerlink" title="5、NVIDIA Driver"></a>5、NVIDIA Driver</h2><h3 id="5-1-准备工作"><a href="#5-1-准备工作" class="headerlink" title="5.1 准备工作"></a>5.1 准备工作</h3><p>安装一些系统依赖</p><p><code>yum install kernel-source</code></p><h3 id="5-2-下载驱动程序"><a href="#5-2-下载驱动程序" class="headerlink" title="5.2 下载驱动程序"></a>5.2 下载驱动程序</h3><p>去 <a href="http://www.nvidia.cn/Download/index.aspx" target="_blank" rel="noopener">http://www.nvidia.cn/Download/index.aspx</a> 这里寻找对应的显卡驱动即可，这里选择：</p><ul><li>Product Type: Tesla</li><li>Product Series: P-Series</li><li>Product: Tesla P100</li><li>Operating System: Linux 64-bit</li><li>CUDA Toolkit: 8.0</li><li>Language: English(US)</li></ul><p>这里下载的文件名是：NVIDIA-Linux-x86_64-384.66.run</p><h3 id="5-3-安装"><a href="#5-3-安装" class="headerlink" title="5.3 安装"></a>5.3 安装</h3><p>添加可执行权限，安装</p><p><code>chmod +x NVIDIA-Linux-x86_64-384.66.run</code><br><code>./NVIDIA-Linux-x86_64-384.66.run</code></p><p>进入安装界面后一路同意就可以，是否安装32位的库，我选择的同意。对于最后出现的warning我选择了忽略</p><p>安装完成后需要重启服务器，驱动才会生效</p><p><code>reboot</code></p><h3 id="5-4-验证"><a href="#5-4-验证" class="headerlink" title="5.4 验证"></a>5.4 验证</h3><p>执行</p><p><code>nvidia-smi</code></p><p>后可以看到一些显卡信息，包括驱动版本、风扇转速、温度、显卡型号、已用功率/总功率、已用显存/总显存、GPU计算力利用率等</p><h3 id="5-5-一些错误"><a href="#5-5-一些错误" class="headerlink" title="5.5 一些错误"></a>5.5 一些错误</h3><ul><li>第一个</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ERROR: Unable to find the kernel source tree for the currently running kernel. Please make sure you have installed the kernel source files for your kernel and that they are properly configured; on Red Hat Linux systems, for example, be sure you have the &apos;kernel-source&apos; or &apos;kernel-devel&apos; RPM installed. If you know the correct kernel source files are installed, you may specify the kernel source path with the &apos;--kernel-source-path&apos; command line option.</span><br></pre></td></tr></table></figure><p>如果没做最一开始的准备工作，可能会出现这个错误</p><ul><li>第二个</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">WARNING: nvidia-installer was forced to guess the X library path &apos;/usr/lib64&apos; and X module path &apos;/usr/lib64/xorg/modules&apos;; these paths were not queryable from the system.  If X fails to find the NVIDIA X driver module, please install the `pkg-config` utility and the X.Org SDK/development package for your distribution and reinstall the driver.</span><br></pre></td></tr></table></figure><p>安装完成后的warning，目前没发现有什么问题</p><h2 id="6、CUDA-8-0"><a href="#6、CUDA-8-0" class="headerlink" title="6、CUDA-8.0"></a>6、CUDA-8.0</h2><p>CUDA是英伟达开发的一款针对于使用GPU来加速计算的工具包，所以机器学习或者深度学习想要使用GPU来加速计算的话，就必须使用CUDA</p><h3 id="6-1-下载"><a href="#6-1-下载" class="headerlink" title="6.1 下载"></a>6.1 下载</h3><p>去 <a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-downloads</a> 这里寻找对应平台的文件下载即可。这里有一份详尽官方的 <a href="http://docs.nvidia.com/cuda/cuda-installation-guide-linux/" target="_blank" rel="noopener">说明文档</a></p><p>目前CUDA已经出了9.0，但是TenforFlow官方推荐的是8.0，所以我们安装的还是8.0版本</p><p>这里一些选项的选择为：</p><ul><li>Operating System: Linux</li><li>Architecture: x86_64</li><li>Distribution: CentOS</li><li>Version: 6</li><li>Installer Type: runfile(local)</li></ul><p>下面会显示两个安装文件，一个 Base Installer ，一个Patch。安装完Base后再安装Patch即可</p><h3 id="6-2-安装"><a href="#6-2-安装" class="headerlink" title="6.2 安装"></a>6.2 安装</h3><p>添加可执行权限，安装</p><p><code>chmod +x cuda_8.0.61_375.26_linux.run</code><br><code>./cuda_8.0.61_375.26_linux.run</code></p><p>接下来会有一系列提示需要确认，其中在询问是否要安装显卡驱动时选 n ，因为我们之前已经安装了最新版本的驱动。其他的一路同意即可</p><p>安装完成后会出现以下内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">Installing the CUDA Toolkit in /usr/local/cuda-8.0 ...  </span><br><span class="line">Missing recommended library: libGLU.so  </span><br><span class="line">Missing recommended library: libX11.so  </span><br><span class="line">Missing recommended library: libXi.so  </span><br><span class="line">Missing recommended library: libXmu.so  </span><br><span class="line"></span><br><span class="line">Installing the CUDA Samples in /home/gaixindong ...  </span><br><span class="line">Copying samples to /home/gaixindong/NVIDIA_CUDA-8.0_Samples now...  </span><br><span class="line">Finished copying samples.  </span><br><span class="line"></span><br><span class="line">= Summary =   </span><br><span class="line"></span><br><span class="line">Driver:   Not Selected  </span><br><span class="line">Toolkit:  Installed in /usr/local/cuda-8.0  </span><br><span class="line">Samples:  Installed in /home/gaixindong, but missing recommended libraries  </span><br><span class="line"></span><br><span class="line">Please make sure that  </span><br><span class="line">-   PATH includes /usr/local/cuda-8.0/bin  </span><br><span class="line">-   LD_LIBRARY_PATH includes /usr/local/cuda-8.0/lib64, or, add /usr/local/cuda-8.0/lib64 to /etc ld.so.conf and run ldconfig as root  </span><br><span class="line"></span><br><span class="line">To uninstall the CUDA Toolkit, run the uninstall script in /usr/local/cuda-8.0/bin  </span><br><span class="line"></span><br><span class="line">Please see CUDA_Installation_Guide_Linux.pdf in /usr/local/cuda-8.0/doc/pdf for detailed information on setting up CUDA.  </span><br><span class="line"></span><br><span class="line">***WARNING: Incomplete installation! This installation did not install the CUDA Driver. A driver of version at least 361.00 is required for CUDA 8.0 functionality to work.  </span><br><span class="line">To install the driver using this installer, run the following command, replacing &lt;CudaInstaller&gt; with the name of this run file:  </span><br><span class="line">    sudo &lt;CudaInstaller&gt;.run -silent -driver  </span><br><span class="line"></span><br><span class="line">Logfile is /tmp/cuda_install_132117.log</span><br></pre></td></tr></table></figure><p>根据以上提示内容发现，缺少了一些推荐的库，但是这些库可能是跟运行它提供的samples有关，所以现在并没有安装。日后如果有问题，可以依照这些提示去安装一下</p><p>并且samples也可以选择不安装，因为在cuda的目录下有一份samples</p><p>最后再打一下补丁即可</p><p><code>chmod +x cuda_8.0.61.2_linux.run</code><br><code>./cuda_8.0.61.2_linux.run</code></p><h3 id="6-3-测试"><a href="#6-3-测试" class="headerlink" title="6.3 测试"></a>6.3 测试</h3><p>进入samples目录，选择第一个例子进行测试</p><p><code>cd /usr/local/cuda/samples/1_Utilities/deviceQuery</code><br><code>make</code></p><p>编译完成后执行</p><p><code>./deviceQuery</code></p><p>会看到一系列显卡参数信息，只要最后显示 <code>Result = PASS</code> 即说明CUDA安装成功</p><h2 id="7、cuDNN-6-0"><a href="#7、cuDNN-6-0" class="headerlink" title="7、cuDNN-6.0"></a>7、cuDNN-6.0</h2><p>在CUDA之外，还有个库叫做cuDNN(CUDA Deep Neural Network library)，是专门给深度神经网络针对GPU调优的，也是TensorFlow官方要求必须安装的</p><h3 id="7-1-下载"><a href="#7-1-下载" class="headerlink" title="7.1 下载"></a>7.1 下载</h3><p>去 <a href="https://developer.nvidia.com/rdp/cudnn-download" target="_blank" rel="noopener">https://developer.nvidia.com/rdp/cudnn-download</a> 选择对应的版本下载即可。不过需要先注册开发者账号后才可以下载</p><p>目前cuDNN最新版本是7.0，因为担心兼容性问题所以没有选择最新版本。因为之前安装的CUDA是8.0，所以我们选择</p><p>Download cuDNN v6.0 (April 27, 2017), for CUDA 8.0</p><p>cuDNN v6.0 Library for Linux</p><h3 id="7-2-安装"><a href="#7-2-安装" class="headerlink" title="7.2 安装"></a>7.2 安装</h3><p>执行解压操作</p><p><code>tar -zxvf cudnn-8.0-linux-x64-v6.0-tgz</code></p><p>解压后的文件夹是cuda。执行以下操作把文件复制到相应的位置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cp cuda/include/cudnn.h /usr/local/cuda/include/  </span><br><span class="line">cp cuda/lib64/libcudnn* /usr/local/cuda/lib64/  </span><br><span class="line">chmod a+r /usr/local/cuda/include/cudnn.h  </span><br><span class="line">chmod a+r /usr/local/cuda/lib64/libcudnn*</span><br></pre></td></tr></table></figure><h2 id="8、TensorFlow-1-3-0"><a href="#8、TensorFlow-1-3-0" class="headerlink" title="8、TensorFlow-1.3.0"></a>8、TensorFlow-1.3.0</h2><p>终于到最后一步了，整个过程主要参照 <a href="https://www.tensorflow.org/install/install_sources" target="_blank" rel="noopener">官方教程</a></p><h3 id="8-1-准备工作"><a href="#8-1-准备工作" class="headerlink" title="8.1 准备工作"></a>8.1 准备工作</h3><p>安装一些系统依赖</p><p><code>yum install python-devel</code><br><code>pip install numpy wheel</code></p><h3 id="8-2-下载TensorFlow源码"><a href="#8-2-下载TensorFlow源码" class="headerlink" title="8.2 下载TensorFlow源码"></a>8.2 下载TensorFlow源码</h3><p>最一开始是直接通过pip下载官方编译好的安装包进行安装，但因为CentOS的原因，并不能通过这种方式安装（在网上找到的CentOS安装TensorFlow大部分都是通过编译源码来安装的）。所以这也是导致这次安装如此繁琐的原因</p><p>目前TensorFlow的最新版本为1.3.0，源码可以通过git clone下来，也可以直接去releases界面下载</p><ul><li>git clone</li></ul><p><code>git clone https://github.com/tensorflow/tensorflow</code><br><code>git checkout r1.3</code></p><ul><li>releases界面</li></ul><p><a href="https://github.com/tensorflow/tensorflow/archive/v1.3.0.tar.gz" target="_blank" rel="noopener">TensorFlow 1.3.0</a></p><h3 id="8-3-配置"><a href="#8-3-配置" class="headerlink" title="8.3 配置"></a>8.3 配置</h3><p>进入TensorFlow源码根目录后，执行</p><p><code>./configure</code></p><p>接下来会有一系列提示需要确认，其中</p><p>Do you wish to use jemalloc as the malloc implementation? [Y/n] 选择 n，因为选择y后编译不通过</p><p>Do you wish to build TensorFlow with CUDA support? [y/N] 选择 y，因为我们要使用GPU</p><p>其他的默认就可以（大写字母是默认选择）</p><p>以下是具体的配置选项</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">.........  </span><br><span class="line">You have bazel 0.5.3- installed.  </span><br><span class="line">Please specify the location of python. [Default is /usr/local/bin/python]:  </span><br><span class="line">Found possible Python library paths:  </span><br><span class="line">/usr/local/lib/python2.7/site-packages  </span><br><span class="line">Please input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/site-packages]  </span><br><span class="line"></span><br><span class="line">Using python library path: /usr/local/lib/python2.7/site-packages  </span><br><span class="line">Do you wish to build TensorFlow with MKL support? [y/N]  </span><br><span class="line">No MKL support will be enabled for TensorFlow  </span><br><span class="line">Please specify optimization flags to use during compilation when bazel option &quot;--config=opt&quot; is specified [Default is -march=native]:  </span><br><span class="line">Do you wish to use jemalloc as the malloc implementation? [Y/n] n</span><br><span class="line">jemalloc disabled  </span><br><span class="line">Do you wish to build TensorFlow with Google Cloud Platform support? [y/N]  </span><br><span class="line">No Google Cloud Platform support will be enabled for TensorFlow  </span><br><span class="line">Do you wish to build TensorFlow with Hadoop File System support? [y/N]  </span><br><span class="line">No Hadoop File System support will be enabled for TensorFlow  </span><br><span class="line">Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N]  </span><br><span class="line">No XLA support will be enabled for TensorFlow  </span><br><span class="line">Do you wish to build TensorFlow with VERBS support? [y/N]  </span><br><span class="line">No VERBS support will be enabled for TensorFlow  </span><br><span class="line">Do you wish to build TensorFlow with OpenCL support? [y/N]  </span><br><span class="line">No OpenCL support will be enabled for TensorFlow  </span><br><span class="line">Do you wish to build TensorFlow with CUDA support? [y/N] y</span><br><span class="line">CUDA support will be enabled for TensorFlow  </span><br><span class="line">Do you want to use clang as CUDA compiler? [y/N]  </span><br><span class="line">nvcc will be used as CUDA compiler  </span><br><span class="line">Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]:  </span><br><span class="line">Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:  </span><br><span class="line">Please specify which gcc should be used by nvcc as the host compiler. [Default is /opt/gcc-4.9.4/bin/gcc]:  </span><br><span class="line">Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 6.0]:  </span><br><span class="line">Please specify the location where cuDNN 6 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:  </span><br><span class="line">Please specify a list of comma-separated Cuda compute capabilities you want to build with.  </span><br><span class="line">You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.  </span><br><span class="line">Please note that each additional compute capability significantly increases your build time and binary size.  </span><br><span class="line">[Default is: &quot;6.0&quot;]:  </span><br><span class="line">Do you wish to build TensorFlow with MPI support? [y/N]  </span><br><span class="line">MPI support will not be enabled for TensorFlow  </span><br><span class="line">Configuration finished</span><br></pre></td></tr></table></figure><h3 id="8-4-编译打包安装"><a href="#8-4-编译打包安装" class="headerlink" title="8.4 编译打包安装"></a>8.4 编译打包安装</h3><p>接下来进行编译操作</p><p><code>bazel build --config=opt --config=cuda//tensorflow/tools/pip_package:build_pip_package</code></p><p>如果顺利的话就不会出现什么ERROR提示，整个编译过程大约十分钟。如果出现错误可以加入 <code>--verbose_failures</code> 参数，会提供更丰富的出错信息</p><p>编译完成后，接下来就是打包成 .whl 文件供pip安装使用，文件会放到/tmp/tensorflow_pkg目录下</p><p><code>bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg</code></p><p>安装</p><p><code>pip install /tmp/tensorflow_pkg/tensorflow-1.3.0-py2-none-any.whl</code></p><p>安装过程会自动联网下载一些依赖的包，如果无法连外网的话，那就需要自己下载所需要的包，然后上传到服务器后用pip离线安装</p><p>可以使用 <a href="https://pypi.python.org/pypi/" target="_blank" rel="noopener">官方pip源</a> 或者 <a href="http://mirrors.aliyun.com/pypi/simple/" target="_blank" rel="noopener">阿里pip源</a> 来搜索安装，推荐后者，国内快</p><p>依赖的包有（可能不全）：</p><ul><li>backports.weakref-1.0</li><li>bleach-1.5.0</li><li>funcsigs-1.0.2</li><li>html5lib-0.9999999</li><li>Markdown-2.6.9</li><li>mock-2.0.0</li><li>numpy-1.13.1</li><li>pbr-3.1.1</li><li>protobuf-3.4.0</li><li>six-1.11.0</li><li>tensorflow_tensorboard-0.1.6</li><li>webencodings-0.5.1</li><li>Werkzeug-0.12.2</li><li>wheel-0.30.0</li></ul><h3 id="8-5-验证"><a href="#8-5-验证" class="headerlink" title="8.5 验证"></a>8.5 验证</h3><p>进入python，进行 <code>import tensorflow</code>，没有错误那就代表大功告成了！</p><h3 id="8-6-一些错误"><a href="#8-6-一些错误" class="headerlink" title="8.6 一些错误"></a>8.6 一些错误</h3><p>整个编译过程还是比较坎坷的</p><p>1、第一个错误  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">ERROR: /root/gg/tensorflow/tensorflow/tensorflow/tools/pip_package/BUILD:100:1: no such package &apos;@boringssl//&apos;: Traceback (most recent call last):  </span><br><span class="line">File &quot;/root/gg/tensorflow/tensorflow/tensorflow/workspace.bzl&quot;, line 116  </span><br><span class="line">_apply_patch(repo_ctx, repo_ctx.attr.patch_file)  </span><br><span class="line">File &quot;/root/gg/tensorflow/tensorflow/tensorflow/workspace.bzl&quot;, line 107, in _apply_patch  </span><br><span class="line">_execute_and_check_ret_code(repo_ctx, cmd)  </span><br><span class="line">File &quot;/root/gg/tensorflow/tensorflow/tensorflow/workspace.bzl&quot;, line 91, in _execute_and_check_ret_code  </span><br><span class="line">fail(&quot;Non-zero return code(&#123;1&#125;) when ...))  </span><br><span class="line">Non-zero return code(256) when executing &apos;patch -p1 -d /root/.cache/bazel/_bazel_root/9abfd3cc56b23f8500d978612da34f89/external/boringssl -i /root/gg/tensorflow/tensorflow/third_party/boringssl/add_boringssl_s390x.patch&apos;:  </span><br><span class="line">Stdout:  </span><br><span class="line">Stderr: java.io.IOException: Cannot run program &quot;patch&quot; (in directory &quot;/root/.cache/bazel/_bazel_root/9abfd3cc56b23f8500d978612da34f89/external/boringssl&quot;): error=2, No such file or directory and referenced by &apos;//tensorflow/tools/pip_package:licenses&apos;.  </span><br><span class="line">ERROR: Analysis of target &apos;//tensorflow/tools/pip_package:build_pip_package&apos; failed; build aborted.  </span><br><span class="line">INFO: Elapsed time: 1.401s</span><br></pre></td></tr></table></figure><p>这个错误出现的时候@boringssl和@protobuf是交替出现，<code>no such package &#39;@boringssl</code>。一开始以为是这两个包没有下载成功，因为在info信息中有连接下载链接失败的提示，并且错误信息中是说没有找到对应的包，所以去 <code>tensorflow/workspace.bzl</code> 中查找对应的下载链接，下载到具体的包后在服务器上做了个微型本地服务器，将workspace.bzl中的下载链接改成本地的下载链接。然而这个错误继续出现，后来经过白银指点，发现是 <code>Cannot run program &quot;patch&quot;</code> 这里的问题，遂去 <code>yum install patch</code> 后该错误不再出现。原来的思路一直局限在下载不成功上，没有仔细观察后面的错误信息</p><p>2、第二个错误</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">C++ compilation of rule &apos;@nanopb_git//: nanopb&apos; failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command</span><br></pre></td></tr></table></figure><p>后续内容还有一堆，其中@nanopb_git会变成其他包的名字，所以查找的重点就放在了后面 <code>crosstool_wrapper_driver_is_not_gcc failed: error executing command</code></p><p>3、经过了一顿google，查阅了无数github上的issus，都没有发现类似的问题（有这部分相同的报错，但是后续错误内容不同），问题没有解决，我决定先编译TensorFlow提供的example，看看小范围编译是否存在问题</p><p><code>bazel build --config=opt --config=cuda //tensorflow/cc:tutorials_example_trainer</code></p><p>此时报错：</p><blockquote><p>error: ‘MADV_NOHUGEPAGE’ undeclared</p></blockquote><p>参照这个 <a href="https://github.com/tensorflow/tensorflow/issues/7572" target="_blank" rel="noopener">issus</a> 解决：</p><p>在进行configure配置时，将 Do you wish to use jemalloc as the malloc implementation? [Y/n] 选择 n</p><p>4、在对example编译成功后，又继续回来尝试编译源码，但是依旧出问题。不过，有时候命运就是捉弄人，尝到了‘踏破铁鞋无觅处，得来全不费功夫’的滋味</p><p>看到白银在看 <a href="http://www.jianshu.com/p/fdb7b54b616e" target="_blank" rel="noopener">这篇文章</a>，突然发现在文章的前部有这样一句话：</p><p>“CentOS 6 上glibc最多到2.12，强行使用高版本的glibc会导致程序意外崩溃”</p><p>这篇文章之前看过，但是注意力从来没有留意这句话。突然想到会不会真的是glibc的问题。因为在之前的尝试过程中因为某些报错需要安装glibc2.14版本，而系统里自带的版本只到2.12，所以就去下载编译了glibc2.14版本，并且加入LD_LIBRARY_PATH中</p><p>那么在 .bashrc 中将glibc添加到LD_LIBRARY_PATH中这一行注释掉，结果竟然顺利编译成功！真是惊喜啊</p><p>感谢白银同学，最后两个关键问题都是在他的帮助下解决的～</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li>官方安装指南</li></ul><p><a href="https://www.tensorflow.org/install/install_sources" target="_blank" rel="noopener">https://www.tensorflow.org/install/install_sources</a>  </p><p><a href="https://docs.bazel.build/versions/master/install-compile-source.html" target="_blank" rel="noopener">https://docs.bazel.build/versions/master/install-compile-source.html</a>  </p><p><a href="https://gcc.gnu.org/install/configure.html" target="_blank" rel="noopener">https://gcc.gnu.org/install/configure.html</a>  </p><p><a href="http://docs.nvidia.com/cuda/cuda-installation-guide-linux/" target="_blank" rel="noopener">http://docs.nvidia.com/cuda/cuda-installation-guide-linux/</a>  </p><ul><li>gcc安装</li></ul><p><a href="http://caosiyang.github.io/2016/05/04/installing-gcc/" target="_blank" rel="noopener">http://caosiyang.github.io/2016/05/04/installing-gcc/</a>  </p><p><a href="http://www.cjjjs.com/paper/czxt/2017222114137150.html" target="_blank" rel="noopener">http://www.cjjjs.com/paper/czxt/2017222114137150.html</a>  </p><ul><li>Python以及pip安装</li></ul><p><a href="http://farwmarth.com/%E5%8D%87%E7%BA%A7centos%E4%B8%8A%E7%9A%84python/" target="_blank" rel="noopener">http://farwmarth.com/%E5%8D%87%E7%BA%A7centos%E4%B8%8A%E7%9A%84python/</a>  </p><p><a href="http://blog.csdn.net/tiantuanzi/article/details/50475718" target="_blank" rel="noopener">http://blog.csdn.net/tiantuanzi/article/details/50475718</a>  </p><p><a href="http://www.itdadao.com/articles/c15a1317443p0.html" target="_blank" rel="noopener">http://www.itdadao.com/articles/c15a1317443p0.html</a></p><ul><li>显卡驱动及CUDA相关（文章里关于禁用nouveau、重新建立initramfs image文件的操作在本服务器上不操作也可以正常安装显卡驱动）</li></ul><p><a href="http://www.gpusever.com/show.php?cid=33&amp;id=27" target="_blank" rel="noopener">http://www.gpusever.com/show.php?cid=33&amp;id=27</a>  </p><p><a href="http://www.centoscn.com/image-text/install/2015/0921/6199.html" target="_blank" rel="noopener">http://www.centoscn.com/image-text/install/2015/0921/6199.html</a>  </p><ul><li>TensorFlow安装</li></ul><p><a href="http://www.jianshu.com/p/fdb7b54b616e" target="_blank" rel="noopener">http://www.jianshu.com/p/fdb7b54b616e</a>  </p><p><a href="http://ju.outofmemory.cn/entry/311727" target="_blank" rel="noopener">http://ju.outofmemory.cn/entry/311727</a>  </p><p><a href="https://blog.abysm.org/2016/06/building-tensorflow-centos-6/" target="_blank" rel="noopener">https://blog.abysm.org/2016/06/building-tensorflow-centos-6/</a>  </p><p><a href="http://blog.csdn.net/may0324/article/details/53413024" target="_blank" rel="noopener">http://blog.csdn.net/may0324/article/details/53413024</a>  </p><p><a href="http://www.alfrednanwu.com/machine-learning/-gtx-1080ubuntu1604cuda8cudnn51tensorflow" target="_blank" rel="noopener">http://www.alfrednanwu.com/machine-learning/-gtx-1080ubuntu1604cuda8cudnn51tensorflow</a>  </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;文章首发于微信公众号：链家产品技术团队，欢迎搜索关注~&lt;/p&gt;
&lt;p&gt;公司前一阵搞了两台GPU服务器，终于有“玩具”可以玩了～用的卡是最新的P100（好吧，真正最新的是V100，不过还没铺货）。本着爱折腾的精神，自然就开始了折腾它们的征程（结果是我被折腾了。。。）&lt;/p&gt;
&lt;p&gt;以前自己也搭建过TensorFlow的开发环境（&lt;a href=&quot;https://bluesmilery.github.io/blogs/9a018dfc/&quot;&gt;见链接&lt;/a&gt;），所以一开始以为这次也不会难，结果。。。咳咳，还是要正视自己的水平的。因为服务器上装的是系统是CentOS，以前自己捣鼓的时候用的是Ubuntu，差别还是不少的，所以特此记录自己踩过的坑，也给其他人一些经验帮助。&lt;/p&gt;
&lt;p&gt;这次折腾总共分为两个阶段：最初打算直接通过pip下载TensorFlow安装包安装，结果完全失败，不过在此期间摸清了许多限制条件，也为第二个阶段——通过编译的方式安装TensorFlow打下了基础。&lt;/p&gt;
&lt;p&gt;系统版本：CentOS release 6.8 (Final)    64位&lt;/p&gt;
    
    </summary>
    
      <category term="技术" scheme="https://bluesmilery.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="CentOS" scheme="https://bluesmilery.github.io/tags/CentOS/"/>
    
      <category term="TensorFlow" scheme="https://bluesmilery.github.io/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>增强学习 Reinforcement learning part 4 - Model-Free Prediction</title>
    <link href="https://bluesmilery.github.io/blogs/a6aaca4e/"/>
    <id>https://bluesmilery.github.io/blogs/a6aaca4e/</id>
    <published>2017-05-16T12:56:22.000Z</published>
    <updated>2018-01-31T03:45:13.176Z</updated>
    
    <content type="html"><![CDATA[<p>本文是在学习David Silver所教授的Reinforcement learning课程过程中所记录的笔记。因为个人知识的不足以及全程啃生肉，难免会有理解偏差的地方，欢迎一起交流。</p><p>课程资料：<a href="http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html" target="_blank" rel="noopener">http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html</a></p><h2 id="1、Introduction"><a href="#1、Introduction" class="headerlink" title="1、Introduction"></a>1、Introduction</h2><p>上一节讲的东西是基于已知的MPD，也就是有模型学习，而实际中很多情况下MDP是未知的，各个状态之间的转移概率以及reward很难得知，所以这种环境称为model free。</p><p>首先先讲model free prediction，类似于DP中的policy evaluation，去估计这个未知的MDP中各个状态的value function。]</p><p>下一节会讲model free control，类似于DP中的policy iteration，去最优化这个未知的MDP中各个状态的value function。</p><a id="more"></a><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/2jKbEh0B4d.png" alt=""></p><h2 id="2、Monte-Carlo-Learning"><a href="#2、Monte-Carlo-Learning" class="headerlink" title="2、Monte-Carlo Learning"></a>2、Monte-Carlo Learning</h2><p>在有模型的时候给定一个policy pi后对这个policy进行评估很简单，但是在model free的情况下，不知道MDP的状态转移概率及reward（但是知道state），就无法用原来的公式进行计算。所以MC采用的学习方式是learn directly from episodes of experience。这里episode理解为MDP中的某个序列，比如在之前的student例子中，C1 C2 C3 Pass Sleep就可理解为一个episode。MC的思想就是在MDP中，依据给定的policy pi去行动，会依次经过一些state并且得到reward。如果不断的尝试（尝试一次就是一个episode），那么就可以大体估计出MDP的状态转移概率和reward。</p><p>需要注意的是MC学习用的episode必须是完整的和有终止状态的。（完整可以这么理解，假如从C1 state开始，在policy pi下会依次经过C2 C3 Pass Sleep这些state，其中sleep是终止状态，那么C1到sleep就被称为一个完整的episode，要使用MC方法来计算C1的state value function，那必须使用这个完整的episode。与之相对的是下面要讲的TD方法是使用不完整的episode，比如计算C1的state value function ，只需使用C1 C2就可以）</p><p>使用Monte-Carlo进行policy evaluation。回忆下原来计算state value function的时候是求从状态s开始对未来的期望收益，而在MC中，收益的计算方法依旧一样，因为MC使用完整的episode，所以对于状态s后面的reward都知道。但是不再是对未来的收益求期望了（因为转移概率不知道），我们采用最简单的方法，求平均value = mean return</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/kdmeEgec7H.png" alt=""></p><p>在一个episode，如果经过了状态s，那么N(s)就累加一，S(s)累加上这次的return。然后状态s的V(s)就等于S/N，如果尝试的episode次数足够多，那么求出来的V(s)就是Vpi(s)。（原因是尝试的次数足够多后，整个policy的所有情况都会遍历到，从状态s往后的各种情况的return的概率用频率逼近，所以也就和求期望是一样的了）</p><p>可以看到first标记为红色，意思是在一个episode中，第一次经过状态s才计数，如果一个episode中经过了状态s好几次（循环），那么后面几次都不算的。与之相对的是every，就是每次经过状态s都计数。</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/0CJcGbjE24.png" alt=""></p><p>下面讲了一个例子，玩blackjack 21点</p><p>简单描述下建模过程，每个state由三个因素构成，agent手中牌的点数和，dealer显示的一张牌的点数，agent手中有没有ace（也就是A，这张牌既可以当1点，也可以当10点），这是agent能观察到的environment。agent的action有两种，stick和twist。reward按照胜负来表示，赢了+1，平手为0，输了-1</p><p>后面是使用MC来评估value function，在经过10000次episode（也就是10000局比赛），可以发现图像并不平整还有起伏，说明还有噪声，也就是还没收敛到Vpi，在经过50000局比才后，图像很平整，就可以认为已经收敛到Vpi了</p><table><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/KKdmKJ0l5B.jpg" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/I4LJ5C9EGb.jpg" alt=""></td></tr></table><p>有个问题可以改进，那就是求平均值那里，原来是把所有的值都累加完再求，而其实<strong>均值是可以增量求的</strong>。k个值的均值等于前k-1值的均值加上后面那部分，后面那部分可以看作是一个误差，什么与什么的误差呢，就是前k-1个求出来的均值是uk-1，预计下一个值也是uk-1，但实际上第k个值是xk，所以就产生了个误差，所以就要把这部分误差加上去修正平均值。</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/mh6b7i82G2.png" alt=""></p><p>所以在MC中更新V(s)可以用增量的方式，每完成一个episode，便可以用Gt去增量更新V(s)</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/3Db7lddldb.png" alt=""></p><p>后面误差前的系数虽然是跟次数有关，但实际上也可以用一个固定值代替，因为在实际情况中，记录次数需要额外开销，并且如果是一个动态的系统，也没法从头到尾一直记录次数。可以证明的是，用固定值代替也不影响结果。</p><p>In non-stationary problems, it can be useful to track a running mean, i.e. forget old episodes.</p><p>In real world, don’t want to remember everything.</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/68fe1CC2Ib.png" alt=""></p><h2 id="3、Temporal-Difference-Learning"><a href="#3、Temporal-Difference-Learning" class="headerlink" title="3、Temporal-Difference Learning"></a>3、Temporal-Difference Learning</h2><p>TD称为时序差分学习。MC因为要完成一个episode后才能更新V(s)，效率比较低，所以TD便改进了这一点，learn directly from incomplete episodes of experience，TD使用的是不完整的episode。</p><p>MC使用的是真实的return来更新V(s)，但是TD不是。TD使用一步真实的reward加上对下一个状态的state value function的猜测，所以TD updates a guess towards a guess。这种方法的好处就是走出一步就可以更新V(s)，而不用等整个episode完成。</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/1AgbFc8ADi.png" alt=""></p><p>下面举个例子，下班回家时对耗时的估计。可以看到MC要等整个路程都完成后才能更新各个状态的值，而TD在进行到下一状态后就可以更新上一状态的值。</p><table><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/GmajgKga07.png" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/LI3eBgh2k5.jpg" alt=""></td></tr></table><p>所以MC与TD相比较</p><ul><li>TD can learn before knowing the final outcome<br>TD在每步结束后就可以实时学习（learn online），也就是实时更新state value function。但是MC必须整个episode完成后才能求出Gt，从而更新state value function</li><li>TD can learn without the final outcome<br>TD可以使用不完整的episode学习，MC只能使用完整的episode学习。这样的话，TD可以用于continuing (non-terminating)的environment中，而MC只能用于episodic (terminating)的environment中</li></ul><p>再来讨论下Bias/Variance平衡的问题</p><p>bias：MC中的Gt是属于无偏估计的，因为用到的reward都是真实的没有偏差的，但是TD target是有偏差的估计，因为V(st+1)是猜的，不是真实的。</p><p>variance：因为MC的Gt要加很多项，每一项都会存在随机的噪声，所以MC的variance会很高，而TD只有两项，所以受到的随机因素影响很小，从而TD的variance就小。</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/b9CCaLibjh.png" alt=""></p><p>所以这是MC与TD第二个区别</p><ul><li>MC has high variance, zero bias<ul><li>Good convergence properties</li><li>(even with function approximation)</li><li>Not very sensitive to initial value</li><li>Very simple to understand and use</li></ul></li><li>TD has low variance, some bias<ul><li>Usually more efficient than MC</li><li>TD(0) converges to vpi(s)</li><li>(but not always with function approximation)</li><li>More sensitive to initial value</li></ul></li></ul><p>下面再讲个例子，有一条直线，左右两边是终止状态，中间是ABCDE五个状态，action是向左或者向右，只有在E向右的时候才会得到reward+1，左边是对各个状态的state value function的估计，可以看到经过100次episode后，已经比较接近真实的value了。右图是统计了估计的value与真实的value之间的RMS误差，上半部分是MC，下半部分是TD，分别使用了不同的alpha值，可以看到TD整体是优于MC的，并且随着episode次数的增加，误差逐渐减少并趋于稳定，在平稳部分还有波动那就是噪声引起的，这也可以看出TD的variance比MC的小很多。</p><table><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/54HDEafKDg.jpg" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/jBFgmgFlJg.jpg" alt=""></td></tr></table><p>之前说道MC和TD在experience趋向无穷的时候，V(s)才会收敛于Vpi(s)，那如果experience是有限的呢？</p><p>比如这个例子，总共只有八次episode，求AB两个状态的state value function。可以先求B的，8次中有两次得到的reward是0，6次得到的reward是1，所以V(B) = 6/8 = 0.75。那A的呢，如果以MC的角度去看，出现A的episode只有第一个，并且在AB收到的reward都为0，所以Gt = 0，从而V(A) = 0。但是从TD的角度看，我们先用得到的episode构建出MDP，所以V(A) = 0 + V(B) = 0.75</p><table><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/hBgA214Af3.png" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/A163FgDI4J.jpg" alt=""></td></tr></table><p>从这个例子中也可以看出在有限的experience下，MC和TD的计算方法是不一样的</p><p>MC：是求最小均方误差，就是使用出现过A的episode来的计算Gt，然后更新state value function</p><p>TD：是使用最大似然马尔可夫模型，根据所有的episode计算出状态转移概率和reward，从而求出state value function</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/jdjIBL6A94.png" alt=""></p><p>通过这个例子可以看出MC与TD的第三点区别，TD能利用马尔可夫性，所以在马尔可夫环境下更有效率，而MC无法利用马尔可夫性，所以在非马尔可夫环境下更有效率</p><ul><li>TD exploits Markov property<ul><li>Usually more efficient in Markov environments</li></ul></li><li>MC does not exploit Markov property<ul><li>Usually more effiective in non-Markov environments</li></ul></li></ul><p>下面总结一下MC、TD、DP的区别，MC是使用一个完整的episode来更新V(s)，TD是使用不完整的episode来更新V(s)，而DP是使用状态s后面下一步所有的状态来计算V(s)</p><table><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/gc6h0AkGKa.png" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/2JL8I4e6b4.png" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/71J69Gjde6.png" alt=""></td></tr></table><p>而且还有两个概念前面没有提及，bootstrapping和sampling</p><p>sampling可以理解成抽样，就是使用一个episode，可以看到MC和TD是使用sampling的，而DP是使用遍历下一层所有的情况，所以不使用sampling</p><p>bootstrapping可以理解成是不是需要使用完整的episode，像TD和DP都是使用下一层来计算上一层的，而MC是使用一个完整的episode，一直到terminate</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/8G3Imk6g58.png" alt=""></p><p>下面这张图从bootstrapping和sampling两个维度把之前讲的方法进行了分类，一目了然</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/712FL9gdBI.jpg" alt=""></p><h2 id="4、TD-λ"><a href="#4、TD-λ" class="headerlink" title="4、TD(λ)"></a>4、TD(λ)</h2><p>之前讲的TD都是只考虑了未来的一步，那可不可以考虑未来的两步呢，甚至是三步四步？当然可以，见下图，这就称作n-step TD，如果n无穷，那么就是MC方法了</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/iDHcgib9L4.jpg" alt=""></p><p>在n-step TD中Gt的计算方法如下</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/jBmfE4d9F0.png" alt=""></p><p>那么n-step TD的V(s)更新公式变为</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/KfG3jKDlIF.png" alt=""></p><p>下图是Large Random Walk Example，统计了经过10次episode后的RMS误差，横坐标为选择不同的alpha，图中不同的曲线代表了n的不同取值。</p><p>上面的是online的，immediately update value function。下面的是offline，delay update until the end of episode</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/0L8d8I2CcB.jpg" alt=""></p><p>可以看到online与offline误差最小的位置不一样，如果我们换了一个MDP或者做了一些其他的变化，那这个最优点就会变化，选的n就会变，如果每次都要画这种图来比较的话就太麻烦了，有没有什么办法可以解决一下？</p><p>答案是有的，那就是把不同的n-step组合一下，比如把n=4和n=2按照一半一半的比例组合一下。但是这么组合很傻瓜，有没有什么更有效的组合方法？</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/IFCa31G44j.png" alt=""></p><p>答案也是有的，把所有的n-step都综合起来，各自的比例分别是(1-λ)λn-1，所以新的Gtλ就是把Gt(n)按照各自的比例加起来，然后用Gtλ来更新V(s)，所以这种TD称作Forward-view TD(λ)</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/gdhelHEg0b.jpg" alt=""></p><p>关于n-step各自的比例(1-λ)λn-1是服从指数分布的</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/HmckfdB5f2.jpg" alt=""></p><p>Forward-view TD(λ)如下图所示，在更新状态St时需要用到未来的值，所以被称作像前看forward view。但是需要注意的是，因为需要用到所有的n-step，所以它也需要完整的episode的才能计算，这一点与MC一样</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/a2e76f9GLe.jpg" alt=""></p><p>这是Forward-view TD(λ) on Large Random Walk，图中不同的曲线代表着选择了不同的λ</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/8fLKcJHem0.jpg" alt=""></p><p>既然有forward view，那么肯定有backward view</p><ul><li>Forward view provides theory</li><li>Backward view provides mechanism</li><li><strong>Update online, every step, from incomplete sequences</strong></li></ul><p>讲述一个新概念，Eligibility Traces，字面理解是资格迹或者传导径迹，其实质是判断过去经历过的状态中哪些对现在的状态有影响。比如依次经过了三次响铃，一次灯亮后，你遭到了电击，那么是三次响铃导致了电击，还是说灯亮后会导致电击。当然都有可能，所以我们从把这两方面都考虑进去，考虑频率的称为frequency heuristic，考虑最近的称为recency heuristic，然后把它们俩结合起来。</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/1L1I92h7af.jpg" alt=""></p><p>下面是backward view TD(λ)的主要含义，对于每个state来说都有自己的eligibility trace E(s)，在更新V(s)的时候把E(s)也考虑进去了，乘在TD error部分</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/Ihdf4HLdfg.jpg" alt=""></p><p>E(s)初始化为0，在第一次经过状态s的时候便会+1，并且每一时刻E(s)都会按照</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/8l7BhAe1Ci.jpg" alt=""></p><p>进行指数级衰减，如果再次经过了状态s，那么再+1。这个+1就是下图中突然向上的那一下。</p><p>假设下图描述的就是状态s的eligibility trace，在最一开始的时候为0，然后第一次经过状态s后+1，然后开始衰减，但是过了很短的时间又经过了状态s，所以又向上冲了一下。所以图中前面一部分就是很短的时间内经过了状态s四次，所以此时E(s)就很高，但是在接下来很长的时间内没有在经过状态s，所以E(s)快速衰减，接近于0。此时再次连续两次经过状态s，又很长时间没经过，然后再次经过状态s，然后就结束了。所以图片就是状态s的E(s)的值的变化情况。</p><table><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/2l20ahiH1f.jpg" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/68916dFA16.png" alt=""></td></tr></table><p>backward view TD(λ)的伪代码，看着比较容易理解这个算法是怎么样的，以及eligibility trace是如何更新的</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/IAkd36Ld6e.jpg" alt=""></p><p>Forward view和backward view的本质是一样的，只是从两个角度去描述了同一件事情。Forward view是向未来看，用未来的reward来更新当前状态，而backward view是向过去看，在当前状态收到reward了以后，它会把reward传递回过去，去更新过去的点（以过去状态的角度看来，这不也就是用未来的reward来更新自己吗）。并且forward view和backward view都是离当前状态近的占的比例高，如果两个状态离的很远（假设为A,B，以时间为顺序A在过去B在未来），从forward view的角度看，B对A的影响很小，从backward view的角度看，B往回传递给A的影响也很小，是统一的。</p><p>当λ=0时，便是最一开始讲的TD(0)</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/3aBdBHGecG.png" alt=""></p><p>而当λ=1时，是与MC是等价的，下面几张图就说的是这件事</p><ul><li>When λ = 1, credit is deferred until end of episode</li><li>Consider episodic environments with offline updates</li><li>Over the course of an episode, total update for TD(1) is the</li><li>same as total update for MC</li></ul><table><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/fk3H2D6l76.png" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/9hhAF66270.png" alt=""></td></tr></table><ul><li>TD(1) is roughly equivalent to every-visit Monte-Carlo</li><li>Error is accumulated online, step-by-step</li><li>If value function is only updated offline at end of episode</li><li>Then total update is exactly the same as MC</li></ul><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/69I12jlK8I.png" alt=""></p><p>实质上forward-view TD(λ)和backward view TD(λ)是等价的，是一个算法从两个角度去看</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/H3HLj6deh9.png" alt=""></p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/ImFcjGIgDd.png" alt=""></p><p>Offline updates</p><ul><li>Updates are accumulated within episode</li><li>but applied in batch at the end of episode</li></ul><p>Online updates</p><ul><li>TD(λ) updates are applied online at each step within episode</li><li>Forward and backward-view TD(λ) are slightly different</li><li>NEW: Exact online TD(λ) achieves perfect equivalence</li><li>By using a slightly dierent form of eligibility trace</li><li>Sutton and von Seijen, ICML 2014</li></ul><p>总结：</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/lkKf7hkl2l.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是在学习David Silver所教授的Reinforcement learning课程过程中所记录的笔记。因为个人知识的不足以及全程啃生肉，难免会有理解偏差的地方，欢迎一起交流。&lt;/p&gt;
&lt;p&gt;课程资料：&lt;a href=&quot;http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;1、Introduction&quot;&gt;&lt;a href=&quot;#1、Introduction&quot; class=&quot;headerlink&quot; title=&quot;1、Introduction&quot;&gt;&lt;/a&gt;1、Introduction&lt;/h2&gt;&lt;p&gt;上一节讲的东西是基于已知的MPD，也就是有模型学习，而实际中很多情况下MDP是未知的，各个状态之间的转移概率以及reward很难得知，所以这种环境称为model free。&lt;/p&gt;
&lt;p&gt;首先先讲model free prediction，类似于DP中的policy evaluation，去估计这个未知的MDP中各个状态的value function。]&lt;/p&gt;
&lt;p&gt;下一节会讲model free control，类似于DP中的policy iteration，去最优化这个未知的MDP中各个状态的value function。&lt;/p&gt;
    
    </summary>
    
      <category term="笔记" scheme="https://bluesmilery.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement Learning" scheme="https://bluesmilery.github.io/tags/Reinforcement-Learning/"/>
    
  </entry>
  
  <entry>
    <title>增强学习 Reinforcement learning part 3 - Planning by Dynamic Programming</title>
    <link href="https://bluesmilery.github.io/blogs/b96003ba/"/>
    <id>https://bluesmilery.github.io/blogs/b96003ba/</id>
    <published>2017-04-10T12:04:48.000Z</published>
    <updated>2018-01-31T03:45:02.164Z</updated>
    
    <content type="html"><![CDATA[<p>本文是在学习David Silver所教授的Reinforcement learning课程过程中所记录的笔记。因为个人知识的不足以及全程啃生肉，难免会有理解偏差的地方，欢迎一起交流。</p><p>课程资料：<a href="http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html" target="_blank" rel="noopener">http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html</a></p><h2 id="1、Introduction"><a href="#1、Introduction" class="headerlink" title="1、Introduction"></a>1、Introduction</h2><p>这一部分是用动态规划来解决MDP问题，所以先介绍下什么是动态规划</p><p><mark>Dynamic Programming</mark>（动态规划），是用来解决一些复杂的问题，将复杂的问题分解为一些子问题，然后分别解决这些子问题，最后再把解决方案合并得到复杂问题的解</p><ul><li>Dynamic：代表这个问题是连续的或者是跟时间相关的</li><li>Programming：不是编程的意思，而指的是优化一个程序</li></ul><a id="more"></a><p>想要使用DP来解决一个问题，那么这个问题需要具有以下两种性质：</p><ul><li>Optimal substructure：满足最优化原理，最优解分解后针对子问题也是最优解</li><li>Overlapping subproblems：子问题重复出现，对每一种子问题只计算一次，然后将结果存起来，下次直接使用</li></ul><p>而MDP刚好满足这两条性质，bellman equation具有递归分解性，说明可以分解为子问题并且会重复出现（类似于斐波那契数列）；value function就相当于能够存储并且重复利用的解</p><p><strong>要使用DP来解决MDP问题，假设条件是对MDP是充分认知（full knowledge）的</strong></p><p>对于MDP来说，还是从两个方面来说，prediction and control</p><p>prediction的目的是给你policy，然后去预测使用这个policy的话各个state的value function，也就是预测使用这个policy的话未来的收益</p><p>control的目的是不给你policy，自己去寻找最优的policy以及未来最优的收益</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/J028IlH8i7.png" alt=""></p><p>扩展：动态规划还可以解决其他许多问题</p><ul><li>Scheduling algorithms</li><li>String algorithms (e.g. sequence alignment)</li><li>Graph algorithms (e.g. shortest path algorithms)</li><li>Graphical models (e.g. Viterbi algorithm)</li><li>Bioinformatics (e.g. lattice models)</li></ul><h2 id="2、Policy-Evaluation"><a href="#2、Policy-Evaluation" class="headerlink" title="2、Policy Evaluation"></a>2、Policy Evaluation</h2><p>策略评估</p><p>需要解决的问题：评估一个给定的policy</p><p>解决方案：以迭代次数为维度（或者理解成时间，一次迭代相当于过去了一个时刻，agent执行了一次action）不断去迭代Bellman Expectation Equation</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/eLDB0cbdGh.png" alt=""></p><p>可以证明，<strong>最终value function会收敛到vpi</strong></p><p>采用同步更新的方式，在新的时刻所有的state同时更新</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/7F8m89m2L7.png" alt=""></p><p>计算方法如下：</p><p>套用Bellman Expectation Equation，加上迭代次数维度，依据马尔可夫性，未来只与现在有关，所以求state s的k+1时刻的value function，就要使用k时刻state s’的value function，其中s’是s的后续state</p><table><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/Ll2i3G7E1c.png" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/IFA6cI4gEA.png" alt=""></td></tr></table><p>上面的是RL part2中的计算方程，相当于是下面的计算方程迭代到最后已经收敛的形式</p><table><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/0HJ54KeD14.png" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/ejd0leDJCF.png" alt=""></td></tr></table><p>下面举个例子gridworld</p><p>这是个4x4的方格，那两个灰色的方格是特殊的state，可以看作是终点，其他14个白色的方格是普通的state，在白色方格时有四种action选择，向上下左右移动，各自的概率为0.25。每移动一步reward减1，直到灰色的方格</p><p>所以这个例子可以看作你随机出现在某个方格上，然后用最少的步数走到灰色方格。</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/4KkGKDG329.png" alt=""></p><p>下面是进行policy evaluation的迭代过程，k代表第几次迭代</p><p>先说左边的，选择的policy是random policy，就是在任何一个state，选择四个方向的概率都为0.25。初始v0的时候所有state的value都是0。然后v1时，利用上面vk+1(s)的公式进行计算（0.25x(-1+0)x4）。下面以v2中两个state来说明具体的计算过程</p><p><u>先计算state6</u>（就是第二行第三个，这里对应公式的话就是s）在这次迭代中（对应公式的话就是k+1）的state value：如果向上走的话会移动到state2（对应公式的话就是s’），那这个action value就是reward(-1)加上state2上一时刻（对应公式的话就是k）的state value(-1)，所以向上移动是0.25x(-1-1)。向其他三个方向移动的计算方式和向上移动类似，结果也都是0.25x(-1-1)。所以state6在v2时的state value就是0.25x(-1-1)x4=-2</p><p><u>再计算state1</u>（第一行第二个）的state value：如果选择向上走的话，因为上面没有格子了，所以会停在原地，那这次action value就是移动的reward(-1)加上state1上一时刻的state value(-1)，所以向上移动是0.25x(-1-1)。向下向右类似。向左移动的话是reward(-1)加上灰色格子上一时刻的state value(0)，所以向左移动就是0.25x(-1+0)。所以state1在v2时的state value就是0.25x(-1-1)x3+0.25x(-1+0)=-1.75。图中显示是-1.7，那是因为只能显示两位数。</p><p>如果一直迭代下去，当k为无穷的时候，各个state的value就已经收敛不会变了。</p><table><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/ilhLcGJKm0.png" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/JAB7kbLJbK.png" alt=""></td></tr></table><p>再说下右边的，显示的是每次迭代根据左边计算出来的state value选择出来的greedy policy。greedy policy指的是选出各个action value中最高的所对应的action。比如在v2时，state1的四个action value中，向左移动的action value是最高的，所以greedy policy在state1处只选择向左移动</p><p>当random policy经过许多步迭代后state value收敛到vpi时，这时候选出的greedy policy就是random policy的optimal policy</p><p>注意：evaluate任何一个policy（这里是random policy）都能得到greedy policy</p><p>总结：进行策略评估的实质就是计算在使用policy pi时各个state的value function，但是因为reword和action的存在，随着时间的推移（action次数的增加），每个state的value function会变化，但是最终会收敛到一个固定值vpi，然后使用greedy policy选出policy pi的optimal policy</p><h2 id="3、Policy-Iteration"><a href="#3、Policy-Iteration" class="headerlink" title="3、Policy Iteration"></a>3、Policy Iteration</h2><p>策略迭代</p><p>在策略评估中，讲的是在给定一个policy pi后，先对其进行evaluate得到vpi，然后使用greedy policy来improve policy，得到新的policy pi’</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/101a9hKiaC.png" alt=""></p><p>在上面gridworld的例子中，因为这个例子很简单，所以经过一次improve后得到的pi’就已经是optimal policy pi*</p><p>但是通常说来，一般需要经过多次evaluation/improvement的过程才能得到pi*</p><p>这种不断evaluation/improvement的过程就称作策略迭代</p><p><strong>注意：类似于policy evaluation中state value会收敛到vpi，在policy iteration中policy肯定会收敛到pi*</strong></p><table><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/BFA2CI6G5m.jpg" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/Hel0eHdai0.png" alt=""></td></tr></table><p>evaluation和improvement过程中的算法可以是任意的</p><p>example：</p><table><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/8CeJK6GFi3.jpg" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/I21d5hiche.jpg" alt=""></td></tr></table><p>下面是具体讲一下improve过程，证明了为什么improve后vpi’(s)&gt;=vpi(s)</p><table><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/kJ4B7BlhCJ.png" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/97b7kJ4dJ2.png" alt=""></td></tr></table><p>modified：在evaluation部分，一定要等state value达到vpi吗？</p><p>事实上，这并不是必须的，就好比上面的gridworld例子，在第三次迭代的时候其实就已经是optimal policy了，所以没必要继续迭代下去</p><p>所以我们可以通过一些方法提前停止evaluation，比如在state value收敛的过程中设置epsilon，小于则停止迭代；或者是设置迭代k次就停止</p><p>如果设置evaluation只迭代一次，即k=1，那么这就是下面要讲的value iteration</p><p>总结：policy iteration相比于policy evaluation而言，后者是一次evaluation/improvement过程，只能得到比policy pi更好的policy pi’，而前者是多次evaluation/improvement过程，最终能得到该MDP的optimal policy pi*</p><h2 id="4、Value-Iteration"><a href="#4、Value-Iteration" class="headerlink" title="4、Value Iteration"></a>4、Value Iteration</h2><p>值迭代</p><p>上面说过能用DP解决的问题需要具有一个特点，满足最优化原理（Principle of Optimality），就是最优解分解之后针对每个子问题也是最优解，对于MDP来说，就是optimal policy可以分为两部分，当前state的optimal action A*，后续state的policy也是optimal policy<br>Any optimal policy can be subdivided into two components:</p><ul><li>An optimal first action A*</li><li>Followed by an optimal policy from successor state S0</li></ul><p>所以，若要满足一个policy pi使的state s的value为optimal value，当且仅当policy pi使得s的后续state s’的value为optimal value</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/f9GH2k4901.png" alt=""></p><p>所以知道了s’的optimal value便可以倒推出来s的optimal value</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/chHIB2ahGI.jpg" alt=""></p><p>所以这便是value iteration的思想所在，迭代重复上面这个公式</p><p>直观上看来，是从最后一个state开始，倒推到第一个state，下面以一个例子来说明下</p><p>这个例子的目的是每个state找出到达左上角的最短路径（步数），其他方面类似于之前gridworld的例子，action是向四个方向移动，每移动一步reward减1</p><p>V1初始时所有的state的value都是0，然后在V2时，每个state从四个action value中选出最高的那个作为自己的state value（在policy iteration中是将四个action value求期望作为自己的state value），以第二行第二个为例，它在V1V2V3的时候四个action value都是一样的，所以没有相对的optimal，但是从V4开始，向上和向左比向下向右好，所以它的state value就一直为-2，拿V6来说，它的state value = max(-2, -2, -4, -4) = -2</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/888CK5hL90.png" alt=""></p><p>在这个例子中需要注意这么几个问题：</p><ul><li>每一次迭代中所有的state都要计算value，只不过有些state每次max的时候都是那些值（先达到了optimal value）所以看起来就不变了</li><li>在开始的时候不像policy iteration那样有个明确的policy，而且在迭代过程中state value并不对应于哪个policy，只有在最终迭代完成后（在例子中是V7）才对应于一个policy，而且还是optimal policy</li><li>如果以右下角那个state为开始的话，那路径就是-6 -5 -4 -3 -2 -1 0，而计算过程是-1 -2 -3 -4 -5 -6，所以符合上面说的Principle of Optimality的倒推性</li></ul><p>所以对于value iteration来说</p><p>需要解决的问题：找到optimal policy pi*</p><p>解决方案：不断去迭代Bellman optimality Equation</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/7FbcF26E5K.png" alt=""></p><p>可以证明， <strong>最终value function会收敛到v*</strong></p><p>采用同步更新的方式，在新的时刻所有的state同时更新</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/0cL2DLfmib.png" alt=""></p><p>计算过程：</p><table><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/EeFJ4dfg6c.png" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/FjBAl9jHbk.png" alt=""></td></tr></table><p>总结：value iteration的本质就是在每次迭代中每个state都选择optimal value，那么当所有的state都选择出了optimal value后，所对应的policy就是optimal policy。</p><p>对比policy iteration，是迭代到收敛后每个state才选择optimal value，然后这时候对应的policy就是improve后的policy pi’</p><p>有个value iteration的例子：需要安装java环境</p><p><a href="http://www.cs.ubc.ca/~poole/demos/mdp/vi.html" target="_blank" rel="noopener">http://www.cs.ubc.ca/~poole/demos/mdp/vi.html</a></p><p>下表是对DP的一个总结</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/I7B4KjDCFG.png" alt=""></p><p>5、</p><ul><li>Extensions to Dynamic Programming</li><li>Asynchronous Dynamic Programming</li><li>In-Place Dynamic Programming</li><li>Prioritised Sweeping</li><li>Real-Time Dynamic Programming</li><li>Full-Width Backups</li><li>Approximate Dynamic Programming</li></ul><p>6、</p><ul><li>Contraction Mapping</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是在学习David Silver所教授的Reinforcement learning课程过程中所记录的笔记。因为个人知识的不足以及全程啃生肉，难免会有理解偏差的地方，欢迎一起交流。&lt;/p&gt;
&lt;p&gt;课程资料：&lt;a href=&quot;http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;1、Introduction&quot;&gt;&lt;a href=&quot;#1、Introduction&quot; class=&quot;headerlink&quot; title=&quot;1、Introduction&quot;&gt;&lt;/a&gt;1、Introduction&lt;/h2&gt;&lt;p&gt;这一部分是用动态规划来解决MDP问题，所以先介绍下什么是动态规划&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;Dynamic Programming&lt;/mark&gt;（动态规划），是用来解决一些复杂的问题，将复杂的问题分解为一些子问题，然后分别解决这些子问题，最后再把解决方案合并得到复杂问题的解&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dynamic：代表这个问题是连续的或者是跟时间相关的&lt;/li&gt;
&lt;li&gt;Programming：不是编程的意思，而指的是优化一个程序&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="笔记" scheme="https://bluesmilery.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement Learning" scheme="https://bluesmilery.github.io/tags/Reinforcement-Learning/"/>
    
  </entry>
  
  <entry>
    <title>增强学习 Reinforcement learning part 2 - Markov Decision Process</title>
    <link href="https://bluesmilery.github.io/blogs/e4dc3fbf/"/>
    <id>https://bluesmilery.github.io/blogs/e4dc3fbf/</id>
    <published>2017-03-21T11:11:51.000Z</published>
    <updated>2018-01-31T03:44:55.006Z</updated>
    
    <content type="html"><![CDATA[<p>本文是在学习David Silver所教授的Reinforcement learning课程过程中所记录的笔记。因为个人知识的不足以及全程啃生肉，难免会有理解偏差的地方，欢迎一起交流。</p><p>课程资料：<a href="http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html" target="_blank" rel="noopener">http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html</a></p><h2 id="1、Markov-Processes"><a href="#1、Markov-Processes" class="headerlink" title="1、Markov Processes"></a>1、Markov Processes</h2><p>在RL中，<strong>MDP是用来描述environment的，并且假设environment是full observable的</strong></p><p>i.e. The current state completely characterizes the process</p><p>许多RL问题可以用MDP来表示：</p><ul><li>Optimal control primarily deals with continuous MDPs</li><li>Partially observable problems can be converted into MDPs</li><li>Bandits are MDPs with one state</li></ul><a id="more"></a><p>先介绍两个基本知识</p><p><em>Markov Property</em>：The future is independent of the past given the present</p><p>未来只与现在有关，与过去无关。这一点性质相当于是简化了模型。</p><p><em>State Transition Matrix</em>：从状态s转换到状态s’的概率</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/5IDhf7mckm.png" alt=""></p><p>把所有的状态转换概率写成矩阵P     PS：矩阵每一行和为1</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/3fDk2ChCAi.png" alt=""></p><p><strong>Markov Process</strong>：具有马尔可夫性的随机过程，用&lt;S, P&gt;来表示</p><ul><li>S is a (finite) set of states</li><li>P is a state transition probability matrix</li></ul><p>example：</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/AAJkdFdCBI.png" alt=""></p><h2 id="2、Markov-Reward-Processes"><a href="#2、Markov-Reward-Processes" class="headerlink" title="2、Markov Reward Processes"></a>2、Markov Reward Processes</h2><p>MRP是在MP的基础上增加了一个值——reward。MRP用&lt;S, P, R, gamma&gt;来表示</p><p>其中S和P的定义与MP中的一样，R代表的是到达状态s后（或者说在状态s时）会得到的奖励（或者说是收益），gamma是折现因子，用于表示未来的收益折算到现在的比例，范围是0~1</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/ebHbbF59ha.png" alt=""></p><p>example：</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/hH93g4EKfc.png" alt=""></p><p><strong>Return</strong>：从第t步开始，未来N步的总收益，未来的部分是有折扣的。return只是针对某个sequence而言</p><p>The return Gt is the total discounted reward from time-step t.</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/EabBfi6BEa.png" alt=""></p><p>gamma越接近于0表示越看重眼前的收益，越接近于1表示越看重未来的收益。</p><p>为什么要用gamma这个discounted factor？</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/FCHihJcBkk.png" alt=""></p><p>example：</p><p>假如MRP中某个sequence是C1 C2 C3 Pass Sleep，那么在C1时的return G1 = - 2 - 2 <em> 0.5 - 2 </em> 0.25 + 10 * 0.125 = -2.25</p><p>注意这里R2就表示在C1（S1）处的reward，之所以下标不一样，是因为在建模时，对于t的界定是新的t+1是从agent的action结束后，environment反馈reward和observation时开始算，所以状态St = s的reward用Rt+1来表示</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/bGGk6Kc4m5.png" alt="">&gt;</p><p><strong>Value Function</strong>：MRP中只有state value function——v(s)，状态s的value function。表示的是从状态s开始对未来的期望收益。（从状态s开始往后所有的sequence的收益的期望）</p><p>The state value function v(s) of an MRP is the expected return starting from state s</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/h6I1L65Jde.png" alt=""></p><p>example：</p><p>计算方法见Bellman Equation部分</p><table><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/LggGfhFbBB.png" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/bcCcb8jEhf.png" alt=""></td></tr></table><p><strong>Bellman Equation</strong>：我们将v(s)进行一些数学变换，会发现v(s)可以分解成两部分，一部分是状态s的immediate reward Rt+1，另一部分是折扣过的下一state的v(St+1)</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/4hE85lCam4.png" alt=""></p><p>假设下一个state有两个，那么结构类似与下面这种树状图，将下一个的所有state都average起来求期望。就相当于树下面的加起来给上面的根节点</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/A0HmB32GmB.png" alt=""></p><p>所以最终v(s)可以表示为</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/db2413DJ2A.png" alt=""></p><p>所以上面那个例子中每个state的v(s)计算过程如下：</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/HdEeDH6L9m.png" alt=""></p><p>可能会注意到计算class3的时候用到了pub，而如果计算pub的时候又会用到class3，有种循环递归的问题，但是实际上v(s)的计算表达式可以用矩阵的形式来表示</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/55F9Cg7dHd.png" alt=""></p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/FHHiIGIEgD.png" alt=""></p><p>由于这是个线性方程，所以可以直接解出。那么上面说的循环递归的问题就不存在了，因为所有state的value function是同时求出来的</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/chab04bLEj.png" alt=""></p><p>但是直接解的复杂度为O(n3)，n为state的个数，所以如果要用直接解法那么只能用于很小的MRP，对于很大的MRP，有很多迭代的方法可以用：</p><ul><li>Dynamic programming（动态规划）</li><li>Monte-Carlo evaluation（蒙特卡罗强化学习）</li><li>Temporal-Difference learning（时序差分学习）</li></ul><h2 id="3、Markov-Decision-Processes"><a href="#3、Markov-Decision-Processes" class="headerlink" title="3、Markov Decision Processes"></a>3、Markov Decision Processes</h2><p>MDP是在MRP的基础上再加一个元素——action。MDP用&lt;S, A, P, R, gamma&gt;来表示</p><p>MDP是agent在自己的大脑中用来描述environment模型的，MDP中每个state都具有马尔可夫性</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/HC6IEIa7ma.png" alt=""></p><p>example：与MRP的example相比，概率变成了action，pub的state消失了，在这里如果在class3选择了pub这个action，直接会有0.2概率去class1，而不会有个pub这种state来停留</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/F604DLhE9d.png" alt=""></p><p><strong>Policy</strong>：在一个state处，agent接下来会采取各种action的概率分布。实质上policy就是定义了agent的行为，在这个state会怎么选，在下一个state会怎么选等等</p><p>MDP的policy具有马尔可夫性，只与当前state有关。所以policy也是time-independent的</p><p>加入了policy后，P和R的计算方式</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/3eB784FE2d.jpg" alt=""></p><p>Value Function：在MDP中有两种value function</p><p><strong>state-value function</strong>——vπ(s)，状态s的value function。在遵循policy的前提下，从状态s算起的未来期望收益</p><p>The state-value function vπ(s) of an MDP is the expected return starting from state s, and then following policy pi</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/ce9ad85gLD.jpg" alt=""></p><p><strong>action-value function</strong>——qπ(s, a)，在状态s处执行action a的value function。在遵循policy的前提下，在状态s处执行action a后算起的未来期望收益</p><p>The action-value function qπ(s, a) is the expected return starting from state s, taking action a, and then following policy pi</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/F5iH7582Ag.jpg" alt=""></p><p>v是针对state而言，q是针对action而言</p><p>example：</p><p>计算方法见Bellman Expectation Equation部分</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/1DKibehlEc.jpg" alt=""></p><p><strong>Bellman Expectation Equation</strong>：类似于MRP中的Bellman Equation，vpi(s)和qpi(s, a)也可以写成类似的表达式</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/mI4ECD0kJe.jpg" alt=""></p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/maf91FD5iL.jpg" alt=""></p><p>类似的，也可以用树状图来表示其计算过程</p><p>在state s处，有两种action的选择，每种action有自己的value function，所以state s的value function就等于两种action的value function的加权和。</p><p>而选好一种action后，可能会跳转到的下一state有两种，所以action a的value function就等于两个下一state的value function的加权和。</p><table><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/haddhfgmLh.jpg" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/m6eEiJ4c4I.jpg" alt=""></td></tr></table><p>把上面的树形结构链接起来后就是下面这个样子，如果要求state s的value function（也就是根结点是白点），那么如图左；如果要求action a的value function（也就是根结点是黑点），那么如图右。</p><table><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/kaglF63HfB.jpg" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/fJmf4l7Elc.jpg" alt=""></td></tr></table><p>example：</p><p>在MDP中vpi(s)的计算过程如下，假设选取每种action的概率相同</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/LhHCEmEEIK.jpg" alt=""></p><p>当然vpi(s)的计算还是可以用矩阵来表示的</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/L8f4KI62DE.jpg" alt=""></p><p>direct solution：</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/7i5c464i1A.jpg" alt=""></p><p>当然，在实际中我们的目标是寻求一个最优解，在state处选择哪个action收益最好，所以就有了以下的内容</p><p>Optimal Value Function：</p><p><strong>optimal state-value function</strong>——v* (s)，在所有的policy中，选出最大的vpi(s)<br>The optimal state-value function v*(s) is the maximum value function over all policies</p><p><strong>optimal action-value function</strong>——q* (s, a)，在所有的policy中，选出最大的qpi(s, a)<br>The optimal action-value function q*(s, a) is the maximum action-value function over all policies</p><p>OVF代表着MDP中最优的表现（收益）。如果我们知道了OVF，也就意味着这个MDP是solved</p><p>example：</p><p>右边那个图pub的q* 应该是9.4 = 1 + (0.2 x 6 + 0.4 x 8 + 0.4 x 10)</p><table><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/BE6Ljme1Ck.jpg" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/Dmc4ibc94a.jpg" alt=""></td></tr></table><p><strong>Optimal Policy</strong>：</p><p>先定义一下policy的好坏，如下，pi好于pi’</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/Ge7lehFkCd.jpg" alt=""></p><p>比较重要的是在任意一个MDP中，肯定会有一个optimal policy pi*</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/G82cILLLE5.jpg" alt=""></p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/AhbDKjh5Jf.jpg" alt=""></p><p>example：</p><p>红色的箭头就代表了optimal policy</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/E19k621JI0.jpg" alt=""></p><p><strong>Bellman Optimality Equation</strong>：</p><p>依旧用树状图来表示计算过程</p><table><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/lD95b50FCc.jpg" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/gHaD9gf6e1.jpg" alt=""></td></tr><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/1E5eA59KiC.jpg" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/b757mj0CDh.jpg" alt=""></td></tr></table><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/IHe7BDjEFD.jpg" alt=""></p><p>既然要找到MDP的最优解，那么就要解Bellman Optimality Equation</p><p>因为存在max的计算过程，所以BOE是一个非线性方程，通常没有什么直接解法，但是可以用一些迭代方法来解</p><ul><li>Value Iteration</li><li>Policy Iteration</li><li>Q-learning</li><li>Sarsa</li></ul><h2 id="4、Extensions-to-MDPs"><a href="#4、Extensions-to-MDPs" class="headerlink" title="4、Extensions to MDPs"></a>4、Extensions to MDPs</h2><ul><li>Infinite and continuous MDPs</li><li>Partially observable MDPs</li><li>Undiscounted, average reward MDPs</li></ul><p>思考：<br>episode是sequence</p><p>为什么用discount，因为我们没有一个perfect model，我们对未来做的决定不是完全相信，不能确定是百分百正确的，所以因为这种不完美不确定性，用discount来减少对现在的影响</p><p>alphaGo 考虑几步之后，这不就是考虑未来的reward，用gamma来控制考虑几步之后</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是在学习David Silver所教授的Reinforcement learning课程过程中所记录的笔记。因为个人知识的不足以及全程啃生肉，难免会有理解偏差的地方，欢迎一起交流。&lt;/p&gt;
&lt;p&gt;课程资料：&lt;a href=&quot;http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;1、Markov-Processes&quot;&gt;&lt;a href=&quot;#1、Markov-Processes&quot; class=&quot;headerlink&quot; title=&quot;1、Markov Processes&quot;&gt;&lt;/a&gt;1、Markov Processes&lt;/h2&gt;&lt;p&gt;在RL中，&lt;strong&gt;MDP是用来描述environment的，并且假设environment是full observable的&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;i.e. The current state completely characterizes the process&lt;/p&gt;
&lt;p&gt;许多RL问题可以用MDP来表示：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Optimal control primarily deals with continuous MDPs&lt;/li&gt;
&lt;li&gt;Partially observable problems can be converted into MDPs&lt;/li&gt;
&lt;li&gt;Bandits are MDPs with one state&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="笔记" scheme="https://bluesmilery.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement Learning" scheme="https://bluesmilery.github.io/tags/Reinforcement-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习 Machine learning part 1 - Linear Regression</title>
    <link href="https://bluesmilery.github.io/blogs/18a3f212/"/>
    <id>https://bluesmilery.github.io/blogs/18a3f212/</id>
    <published>2017-03-20T03:36:02.000Z</published>
    <updated>2018-01-31T03:37:12.741Z</updated>
    
    <content type="html"><![CDATA[<p>本文是在学习Andrew Ng所教授的Machine learning课程过程中所记录的笔记。因为个人知识的不足以及英文教学，难免会有理解偏差的地方，欢迎一起交流。</p><p>课程资料：<a href="https://www.coursera.org/learn/machine-learning" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning</a></p><p>Machine learning 主要分为两类：</p><p><strong>Supervised Learning</strong>：regression problem、classification problem</p><p>例子：房价估计，良性恶性肿瘤判断</p><p>supervised learning:”right answers” given</p><ul><li>regression: predict continuous valued output(price)</li><li>classification: discrete valued output(0 or 1)</li></ul><p><strong>Unsupervised Learning</strong>：clustering algorithm</p><p>例子：谷歌新闻，基因，organize computing clusters，social network analysis，market segmentation，astronomical data analysis，cocktail party problem（录音分辨）</p><a id="more"></a><p>编程作业工具：octave\matlab</p><hr><p>第一周测验，这道题总是错</p><p>A computer program is said to learn from experience E with respect to some task T and some performance measure P if its performance on T, as measured by P, improves with experience E. Suppose we feed a learning algorithm a lot of historical weather data, and have it learn to predict weather. What would be a reasonable choice for P?</p><p>这是这节课wiki的地址： <a href="https://share.coursera.org/wiki/index.php/ML:Main" target="_blank" rel="noopener">https://share.coursera.org/wiki/index.php/ML:Main</a>.</p><p>我在这上面找到了答案</p><p>Two definitions of Machine Learning are offered. Arthur Samuel described it as: “the field of study that gives computers the ability to learn without being explicitly programmed.” This is an older, informal definition.</p><p>Tom Mitchell provides a more modern definition: “A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.”</p><p>Example: playing checkers.</p><ul><li>E = the experience of playing many games of checkers</li><li>T = the task of playing checkers.</li><li>P = the probability that the program will win the next game.</li></ul><hr><h2 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h2><p>regression problem：<strong>用training set来进行训练</strong>。</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/180F9D3liD.png" alt=""></p><p><strong>Linear regression model：</strong></p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/747fe651B4.jpg" alt=""></p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/Gcegg8Hhed.jpg" alt=""></p><p>求h的思路：idea</p><p><strong>J函数被称为cost function，用于确定h中的θ</strong></p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/m4Jd2c2BBI.jpg" alt=""></p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/J2k9EfCAJd.jpg" alt=""></p><p><strong>h与J的关系，只有θ1</strong></p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/5c27k5Kejm.jpg" alt=""></p><p><strong>h与J的关系，有θ0和θ1</strong>，右边的叫做contour plot等高线图</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/Li2L8fbik4.jpg" alt=""></p><h2 id="An-algorithm-gradient-descent-梯度下降"><a href="#An-algorithm-gradient-descent-梯度下降" class="headerlink" title="An algorithm: gradient descent 梯度下降"></a>An algorithm: gradient descent 梯度下降</h2><p>For minimizing the cost function J.</p><p>gradient descent不止用于最小化J函数，它是一个非常通用的算法。</p><p>可以把这个算法想象成你在一个山上，要下山，要走下山最快的路</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/Cge5LAccF9.jpg" alt=""></p><h2 id="Multivariate-linear-regression多元线性回归-linear-regression-with-multiple-variables"><a href="#Multivariate-linear-regression多元线性回归-linear-regression-with-multiple-variables" class="headerlink" title="Multivariate linear regression多元线性回归(linear regression with multiple variables)"></a>Multivariate linear regression多元线性回归(linear regression with multiple variables)</h2><p>此时有多个变量，而不只有面积着一个变量。</p><p>注意x上标和下标的意思。上标：第几个training example；下标：第几个变量</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/Id89GIieiI.jpg" alt=""></p><p><strong>此时的hypothesis</strong>：<img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/gBAFhbd06E.jpg" alt="">，对其进行线性代数化简</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/6GI3h2HKh3.jpg" alt=""></p><p><strong>多元变量下的gradient descent</strong></p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/BmAmIAFA2i.jpg" alt=""></p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/96I82lf9G2.jpg" alt=""></p><h2 id="梯度下降运算中的使用技巧"><a href="#梯度下降运算中的使用技巧" class="headerlink" title="梯度下降运算中的使用技巧"></a>梯度下降运算中的使用技巧</h2><h5 id="1）feature-scaling-特征缩放"><a href="#1）feature-scaling-特征缩放" class="headerlink" title="1）feature scaling 特征缩放"></a>1）feature scaling 特征缩放</h5><p>如果不同变量（feature，就是那些参数）的取值范围差的很多，（假如只有两个feature）那么画出来的等高线图会特别椭圆，这个时候gradient descent下降的会很慢，θ收敛的很慢。下面为比较</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/34D2IHAm9I.jpg" alt=""></p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/83BGhH7LL8.jpg" alt=""></p><p><strong>特征缩放一般有两种方式</strong></p><ul><li>一种是除以范围绝对值的最大值，使其范围在－1到＋1之间。（不用严格满足，比如－3 to 3或者－0.3 to 0.3都可以，只要差的不是太多，并且所有feature的范围接近即可）</li></ul><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/i4H8dlH5lf.jpg" alt=""></p><ul><li>另一种方法叫做mean normalization均值归一化</li></ul><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/0IIm3me9e5.jpg" alt=""></p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/i5IFKFj8bl.jpg" alt=""></p><h5 id="2）learning-rate－α的选择"><a href="#2）learning-rate－α的选择" class="headerlink" title="2）learning rate－α的选择"></a>2）learning rate－α的选择</h5><p>对于gradient descent：</p><ul><li>“debugging”:how to make sure gradient descent is working correctly.</li><li>how to choose learning rate α</li></ul><p>对于每一次迭代，J函数都应该下降</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/cdk77J3B7e.jpg" alt=""></p><p>可以用自动收敛测试，比如每次J函数下降小于1e-3，但是不推荐，因为选阀值很难。</p><p>以下这些情况都要选择更小的α</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/h8IKD6542a.jpg" alt=""></p><p><strong>总结：α太小，收敛慢；α太大，J不收敛</strong></p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/DCbKB08c6A.jpg" alt=""></p><p>吴恩达选择α的方式：三倍一选</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/A0k0dkbI8l.jpg" alt=""></p><h2 id="features-and-polynomial-regression"><a href="#features-and-polynomial-regression" class="headerlink" title="features and polynomial regression"></a>features and polynomial regression</h2><p>合理选择feature（变量）能有效降低假设的复杂度。比如有两个feature房屋的长度和宽度，可以组合成一个feature面积。</p><p>有时候也可以选择多项式回归，使得model更适合data。</p><p>至于怎么选择后面的课会讲。</p><h2 id="normal-equation："><a href="#normal-equation：" class="headerlink" title="normal equation："></a>normal equation：</h2><p>目前线性回归的算法有</p><ol><li><strong>梯度下降法</strong>，多次迭代逐渐收敛到J函数的全局最小值。</li><li><strong>normal equation</strong>，method to solve for θ analytically。解析解法，一次求出θ最优值</li></ol><h2 id="向量化："><a href="#向量化：" class="headerlink" title="向量化："></a>向量化：</h2><p>编程的时候向量化，可以让运算速度更快，效率更高</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/FmC13ik71I.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是在学习Andrew Ng所教授的Machine learning课程过程中所记录的笔记。因为个人知识的不足以及英文教学，难免会有理解偏差的地方，欢迎一起交流。&lt;/p&gt;
&lt;p&gt;课程资料：&lt;a href=&quot;https://www.coursera.org/learn/machine-learning&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/machine-learning&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Machine learning 主要分为两类：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Supervised Learning&lt;/strong&gt;：regression problem、classification problem&lt;/p&gt;
&lt;p&gt;例子：房价估计，良性恶性肿瘤判断&lt;/p&gt;
&lt;p&gt;supervised learning:”right answers” given&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;regression: predict continuous valued output(price)&lt;/li&gt;
&lt;li&gt;classification: discrete valued output(0 or 1)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Unsupervised Learning&lt;/strong&gt;：clustering algorithm&lt;/p&gt;
&lt;p&gt;例子：谷歌新闻，基因，organize computing clusters，social network analysis，market segmentation，astronomical data analysis，cocktail party problem（录音分辨）&lt;/p&gt;
    
    </summary>
    
      <category term="笔记" scheme="https://bluesmilery.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Machine Learning" scheme="https://bluesmilery.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>增强学习 Reinforcement learning part 1 - Introduction</title>
    <link href="https://bluesmilery.github.io/blogs/481fe3af/"/>
    <id>https://bluesmilery.github.io/blogs/481fe3af/</id>
    <published>2017-03-20T01:24:02.000Z</published>
    <updated>2018-01-31T03:44:45.675Z</updated>
    
    <content type="html"><![CDATA[<p>本文是在学习David Silver所教授的Reinforcement learning课程过程中所记录的笔记。因为个人知识的不足以及全程啃生肉，难免会有理解偏差的地方，欢迎一起交流。</p><p>课程资料：<a href="http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html" target="_blank" rel="noopener">http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html</a></p><p>关于 Reinforcement learning的两本参考：</p><p>An Introduction to Reinforcement Learning</p><p><a href="https://webdocs.cs.ualberta.ca/~sutton/book/the-book-1st.html" target="_blank" rel="noopener">https://webdocs.cs.ualberta.ca/~sutton/book/the-book-1st.html</a></p><p>Algorithms for Reinforcement Learning</p><p><a href="https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf" target="_blank" rel="noopener">https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf</a></p><a id="more"></a><h2 id="1、About-Reinforcement-Learning"><a href="#1、About-Reinforcement-Learning" class="headerlink" title="1、About Reinforcement Learning"></a>1、About Reinforcement Learning</h2><p>Many Faces of Reinforcement Learning</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/Hcm8Hgd1m0.jpg" alt=""></p><p>Machine Learningd的三个分支：Supervised Learning、Unsupervised Learning、Reinforcement Learning</p><p>RL与其他两种的区别：</p><ul><li>There is no supervisor, only <strong>a reward signal</strong></li><li><strong>Feedback is delayed</strong>, not instantaneous</li><li><strong>Time</strong> really matters (sequential, non i.i.d data)</li><li>Agent’s <strong>actions affect the subsequent data</strong> it receives</li></ul><h2 id="2、The-Reinforcement-Learning-Problem"><a href="#2、The-Reinforcement-Learning-Problem" class="headerlink" title="2、The Reinforcement Learning Problem"></a>2、The Reinforcement Learning Problem</h2><p>介绍三个概念：reward、environment、state</p><p>==<strong>reward</strong>==</p><p>用Rt来表示reward（标量，就是个’数’），衡量在第t步agent表现的好坏（收益），agent的目标就是最大化累计reward</p><p>Reinforcement learning is based on the <strong>reward hypothesis</strong>（All goals can be described by the maximisation of expected cumulative reward）</p><p>简而言之就是假设所有的目标都可以用最大化累计收益来表示</p><p>example：</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/B0amGIICH0.png" alt=""></p><p>Sequential Decision Making</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/8dk3DCkdf9.png" alt=""></p><p>==<strong>environment</strong>==</p><p>agent与environment的关系，agent执行action影响environment，environment给agent关于observation和reward的反馈</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/7aI7kG3Did.jpg" alt=""></p><p>==<strong>state</strong>==</p><p><em>history</em>：在第t步之前的observation、reward、action。注意没有At，因为agent是基于observation和reward来选择action，在选择action之前的这个时间点，在此之前的都算是过去</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/336jfc5JGh.png" alt=""></p><p><em>state</em>：is the information used to determine what happens next。就是说我利用了history中某些信息来判断接下来会发生什么，所利用的这些信息就被称为state</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/bJA5FC9L2h.png" alt=""></p><p>这里what happens next分为两部分</p><ul><li>The agent selects actions。agent会选择什么action</li><li>The environment selects observations/rewards。environment会给出什么observation/reward</li></ul><p>state又分为environment state、agent state、information state</p><p><em>environment state</em>：Ste is the environment’s private representation。对于agent而言一般是invisible的，就算是visible，那也包含不相关的信息</p><p><em>agent state</em>：Sta is the agent’s internal representation。It can be any function of history</p><p><em>information state</em>：又被称为markov state。所以具有’The future is independent of the past given the present’</p><p>The environment state and the history are Markov</p><p>例子：每一行为一次过程，第一次灯亮灯亮，老鼠按下开关，然后铃响，结果老鼠遭到电击。第二次先铃响灯亮，然后老鼠两次按下开关，结果老鼠得到奶酪。第三次老鼠先按下开关，然后灯亮，然后老鼠又按下开关，然后铃响，猜测老鼠会得到什么？</p><p>分析：如果agent state是利用最后三个动作的顺序，那么老鼠会遭到电击。如果agent state是利用灯亮铃响按下开关的次数，那么老鼠会得到奶酪。如果agent state是利用整个序列，那我们也不知道会发生什么</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/mfafgjBcBd.jpg" alt=""></p><p><em>Fully Observable Environments</em>：agent <strong>directly</strong> observes environment state。这种被称为<strong>Markov decision process</strong> (MDP)</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/kDKG917gjc.png" alt=""></p><p>agent state = environment state = information state</p><p><em>Partially Observable Environments</em>：agent <strong>indirectly</strong> observes environment。这种被称为<strong>partially observable Markov decision process</strong>（POMDP）</p><ul><li>A robot with camera vision isn’t told its absolute location</li><li>A trading agent only observes current prices</li><li>A poker playing agent only observes public cards</li></ul><p>agent state 不等于 environment state</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/ejAcm539ga.png" alt=""></p><h2 id="3、Inside-An-Reinforcement-Learning-Agent"><a href="#3、Inside-An-Reinforcement-Learning-Agent" class="headerlink" title="3、Inside An Reinforcement Learning Agent"></a>3、Inside An Reinforcement Learning Agent</h2><p>agent的三要素</p><ul><li>Policy：agent采取的行为策略（behaviour function）</li><li>Value Function：评估state/action的好坏</li><li>Model：agent对environment所构建的模型（在agent眼中environment的样子）</li></ul><p>==<strong>Policy</strong>==：agent的策略，也就是agent在某个状态会采取什么样的行动，所以policy is a map from state to action</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/3bcFHGCgCL.png" alt=""></p><p>==<strong>Value Function</strong>==：是对未来收益的一个预测，用来评估状态的好坏程度</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/Cj20H41d1d.png" alt=""></p><p>其中gamma是discounted系数，表示了未来的收益对现在的影响，越远的影响越小。比如gamma是0.9，那么这个预测的时间跨度大约是未来三四十步</p><p>example：左上角那个是state value function，游戏画面上有一个紫色的，那个是mothership，击落的分数奖励更高，所以当mothership从右边出现后，对未来收益的预测增加，从而value function的值开始上升。当mothership从眼前过去后，不管打没打中，value function都会陡然下降，因为后面都是小兵，所以对未来收益的预期也就回到了一般水平。</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/3F4CBI33g8.jpg" alt=""></p><p>还有个打砖块的例子，越靠上面的砖块分数越高，所以在游戏刚开始的时候value function比较平滑，当下面的打了好多以后，打到更深的砖块的概率上升，所以value function的波动增加了。</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/hgbl3iHKeh.jpg" alt=""></p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/aLcfGI54cc.jpg" alt=""></p><p>==<strong>Model</strong>==：agent对environment构建的模型，用来预测environment下一步会干什么（会跳转到哪个state，会给出什么reward）</p><p>P predicts the next state</p><p>R predicts the next (immediate) reward</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/DkLL2HD7Ed.png" alt=""></p><p>基于上面三要素，RL agent有以下几种分类</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/kcife76K2j.jpg" alt=""></p><h2 id="4、Problems-within-Reinforcement-Learning"><a href="#4、Problems-within-Reinforcement-Learning" class="headerlink" title="4、Problems within Reinforcement Learning"></a>4、Problems within Reinforcement Learning</h2><p>==<strong>Learning and Planning</strong>==</p><p>Two fundamental problems in sequential decision making</p><ul><li>Reinforcement Learning:<ul><li><strong>The environment is initially unknown</strong></li><li>The agent interacts with the environment</li><li>The agent improves its policy</li></ul></li><li>Planning:<ul><li><strong>A model of the environment is known</strong></li><li>The agent performs computations with its model (without any external interaction)</li><li>The agent improves its policy</li><li>a.k.a. deliberation, reasoning, introspection, pondering, thought, search</li></ul></li></ul><p>Reinforcement Learning的例子：游戏的机制不清楚，只能通过玩来学习，通过观察得分与游戏画面来选择下一步行动</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/9e9a5184Em.jpg" alt=""></p><p>Planning的例子：游戏机制很清楚，下一步是什么样子的都知道，有完整的策略（就像是玩游戏有攻略一样）</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/KK1aE0KC7f.png" alt=""></p><p>==<strong>Exploration and Exploitation</strong>==</p><p>Reinforcement learning is like trial-and-error learning</p><ul><li>Exploration：探索，更多的去探索environment的信息</li><li>Exploitation：利用，更多的利用已知的environment信息来最大化reward</li></ul><p>举个例子，吃饭选择餐厅，exploration是选择一个新餐厅，exploitation是选择自己平时最喜欢吃的餐厅</p><p>==<strong>Prediction and Control</strong>==</p><ul><li>Prediction：估计未来的收益，given a policy</li><li>Control：最优化未来的收益，find the best policy</li></ul><p>Gridworld Example，没看懂</p><p>后记：有了一些理解，如果移动到A的话那么就会跳转到A’，并且reward +10，如果移动到B的话那么就会跳转到B’，并且reward + 5</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/cBJiHjmgfg.png" alt=""></p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/HJ2bcbljlB.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是在学习David Silver所教授的Reinforcement learning课程过程中所记录的笔记。因为个人知识的不足以及全程啃生肉，难免会有理解偏差的地方，欢迎一起交流。&lt;/p&gt;
&lt;p&gt;课程资料：&lt;a href=&quot;http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;关于 Reinforcement learning的两本参考：&lt;/p&gt;
&lt;p&gt;An Introduction to Reinforcement Learning&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://webdocs.cs.ualberta.ca/~sutton/book/the-book-1st.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://webdocs.cs.ualberta.ca/~sutton/book/the-book-1st.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Algorithms for Reinforcement Learning&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="笔记" scheme="https://bluesmilery.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement Learning" scheme="https://bluesmilery.github.io/tags/Reinforcement-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习系统环境配置指南 —— GTX 1080 + Ubuntu16.04 + CUDA8 + cuDNN5.1 + TensorFlow</title>
    <link href="https://bluesmilery.github.io/blogs/9a018dfc/"/>
    <id>https://bluesmilery.github.io/blogs/9a018dfc/</id>
    <published>2017-03-15T12:51:45.000Z</published>
    <updated>2018-01-31T03:43:30.758Z</updated>
    
    <content type="html"><![CDATA[<p>最近开始学习机器学习，所以需要配一台电脑。本文主要写的是系统环境配置的内容，依据前人经验总结自己的安装过程，希望可以给大家一个参考。</p><p>主机配置：i7-6700 + 24G内存 + GTX 1080</p><p>系统环境配置：</p><ul><li>Ubuntu 16.04 LTS 64位</li><li>CUDA 8.0</li><li>cuDNN v5.1</li><li>TensorFlow v0.12.0 RC1</li><li>Python 2.7</li><li>Bazel 0.4.2</li></ul><p>在整个环境配置过程中，有许多东西可以提前下载好，在配置时便可以节省时间了。</p><ul><li><a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="noopener">CUDA 8.0</a> (1.4GB)：Linux &gt; x86_64 &gt; Ubuntu &gt; 16.04 &gt; runfile(local)</li><li><a href="https://developer.nvidia.com/rdp/cudnn-download" target="_blank" rel="noopener">cuDNN v5.1</a> (100MB)： 需要注册Nvidia开发者账号，Download cuDNN v5.1 (August 10, 2016), for CUDA 8.0 &gt; cuDNN v5.1 Library for Linux。最好在Linux系统下下载，格式为.tgz。在Windows下下载的格式会识别成.solitairetheme8格式。</li><li><a href="https://github.com/tensorflow/tensorflow/releases" target="_blank" rel="noopener">TensorFlow 源码release版</a> (10MB+)：下载v0.12.0 RC1，zip或者tar.gz均可</li><li><a href="https://github.com/tensorflow/tensorflow" target="_blank" rel="noopener">TensorFlow pip安装包</a> (CPU版40MB+，GPU版80MB+)：选择Linux和Python2的版本，CPU和GPU的都下。pip安装包只会下载最新版本</li><li><a href="https://github.com/bazelbuild/bazel/releases" target="_blank" rel="noopener">Bazel 源码</a> (100MB+)：下载0.4.2版本，选择bazel-0.4.2-installer-linux-x86_64.sh</li></ul><a id="more"></a><h2 id="一、安装Nvidia显卡驱动（GTX-1080）"><a href="#一、安装Nvidia显卡驱动（GTX-1080）" class="headerlink" title="一、安装Nvidia显卡驱动（GTX 1080）"></a>一、安装Nvidia显卡驱动（GTX 1080）</h2><p>1、安装完Ubuntu16.04系统后，第一次进入系统分辨率很低，所以先简单修改一下分辨率。</p><p>打开terminal，执行：<code>sudo gedit /etc/default/grub</code></p><p>有一行内容是 #GRUB_GFXMODE=640x480，然后把#号去掉，后面的640x480改为1024x768。</p><p>然后执行：<code>sudo update-grub</code></p><p>重启电脑后，看起来比刚才舒服一些了</p><p>2、更新软件源，这里用的是中科大的源</p><p><code>cd /etc/apt/</code></p><p><code>sudo cp sources.list sources.list.bak</code></p><p><code>sudo gedit sources.list</code></p><p>把下面的内容添加到sources.list文件头部：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">deb http://mirrors.ustc.edu.cn/ubuntu/ xenial main restricted universe multiverse</span><br><span class="line">deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-security main restricted universe multiverse</span><br><span class="line">deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse</span><br><span class="line">deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse</span><br><span class="line">deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial-security main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse</span><br></pre></td></tr></table></figure><p>然后更新源和更新已安装的包</p><p><code>sudo apt-get update</code></p><p><code>sudo apt-get upgrade</code></p><p>3、安装Nvidia显卡驱动</p><p>先添加Ubuntu社区建立的Graphics Drivers PPA</p><p><code>sudo add-apt-repository ppa:graphics-drivers/ppa</code></p><p>出现一系列内容后，回车后继续</p><p><code>sudo apt-get update</code></p><p>此时，有两种方式可以安装驱动</p><p>1）第一种是进入System Settings &gt; Software&amp;Updates &gt; Additional Drivers，然后选择想安装的驱动，然后点apply，等待系统提醒重启系统即可</p><p>2）第二种是看一下软件源中有哪些Nvidia驱动</p><p><code>sudo apt-cache search nvidia</code></p><p>我选择安装最新的驱动版本375</p><p><code>sudo apt-get install nvidia-375</code></p><p>重启电脑后驱动生效</p><p>可以执行：<code>nvidia-smi</code>来查看信息，或者执行：<code>nvidia-settings</code>查看更详细的信息</p><h2 id="二、安装CUDA8-0"><a href="#二、安装CUDA8-0" class="headerlink" title="二、安装CUDA8.0"></a>二、安装CUDA8.0</h2><p>1、先去Nvidia开发者网站下载<a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="noopener">CUDA8.0</a><br><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/L8H4mIGh9L.png" alt=""></p><p>下载好后，执行：<code>sudo sh cuda_8.0.44_linux.run</code>进行安装。如果出现了提示空间不足，那么执行：<code>sudo sh cuda_8.0.27_linux.run --tmpdir=/opt/temp/</code></p><p>执行后会有一系列提示需要确认，其中，询问是否安装Nvidia显卡驱动的时候选n，因为我们之前已经装过了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">Do you accept the previously read EULA?</span><br><span class="line">accept/decline/quit:               accept</span><br><span class="line"></span><br><span class="line">Install NVIDIA Accelerated Graphics Driver for Linux-x86_64 367.48?</span><br><span class="line">(y)es/(n)o/(q)uit: n</span><br><span class="line"></span><br><span class="line">Install the CUDA 8.0 Toolkit?</span><br><span class="line">(y)es/(n)o/(q)uit: y</span><br><span class="line"></span><br><span class="line">Enter Toolkit Location</span><br><span class="line"> [ default is /usr/local/cuda-8.0 ]:</span><br><span class="line"></span><br><span class="line">Do you want to install a symbolic link at /usr/local/cuda?</span><br><span class="line">(y)es/(n)o/(q)uit: y</span><br><span class="line"></span><br><span class="line">Install the CUDA 8.0 Samples?</span><br><span class="line">(y)es/(n)o/(q)uit: y</span><br><span class="line"></span><br><span class="line">Enter CUDA Samples Location</span><br><span class="line"> [ default is /home/gai ]:</span><br><span class="line"></span><br><span class="line">Installing the CUDA Toolkit in /usr/local/cuda-8.0 ...</span><br><span class="line">Missing recommended library: libGLU.so</span><br><span class="line">Missing recommended library: libX11.so</span><br><span class="line">Missing recommended library: libXi.so</span><br><span class="line">Missing recommended library: libXmu.so</span><br><span class="line"></span><br><span class="line">Installing the CUDA Samples in /home/gai ...</span><br><span class="line">Copying samples to /home/gai/NVIDIA_CUDA-8.0_Samples now...</span><br><span class="line">Finished copying samples.</span><br><span class="line"></span><br><span class="line">===========</span><br><span class="line">= Summary =</span><br><span class="line">===========</span><br><span class="line"></span><br><span class="line">Driver:   Not Selected</span><br><span class="line">Toolkit:  Installed in /usr/local/cuda-8.0</span><br><span class="line">Samples:  Installed in /home/gai, but missing recommended libraries</span><br><span class="line"></span><br><span class="line">Please make sure that</span><br><span class="line"> -   PATH includes /usr/local/cuda-8.0/bin</span><br><span class="line"> -   LD_LIBRARY_PATH includes /usr/local/cuda-8.0/lib64, or, add /usr/local/cuda-8.0/lib64 to /etc/ld.so.conf and run ldconfig as root</span><br><span class="line"></span><br><span class="line">To uninstall the CUDA Toolkit, run the uninstall script in /usr/local/cuda-8.0/bin</span><br><span class="line"></span><br><span class="line">Please see CUDA_Installation_Guide_Linux.pdf in /usr/local/cuda-8.0/doc/pdf for detailed information on setting up CUDA.</span><br><span class="line"></span><br><span class="line">***WARNING: Incomplete installation! This installation did not install the CUDA Driver. A driver of version at least 361.00 is required for CUDA 8.0 functionality to work.</span><br><span class="line">To install the driver using this installer, run the following command, replacing &lt;CudaInstaller&gt; with the name of this run file:</span><br><span class="line">    sudo &lt;CudaInstaller&gt;.run -silent -driver</span><br><span class="line"></span><br><span class="line">Logfile is /tmp/cuda_install_6100.log</span><br></pre></td></tr></table></figure><p>可以发现系统提示缺少一些推荐安装的库：libGLU.so、libX11.so、libXi.so、libXmu.so</p><p>使用<a href="http://packages.ubuntu.com/" target="_blank" rel="noopener">Ubuntu Packages Search</a>进行搜索，在Search the contents of packages处分别键入以上四个库，可以发现需要安装以下软件包：libglu1-mesa-dev、libx11-dev、libxi-dev、libxmu-dev</p><p>所以执行：<code>sudo apt-get install libglu1-mesa-dev libx11-dev libxi-dev libxmu-dev</code></p><p>再参考下<a href="http://docs.nvidia.com/cuda/cuda-installation-guide-linux/#mandatory-post" target="_blank" rel="noopener">官方的安装指南</a>，发现还要配置环境变量，在home目录下执行：<code>sudo gedit .bashrc</code>，然后在文件末尾添加上下面两行内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export PATH=/usr/local/cuda-8.0/bin$&#123;PATH:+:$&#123;PATH&#125;&#125;</span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;</span><br></pre></td></tr></table></figure><p>然后执行：<code>source ~/.bashrc</code>更新一下</p><p>2、测试CUDA</p><p>测试几个官方CUDA的例子</p><p>在CUDA例子的1_Utilities/deviceQuery目录下执行：<code>make</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&quot;/usr/local/cuda-8.0&quot;/bin/nvcc -ccbin g++ -I../../common/inc  -m64    -gencode arch=compute_20,code=sm_20 -gencode arch=compute_30,code=sm_30 -gencode arch=compute_35,code=sm_35 -gencode arch=compute_37,code=sm_37 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_60,code=compute_60 -o deviceQuery.o -c deviceQuery.cpp</span><br><span class="line">nvcc warning : The &apos;compute_20&apos;, &apos;sm_20&apos;, and &apos;sm_21&apos; architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).</span><br><span class="line">&quot;/usr/local/cuda-8.0&quot;/bin/nvcc -ccbin g++   -m64      -gencode arch=compute_20,code=sm_20 -gencode arch=compute_30,code=sm_30 -gencode arch=compute_35,code=sm_35 -gencode arch=compute_37,code=sm_37 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_60,code=compute_60 -o deviceQuery deviceQuery.o</span><br><span class="line">nvcc warning : The &apos;compute_20&apos;, &apos;sm_20&apos;, and &apos;sm_21&apos; architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).</span><br><span class="line">mkdir -p ../../bin/x86_64/linux/release</span><br><span class="line">cp deviceQuery ../../bin/x86_64/linux/release</span><br></pre></td></tr></table></figure><p>编译完成，然后执行：<code>./deviceQuery</code>，会得到如下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">./deviceQuery Starting...</span><br><span class="line"></span><br><span class="line"> CUDA Device Query (Runtime API) version (CUDART static linking)</span><br><span class="line"></span><br><span class="line">Detected 1 CUDA Capable device(s)</span><br><span class="line"></span><br><span class="line">Device 0: &quot;GeForce GTX 1080&quot;</span><br><span class="line">  CUDA Driver Version / Runtime Version          8.0 / 8.0</span><br><span class="line">  CUDA Capability Major/Minor version number:    6.1</span><br><span class="line">  Total amount of global memory:                 8110 MBytes (8504279040 bytes)</span><br><span class="line">  (20) Multiprocessors, (128) CUDA Cores/MP:     2560 CUDA Cores</span><br><span class="line">  GPU Max Clock rate:                            1810 MHz (1.81 GHz)</span><br><span class="line">  Memory Clock rate:                             5005 Mhz</span><br><span class="line">  Memory Bus Width:                              256-bit</span><br><span class="line">  L2 Cache Size:                                 2097152 bytes</span><br><span class="line">  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)</span><br><span class="line">  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers</span><br><span class="line">  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers</span><br><span class="line">  Total amount of constant memory:               65536 bytes</span><br><span class="line">  Total amount of shared memory per block:       49152 bytes</span><br><span class="line">  Total number of registers available per block: 65536</span><br><span class="line">  Warp size:                                     32</span><br><span class="line">  Maximum number of threads per multiprocessor:  2048</span><br><span class="line">  Maximum number of threads per block:           1024</span><br><span class="line">  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)</span><br><span class="line">  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)</span><br><span class="line">  Maximum memory pitch:                          2147483647 bytes</span><br><span class="line">  Texture alignment:                             512 bytes</span><br><span class="line">  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)</span><br><span class="line">  Run time limit on kernels:                     Yes</span><br><span class="line">  Integrated GPU sharing Host Memory:            No</span><br><span class="line">  Support host page-locked memory mapping:       Yes</span><br><span class="line">  Alignment requirement for Surfaces:            Yes</span><br><span class="line">  Device has ECC support:                        Disabled</span><br><span class="line">  Device supports Unified Addressing (UVA):      Yes</span><br><span class="line">  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0</span><br><span class="line">  Compute Mode:</span><br><span class="line">     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;</span><br><span class="line"></span><br><span class="line">deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 8.0, CUDA Runtime Version = 8.0, NumDevs = 1, Device0 = GeForce GTX 1080</span><br><span class="line">Result = PASS</span><br></pre></td></tr></table></figure><p>再测试另外一个例子，在5_Simulations/nbody目录下执行<code>make</code>，如果提示cannot find -lglut，那么需要执行下：<code>sudo apt-get install freeglut3-dev</code></p><p>编译完成后执行：<code>./nbody -benchmark -numbodies=256000 -device=0</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">Run &quot;nbody -benchmark [-numbodies=&lt;numBodies&gt;]&quot; to measure performance.</span><br><span class="line">    -fullscreen       (run n-body simulation in fullscreen mode)</span><br><span class="line">    -fp64             (use double precision floating point values for simulation)</span><br><span class="line">    -hostmem          (stores simulation data in host memory)</span><br><span class="line">    -benchmark        (run benchmark to measure performance)</span><br><span class="line">    -numbodies=&lt;N&gt;    (number of bodies (&gt;= 1) to run in simulation)</span><br><span class="line">    -device=&lt;d&gt;       (where d=0,1,2.... for the CUDA device to use)</span><br><span class="line">    -numdevices=&lt;i&gt;   (where i=(number of CUDA devices &gt; 0) to use for simulation)</span><br><span class="line">    -compare          (compares simulation results running once on the default GPU and once on the CPU)</span><br><span class="line">    -cpu              (run n-body simulation on the CPU)</span><br><span class="line">    -tipsy=&lt;file.bin&gt; (load a tipsy model file for simulation)</span><br><span class="line"></span><br><span class="line">NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.</span><br><span class="line"></span><br><span class="line">&gt; Windowed mode</span><br><span class="line">&gt; Simulation data stored in video memory</span><br><span class="line">&gt; Single precision floating point simulation</span><br><span class="line">&gt; 1 Devices used for simulation</span><br><span class="line">gpuDeviceInit() CUDA Device [0]: &quot;GeForce GTX 1080</span><br><span class="line">&gt; Compute 6.1 CUDA device: [GeForce GTX 1080]</span><br><span class="line">number of bodies = 256000</span><br><span class="line">256000 bodies, total time for 10 iterations: 2395.682 ms</span><br><span class="line">= 273.559 billion interactions per second</span><br><span class="line">= 5471.177 single-precision GFLOP/s at 20 flops per interaction</span><br></pre></td></tr></table></figure><p>至此CUDA8.0安装完成。</p><h2 id="三、安装cuDNN-v5-1"><a href="#三、安装cuDNN-v5-1" class="headerlink" title="三、安装cuDNN v5.1"></a>三、安装cuDNN v5.1</h2><p>先去Nvidia开发者网站下载<a href="https://developer.nvidia.com/rdp/cudnn-download" target="_blank" rel="noopener">cuDNN</a>，这个需要注册账号后才能下载。</p><p>选择Download cuDNN v5.1 (August 10, 2016), for CUDA 8.0，然后点cuDNN v5.1 Library for Linux，会下载一个.tgz的文件</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/ef1784Ea8K.png" alt=""></p><p>在文件所在目录下执行：<code>tar -zxvf cudnn-8.0-linux-x64-v5.0-ga.tgz</code></p><p>虽然官方没有提供安装说明，但是google下就能查到。执行下列命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo cp cuda/include/cudnn.h /usr/local/cuda/include/</span><br><span class="line">sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64/</span><br><span class="line">sudo chmod a+r /usr/local/cuda/include/cudnn.h</span><br><span class="line">sudo chmod a+r /usr/local/cuda/lib64/libcudnn*</span><br></pre></td></tr></table></figure><p>这样cuDNN v5.1就安装完成了。</p><h2 id="四、安装TensorFlow"><a href="#四、安装TensorFlow" class="headerlink" title="四、安装TensorFlow"></a>四、安装TensorFlow</h2><p>TensorFlow有自己的<a href="https://www.tensorflow.org/get_started/os_setup#pip-installation" target="_blank" rel="noopener">官方安装文档</a>，提供了多种安装方式，我使用过其中的两种：pip安装和源码编译安装。简单说下两种安装方式的特点，pip安装过程便捷，几句命令就搞定。源码编译安装过程复杂，并且容易遇到各种问题。所以一般我们都会选择pip安装方式。</p><p>安装过程还有些小插曲，第一次安装的时候先尝试了使用pip安装，结果安装完后TensorFlow不识别GPU，捣鼓了很久并且google各种资料安装过程都没有问题，遂放弃pip转为源码编译安装成功了。过了几天重做了系统，又要安装TensorFlow，这次抱着试一试的心态使用pip安装，结果成功了= =然而并不知道为什么，所以第二次也就没用源码安装了。</p><p>本机安装环境为Python2.7，安装的TensorFlow版本为 v0.12.0 RC1</p><h3 id="1、pip安装"><a href="#1、pip安装" class="headerlink" title="1、pip安装"></a>1、pip安装</h3><p>首先安装pip</p><p><code>sudo apt-get install python-pip python-dev</code></p><p>如果使用的是Python3则换成pip3</p><p>安装TensorFlow</p><p><code>sudo pip install tensorflow</code></p><p>安装TensorFlow的GPU版本</p><p><code>sudo pip install tensorflow-gpu</code></p><p>OK，完成！是不是超简单= =哈哈，下面说下注意事项</p><p>如果网速不好的话可以在GitHub上先下载好<a href="https://github.com/tensorflow/tensorflow" target="_blank" rel="noopener">TensorFlow的安装包</a>，然后进行本地安装，当然还是会联网下一些依赖包，但是都比较小<br>在安装包目录下执行</p><p><code>sudo pip install tensorflow-0.12.0rc1-cp27-none-linux_x86_64.whl</code></p><p><code>sudo pip install tensorflow_gpu-0.12.0rc1-cp27-none-linux_x86_64.whl</code></p><p>install后面那部分就是下载的安装包的文件名</p><h3 id="2、源码编译安装"><a href="#2、源码编译安装" class="headerlink" title="2、源码编译安装"></a>2、源码编译安装</h3><p>首先先安装相关依赖包</p><p><code>sudo apt-get install python-pip</code></p><p><code>sudo apt-get install python-numpy swig python-dev python-wheel</code></p><p>在本地编译TensorFlow源码的话需要使用Google开源的一个构建工具——Bazel，也有<a href="http://www.bazel.io/versions/master/docs/install.html" target="_blank" rel="noopener">官方安装文档</a></p><p>首先需要下载Bazel，推荐<a href="https://github.com/bazelbuild/bazel/releases" target="_blank" rel="noopener">去GitHub上下载</a>，下载对应版本的安装包installer-linux-x86_64.sh。之前下载的是0.3.0版本的，结果在配置TensorFlow的时候出了一堆问题，后来换成了0.4.2版本问题解决。所以推荐下载最新版本的</p><p>在Bazel安装包所在目录下执行<br><code>sudo chmod +x bazel-0.4.2-installer-linux-x86_64.sh</code></p><p><code>sudo ./bazel-0.4.2-installer-linux-x86_64.sh --user</code></p><p>需要注意的是Bazel需要Java环境，如果没有的话需要安装，直接使用apt-get安装即可</p><p><code>sudo apt-get update</code></p><p><code>sudo apt-get install default-jre</code></p><p><code>sudo apt-get install default-jdk</code></p><p>安装完成后再执行<code>sudo ./bazel-0.4.2-installer-linux-x86_64.sh --user</code>即可</p><p>然后在 ~/.bashrc中追加：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">source /home/gai/.bazel/bin/bazel-complete.bash</span><br><span class="line">export PATH=$PATH:/home/gai/.bazel/bin</span><br></pre></td></tr></table></figure><p>需要注意的是，把gai换成自己系统的用户名</p><p>至于为什么要追加这个内容，我在Bazel安装文档中找到以下内容</p><blockquote><p>Bazel comes with a bash completion script. To install it:</p><ul><li>Build it with Bazel: bazel build //scripts:bazel-complete.bash.</li><li>Copy the script bazel-bin/scripts/bazel-complete.bash to your completion folder (/etc/bash_completion.d directory under Ubuntu). If you don’t have a completion folder, you can copy it wherever suits you and simply insert source /path/to/bazel-complete.bash in your ~/.bashrc file (under OS X, put it in your ~/.bash_profile file).</li></ul></blockquote><p>我在使用源码编译安装的时候直接在~/.bashrc追加了那两行内容，没有做Bazel安装文档中步骤。等下次使用源码编译安装的时候再尝试下</p><p>在~/.bashrc追加完后执行<code>source ~/.bashrc</code>更新一下</p><p>至此Bazel安装完成，下一步开始编译TensorFlow</p><p>首先先下载TensorFlow源码，可以使用git 命令从GitHub上克隆下来：git clone <a href="https://github.com/tensorflow/tensorflow" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow</a> ，这是最新版，也可以自己<a href="https://github.com/tensorflow/tensorflow" target="_blank" rel="noopener">去GitHub上下载</a>release版，推荐后者</p><p>下载完成后进入TensorFlow主目录，执行：</p><p><code>./configure</code></p><p>开始配置TensorFlow，接下来会有一系列问题需要确认，我印象中会询问是否需要支持Google Cloud、Hadoop、OpenCL、CUDA等，其中CUDA是我们需要的，所以我除了CUDA选了yes以外其他的都选了no</p><p>如果配置成功的话会出现下列信息，可能不完全一样，但至少会有Configuration finished</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">INFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.</span><br><span class="line">.....</span><br><span class="line">____Loading package: tensorflow/contrib/util</span><br><span class="line">____Loading package: tensorflow/tools/test</span><br><span class="line">____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 97,938 bytes</span><br><span class="line">____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 451,148 bytes</span><br><span class="line">____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 802,540 bytes</span><br><span class="line">____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 1,317,340 bytes</span><br><span class="line">____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 2,055,608 bytes</span><br><span class="line">____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 2,247,228 bytes</span><br><span class="line">____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 2,328,350 bytes</span><br><span class="line">____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 2,457,050 bytes</span><br><span class="line">____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 2,585,750 bytes</span><br><span class="line">____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 3,518,110 bytes</span><br><span class="line">____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 3,711,160 bytes</span><br><span class="line">INFO: All external dependencies fetched successfully.</span><br><span class="line">Configuration finished</span><br></pre></td></tr></table></figure><p>然后使用Bazel来编译TensorFlow，在TensorFlow源码目录下执行以下命令</p><p><code>bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer</code></p><p>编译需要一段时间，配置为i7-6700+24G内存大约耗时20分钟</p><p>编译完成后会显示以下信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Target //tensorflow/cc:tutorials_example_trainer up-to-date:</span><br><span class="line">  bazel-bin/tensorflow/cc/tutorials_example_trainer</span><br><span class="line">INFO: Elapsed time: 1196.829s, Critical Path: 986.68s</span><br></pre></td></tr></table></figure><p>执行一下TensorFlow官方提供的例子，看看能否成功调用GPU</p><p><code>bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu</code></p><p>如果看到<strong>successfully opened CUDA library、Creating TensorFlow device (/gpu:0)、显卡信息以及下面的运算过程</strong>，那说明成功调用了GPU</p><p>下面将TensorFlow源码编译成pip安装包供Python使用</p><p>编译CPU版本</p><p><code>bazel build -c opt //tensorflow/tools/pip_package:build_pip_package</code></p><p>如果要编译GPU版本的，不用执行上一句，只需执行以下命令</p><p><code>bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package</code></p><p>然后执行</p><p><code>bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg</code></p><p><code>sudo pip install /tmp/tensorflow_pkg/tensorflow-0.12.0rc1-cp27-cp27mu-linux_x86_64.whl</code></p><p>安装完成~</p><h3 id="3、测试TensorFlow"><a href="#3、测试TensorFlow" class="headerlink" title="3、测试TensorFlow"></a>3、测试TensorFlow</h3><p>下面测试下TensorFlow是否安装成功，并且是否能调用GPU</p><p>首先先配置环境变量，在home目录下执行</p><p><code>sudo gedit .bash_profile</code></p><p>然后在里面添加下面两行内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export LD_LIBRARY_PATH=&quot;$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64&quot;</span><br><span class="line">export CUDA_HOME=/usr/local/cuda</span><br></pre></td></tr></table></figure><p>然后执行：<code>source ~/.bash_profile</code>更新一下</p><p>执行下列代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">$ python</span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">I tensorflow/stream_executor/dso_loader.cc:<span class="number">128</span>] successfully opened CUDA library libcublas.so locally</span><br><span class="line">I tensorflow/stream_executor/dso_loader.cc:<span class="number">128</span>] successfully opened CUDA library libcudnn.so locally</span><br><span class="line">I tensorflow/stream_executor/dso_loader.cc:<span class="number">128</span>] successfully opened CUDA library libcufft.so locally</span><br><span class="line">I tensorflow/stream_executor/dso_loader.cc:<span class="number">128</span>] successfully opened CUDA library libcuda.so<span class="number">.1</span> locally</span><br><span class="line">I tensorflow/stream_executor/dso_loader.cc:<span class="number">128</span>] successfully opened CUDA library libcurand.so locally</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>hello = tf.constant(<span class="string">'Hello, TensorFlow!'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sess = tf.Session()</span><br><span class="line">I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:<span class="number">937</span>] successful NUMA node read <span class="keyword">from</span> SysFS had negative value (<span class="number">-1</span>), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class="line">I tensorflow/core/common_runtime/gpu/gpu_device.cc:<span class="number">885</span>] Found device <span class="number">0</span> <span class="keyword">with</span> properties: </span><br><span class="line">name: GeForce GTX <span class="number">1080</span></span><br><span class="line">major: <span class="number">6</span> minor: <span class="number">1</span> memoryClockRate (GHz) <span class="number">1.8095</span></span><br><span class="line">pciBusID <span class="number">0000</span>:<span class="number">01</span>:<span class="number">00.0</span></span><br><span class="line">Total memory: <span class="number">7.92</span>GiB</span><br><span class="line">Free memory: <span class="number">6.47</span>GiB</span><br><span class="line">I tensorflow/core/common_runtime/gpu/gpu_device.cc:<span class="number">906</span>] DMA: <span class="number">0</span> </span><br><span class="line">I tensorflow/core/common_runtime/gpu/gpu_device.cc:<span class="number">916</span>] <span class="number">0</span>: Y </span><br><span class="line">I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(sess.run(hello))</span><br><span class="line">Hello, TensorFlow!</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.constant(<span class="number">10</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = tf.constant(<span class="number">32</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(sess.run(a + b))</span><br><span class="line"><span class="number">42</span></span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure><p>首先import tensorflow没出错并且能输出<strong>Hello, TensorFlow!</strong>和<strong>42</strong>，这表明TensorFlow是可以使用的</p><p>然后看到<strong>successfully opened CUDA library</strong>和<strong>Creating TensorFlow device (/gpu:0)以及显卡信息</strong>，这表明是能够调用GPU</p><p>此外如何使用GPU以及是否使用了GPU可以<a href="https://www.tensorflow.org/how_tos/using_gpu/" target="_blank" rel="noopener">参考这篇内容</a></p><p>还可以测试一个TensorFlow的neural net model</p><p>执行：<code>python /usr/local/lib/python2.7/dist-packages/tensorflow/models/image/mnist/convolutional.py</code></p><p>第一次执行会下载一些东西然后开始执行。我的电脑执行的时间大约是40s</p><h3 id="4、注意事项"><a href="#4、注意事项" class="headerlink" title="4、注意事项"></a>4、注意事项</h3><ul><li>如果采用pip安装TensorFlow并要启用GPU支持的话，tensorflow和tensorflow-gpu都要安装，并且顺序不可以错。如果卸载了tensorflow，保留了tensorflow-gpu，此时TensorFlow是不好用的，并且在有tensorflow-gpu的情况下再装tensorflow，这时候TensorFlow是无法识别GPU的。所以如果要卸载那么把tensorflow和tensorflow-gpu都卸载，然后再按照先tensorflow后tensorflow-gpu的顺序安装。</li><li>如果采用源码编译安装TensorFlow并要启用GPU支持的话，直接编译GPU版本的pip安装包然后安装即可。</li><li>采用源码编译安装TensorFlow，在进行TensorFlow配置时，如果系统中没有安装OpenCL而在Do you wish to build TensorFlow with OpenCL support? [y/N]时又选择了y，那么会出现”Invalid SYCL 1.2 library path. /usr/local/computecpp/lib/libComputeCpp.so cannot be found “这个错误。</li><li>当初使用Bazel 0.3.0版本，进行TensorFlow配置时出现的错误如下。解决办法是使用了Bazel最新版0.4.2（在Bazel0.3.0时还使用过该命令tensorflow$ git pull –recurse-submodules，然后后来换成的0.4.2，不知是否有影响 ）。这个错误当初参考了以下信息<a href="https://github.com/tensorflow/tensorflow/issues/4365" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/issues/4365</a> 、<a href="https://github.com/tensorflow/tensorflow/issues/5357" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/issues/5357</a> 、<a href="https://github.com/tensorflow/tensorflow/issues/4319" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/issues/4319</a></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">INFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.</span><br><span class="line">.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:17:3: //external:eigen_archive: no such attribute &apos;urls&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:17:3: //external:eigen_archive: missing value for mandatory attribute &apos;url&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:28:3: //external:libxsmm_archive: no such attribute &apos;urls&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:28:3: //external:libxsmm_archive: missing value for mandatory attribute &apos;url&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:44:3: //external:com_googlesource_code_re2: no such attribute &apos;urls&apos; in &apos;http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:44:3: //external:com_googlesource_code_re2: missing value for mandatory attribute &apos;url&apos; in &apos;http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:54:3: //external:gemmlowp: no such attribute &apos;urls&apos; in &apos;http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:54:3: //external:gemmlowp: missing value for mandatory attribute &apos;url&apos; in &apos;http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:64:3: //external:farmhash_archive: no such attribute &apos;urls&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:64:3: //external:farmhash_archive: missing value for mandatory attribute &apos;url&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:80:3: //external:highwayhash: no such attribute &apos;urls&apos; in &apos;http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:80:3: //external:highwayhash: missing value for mandatory attribute &apos;url&apos; in &apos;http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:90:3: //external:nasm: no such attribute &apos;urls&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:90:3: //external:nasm: missing value for mandatory attribute &apos;url&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:101:3: //external:jpeg: no such attribute &apos;urls&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:101:3: //external:jpeg: missing value for mandatory attribute &apos;url&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:112:3: //external:png_archive: no such attribute &apos;urls&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:112:3: //external:png_archive: missing value for mandatory attribute &apos;url&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:123:3: //external:gif_archive: no such attribute &apos;urls&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:123:3: //external:gif_archive: missing value for mandatory attribute &apos;url&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:135:3: //external:six_archive: no such attribute &apos;urls&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:135:3: //external:six_archive: missing value for mandatory attribute &apos;url&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:151:3: //external:protobuf: no such attribute &apos;urls&apos; in &apos;http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:151:3: //external:protobuf: missing value for mandatory attribute &apos;url&apos; in &apos;http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:161:3: //external:gmock_archive: no such attribute &apos;urls&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:161:3: //external:gmock_archive: missing value for mandatory attribute &apos;url&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:187:3: //external:pcre: no such attribute &apos;urls&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:187:3: //external:pcre: missing value for mandatory attribute &apos;url&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:198:3: //external:swig: no such attribute &apos;urls&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:198:3: //external:swig: missing value for mandatory attribute &apos;url&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:222:3: //external:grpc: no such attribute &apos;urls&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:222:3: //external:grpc: missing value for mandatory attribute &apos;url&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:245:3: //external:linenoise: no such attribute &apos;urls&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:245:3: //external:linenoise: missing value for mandatory attribute &apos;url&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:258:3: //external:llvm: no such attribute &apos;urls&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:258:3: //external:llvm: missing value for mandatory attribute &apos;url&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:269:3: //external:jsoncpp_git: no such attribute &apos;urls&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:269:3: //external:jsoncpp_git: missing value for mandatory attribute &apos;url&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:285:3: //external:boringssl: no such attribute &apos;urls&apos; in &apos;http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:285:3: //external:boringssl: missing value for mandatory attribute &apos;url&apos; in &apos;http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:295:3: //external:nanopb_git: no such attribute &apos;urls&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:295:3: //external:nanopb_git: missing value for mandatory attribute &apos;url&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:311:3: //external:zlib_archive: no such attribute &apos;urls&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:311:3: //external:zlib_archive: missing value for mandatory attribute &apos;url&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package &apos;&apos;: Encountered error while reading extension file &apos;cuda/build_defs.bzl&apos;: no such package &apos;@local_config_cuda//cuda&apos;: error loading package &apos;external&apos;: Could not load //external package.</span><br><span class="line">ERROR: missing fetch expression. Type &apos;bazel help fetch&apos; for syntax and help.</span><br></pre></td></tr></table></figure><ul><li>使用GPU执行TensorFlow时可能会出现以下内容，对实际运行没看出来有什么影响</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">E tensorflow/core/framework/op_kernel.cc:925] OpKernel (&apos;op: &quot;NegTrain&quot; device_type: &quot;CPU&quot;&apos;) for unknown op: NegTrain</span><br><span class="line">E tensorflow/core/framework/op_kernel.cc:925] OpKernel (&apos;op: &quot;Skipgram&quot; device_type: &quot;CPU&quot;&apos;) for unknown op: Skipgram</span><br></pre></td></tr></table></figure><h2 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h2><p><a href="http://www.52nlp.cn/深度学习主机环境配置-ubuntu-16-04-nvidia-gtx-1080-cuda-8" target="_blank" rel="noopener">深度学习主机环境配置: Ubuntu16.04+Nvidia GTX 1080+CUDA8.0</a></p><p><a href="http://www.52nlp.cn/深度学习主机环境配置-ubuntu16-04-geforce-gtx1080-tensorflow" target="_blank" rel="noopener">深度学习主机环境配置: Ubuntu16.04+GeForce GTX 1080+TensorFlow</a></p><p><a href="http://www.cnblogs.com/yiruparadise/p/5671620.html" target="_blank" rel="noopener">ubuntu14.04 安装 tensorflow</a></p><p><a href="https://alliseesolutions.wordpress.com/2016/09/08/install-gpu-tensorflow-from-sources-w-ubuntu-16-04-and-cuda-8-0-rc/" target="_blank" rel="noopener">Install GPU TensorFlow From Sources w/ Ubuntu 16.04 and Cuda 8.0</a></p><p><a href="http://darren1231.pixnet.net/blog/post/331300298-安裝-tensorflow-教學-gpu%3Anvidia1070(from-source" target="_blank" rel="noopener">安裝 tensorflow 教學 GPU:Nvidia1070(from source)</a>)</p><h4 id="课外小知识"><a href="#课外小知识" class="headerlink" title="课外小知识"></a>课外小知识</h4><p>为什么使用Bazel来编译TensorFlow，他们是什么关系？</p><p>Bazel是一个构建工具（构建工具：依据文件之间的依赖关系来决定文件编译的顺序），类似于Linux下的make命令。Bazel的出现是Google为了解决自己的问题：Google所有的源代码都在一个源代码仓库，而Google是一个跨国公司，世界各地的程序员都需要下载代码然后编译，所以在项目的构建过程中，性能问题是最关键的需求。</p><p>Bazel相对于其他的构建工具相比，更加强调结构化和速度。</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/357Hc5ff1f.png" alt=""></p><p>在Bazel中文件编译顺序的自由度很高。构建过程可以用这样的二分图来表示。在一次构建过程中并发度越高（即二分图宽度越宽），执行时间就越短，如果机器足够多，那么一次构建的时间就主要由二分图的高度来决定。所以便可以在许多机器上执行分布式并发构建。</p><p>通过这个二分图可以看出，Bazel还具有增量构建的特点。当只有小部分源代码更改的时候，只需构建相应的部分即可。</p><p>构建行为具有函数式特点，即输入是相同的则输出也是相同的。依据这个特点，来缓存和复用构建结果。</p><p>Bazel和TensorFlow师出同门都来自于Google，在Google内部编译都使用Bazel，所以TensorFlow的编译过程自然也使用Bazel啦。</p><p>想进一步了解Bazel的同学可以看看以下内容：</p><p>Google开发Bazel的背景知识</p><p><a href="http://google-engtools.blogspot.co.uk/2011/06/build-in-cloud-accessing-source-code.html" target="_blank" rel="noopener">http://google-engtools.blogspot.co.uk/2011/06/build-in-cloud-accessing-source-code.html</a></p><p><a href="http://google-engtools.blogspot.tw/2011/08/build-in-cloud-how-build-system-works.html" target="_blank" rel="noopener">http://google-engtools.blogspot.tw/2011/08/build-in-cloud-how-build-system-works.html</a></p><p><a href="http://google-engtools.blogspot.jp/2011/09/build-in-cloud-distributing-build-steps.html" target="_blank" rel="noopener">http://google-engtools.blogspot.jp/2011/09/build-in-cloud-distributing-build-steps.html</a></p><p><a href="http://google-engtools.blogspot.tw/2011/10/build-in-cloud-distributing-build.html" target="_blank" rel="noopener">http://google-engtools.blogspot.tw/2011/10/build-in-cloud-distributing-build.html</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近开始学习机器学习，所以需要配一台电脑。本文主要写的是系统环境配置的内容，依据前人经验总结自己的安装过程，希望可以给大家一个参考。&lt;/p&gt;
&lt;p&gt;主机配置：i7-6700 + 24G内存 + GTX 1080&lt;/p&gt;
&lt;p&gt;系统环境配置：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ubuntu 16.04 LTS 64位&lt;/li&gt;
&lt;li&gt;CUDA 8.0&lt;/li&gt;
&lt;li&gt;cuDNN v5.1&lt;/li&gt;
&lt;li&gt;TensorFlow v0.12.0 RC1&lt;/li&gt;
&lt;li&gt;Python 2.7&lt;/li&gt;
&lt;li&gt;Bazel 0.4.2&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在整个环境配置过程中，有许多东西可以提前下载好，在配置时便可以节省时间了。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://developer.nvidia.com/cuda-downloads&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CUDA 8.0&lt;/a&gt; (1.4GB)：Linux &amp;gt; x86_64 &amp;gt; Ubuntu &amp;gt; 16.04 &amp;gt; runfile(local)&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://developer.nvidia.com/rdp/cudnn-download&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;cuDNN v5.1&lt;/a&gt; (100MB)： 需要注册Nvidia开发者账号，Download cuDNN v5.1 (August 10, 2016), for CUDA 8.0 &amp;gt; cuDNN v5.1 Library for Linux。最好在Linux系统下下载，格式为.tgz。在Windows下下载的格式会识别成.solitairetheme8格式。&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/tensorflow/tensorflow/releases&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;TensorFlow 源码release版&lt;/a&gt; (10MB+)：下载v0.12.0 RC1，zip或者tar.gz均可&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/tensorflow/tensorflow&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;TensorFlow pip安装包&lt;/a&gt; (CPU版40MB+，GPU版80MB+)：选择Linux和Python2的版本，CPU和GPU的都下。pip安装包只会下载最新版本&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/bazelbuild/bazel/releases&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Bazel 源码&lt;/a&gt; (100MB+)：下载0.4.2版本，选择bazel-0.4.2-installer-linux-x86_64.sh&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="技术" scheme="https://bluesmilery.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="Linux" scheme="https://bluesmilery.github.io/tags/Linux/"/>
    
      <category term="Machine Learning" scheme="https://bluesmilery.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Linux系统安装——Ubuntu16.04+Windows7双系统</title>
    <link href="https://bluesmilery.github.io/blogs/1509af3a/"/>
    <id>https://bluesmilery.github.io/blogs/1509af3a/</id>
    <published>2017-03-11T12:33:07.000Z</published>
    <updated>2018-01-31T03:43:51.943Z</updated>
    
    <content type="html"><![CDATA[<p>本文安装的是 Ubuntu 16.04 LTS 64位版本，与Windows7构成双系统</p><p>1、先从<a href="https://www.ubuntu.com/download/alternative-downloads" target="_blank" rel="noopener">Ubunbu官方</a>下载系统镜像，选择64位的16.04 LTS版本。</p><p>2、制作USB安装盘。在Windows系统下， 选择<a href="https://www.ezbsystems.com/ultraiso/download.htm" target="_blank" rel="noopener">UltraISO</a>来制作USB安装盘。注意，制作过程会将U盘格式化，请提前备份好资料。<br>进入UltralISO后，选择 文件&gt;打开，选择下载好的Ubuntu镜像。<br><a id="more"></a><br><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180129/i5l1EiiL5E.png" alt=""></p><p>然后选择 启动&gt;写入硬盘映像</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180129/AlIL7ffb3h.png" alt=""></p><p>然后按默认值写入即可。写入方式处可以选择默认的USB-HDD+，也可以选择USB-ZIP+。我选择的是后者。</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180129/6eaG04kj45.png" alt=""></p><p>至此，USB启动盘就做好了。</p><p>3、重启电脑，在BIOS中设置USB启动优先。插上U盘，再开机就可以进入Ubuntu的安装界面了。<br>点击install Ubuntu后会进入该界面。这里可能会有个问题，就是点击安装后屏幕会黑屏，我的电脑显卡是GTX 1080，在一个旧显示器（1440x900）上会出现这种现象，但是换成DELL P2415Q（4K）就没有这个黑屏的问题。如果你有黑屏的问题，可以参考这两篇文章：<a href="http://blog.sciencenet.cn/blog-655584-877622.html" target="_blank" rel="noopener">安装ubuntu黑屏问题的解决</a>、<a href="http://askubuntu.com/questions/38780/how-do-i-set-nomodeset-after-ive-already-installed-ubuntu" target="_blank" rel="noopener">How do I set ‘nomodeset’ after I’ve already installed Ubuntu?</a></p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180129/ABm41HDd92.jpg" alt=""></p><p>这个界面上的两个选项分别是是否现在下载更新以及一些第三方驱动和软件，这里为了不耽误安装进度，可以都不勾选，等进入系统后再更新。</p><p>点击Continue后进入下一界面。</p><ul><li>如果你的电脑之前安装有其他系统，就会有第一个选项，它会在第一块硬盘上的一个空白分区里安装Ubuntu，需要注意的是它只会在第一块硬盘上选，如果你后来加了块新硬盘并且想在新硬盘上安装Ubuntu，需要选第三个选项something else。</li><li>第二个选项是会把整块硬盘格式化，如果你不需要双系统（以及不需要硬盘上原来的资料），那么选这个就可以。</li><li>第三个选项是全手动。如果想要安装双系统，或者Ubuntu的分区想自己控制的话，那么选这个。</li></ul><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180129/EiCgmKdHGm.jpg" alt=""></p><p>选something else后进入下面这个页面。先简单介绍下界面内容。sd代表硬盘，第一块是sda，第二块sdb，依次类推。以第一块硬盘为例，sda1后面的数字表示分区，1234代表主分区，从5往后代表逻辑分区。可以看出我的第一块硬盘现在有四个分区，sda1是Windows自带的隐藏分区，sda2是windows系统盘，sda3是用做了Windows的数据盘，sda5是打算给Ubuntu做数据盘。</p><p>选择你想要安装Ubuntu的分区，点下面的加号开始进行Ubuntu分区。</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180129/9DcH5bAdi1.jpg" alt=""></p><p>下图是分区选项。第一个是分区大小。第二个是选择分区类型，主分区还是逻辑分区（关于主分区和逻辑分区的知识可以自行百度了解，在Windows和Linux下的概念一样），对于Ubuntu系统本身而言，所有分区都是逻辑分区也没事。第三个是分区的起始位置，正着分还是倒着分，一般都选第一个。第四个是分区的文件系统。第五个是文件挂载点。</p><p>如果你对Linux系统完全不了解，并且不想了解分区的各种学问，那么按照我这种最简单的分区方法来做就可以：<strong>分两个区，第一个分区的大小和内存大小挂钩，如果你内存比较小（4G、8G）那么分区大小是内存大小的两倍，如果内存比较大，那么分区大小是内存大小的1.5倍，文件系统选Swap，这个分区是作为虚拟内存使用的；第二个分区的大小就是剩余空间全用上就可以，文件类型选Ext4，挂载点就选”/“。</strong></p><p>网上有各种分区的说法，当然各有各的好处或者是目的，如果你是小白并且只是想简单使用Linux系统，照我的分法肯定没问题。还有关于双系统，网上有说想要装双系统需要把/BOOT分区单独分区来，其实完全没这个必要，按照我的两个分区分法也可以成功安装双系统。</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180129/lBfLlc8hJf.jpg" alt=""></p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180129/kBC1K11jLI.jpg" alt=""></p><p>分完区后界面如下。我在新硬盘上分了一个主分区ext4，一个逻辑分区swap，还有一些空间没有使用。</p><p>在下面的device for boot那里选择”/“分区（如果你分出来了/BOOT分区，那么就选/BOOT）。然后在上面的区域内点击选择”/“分区，然后点Install Now。</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180129/a3IdFjIhFj.jpg" alt=""></p><p>会出现一个确认对话框，点Continue。</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180129/2E3cG20531.jpg" alt=""></p><p>然后下面就是选地区选语言设置用户等等，最后出现下图后点Restart Now就安装完成啦。</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180129/iGBilmeDC0.jpg" alt=""></p><p>如果你点完以后出现下面的鬼提示，不用管，直接电源键强制重启即可。记得拔U盘哦。</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180129/mGdI0hEakm.jpg" alt=""></p><p>4、如果你只安装了Ubuntu，那么重启后就会进入Ubuntu系统了。如果你是要安装双系统，那么重启之后你会进入到Windows系统，那Ubuntu呢，别着急，接下来我们就添加Ubuntu的启动项。</p><p>需要下载一个软件<a href="http://neosmart.net/EasyBCD/" target="_blank" rel="noopener">EasyBCD</a>。进入页面后往下拉选择REGISTER，进入下一页面后不用填信息，直接点Download即可。</p><p>软件的界面大致如下。在工具菜单里可以设置语言。点左边的添加新条目，操作系统选Linux，类型是GRUB 2，名称可以自己随便设，驱动器的话选当初分的”/“分区，根据分区大小来判断（如果当初分了/BOOT分区，那么就选/BOOT分区），点击添加条目。然后点左边的编辑引导菜单，可以看到这时候条目里有两个系统了，下面有一些选项，比如倒计时或者是等待用户选择，依据情况自己设置。</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180129/hmebB4G4Di.png" alt=""></p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180129/a4l48flCDh.png" alt=""></p><p>弄好后重启电脑，看看是不是可以选择两个系统啦，大功告成~</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文安装的是 Ubuntu 16.04 LTS 64位版本，与Windows7构成双系统&lt;/p&gt;
&lt;p&gt;1、先从&lt;a href=&quot;https://www.ubuntu.com/download/alternative-downloads&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Ubunbu官方&lt;/a&gt;下载系统镜像，选择64位的16.04 LTS版本。&lt;/p&gt;
&lt;p&gt;2、制作USB安装盘。在Windows系统下， 选择&lt;a href=&quot;https://www.ezbsystems.com/ultraiso/download.htm&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;UltraISO&lt;/a&gt;来制作USB安装盘。注意，制作过程会将U盘格式化，请提前备份好资料。&lt;br&gt;进入UltralISO后，选择 文件&amp;gt;打开，选择下载好的Ubuntu镜像。&lt;br&gt;
    
    </summary>
    
      <category term="技术" scheme="https://bluesmilery.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="Linux" scheme="https://bluesmilery.github.io/tags/Linux/"/>
    
  </entry>
  
</feed>
