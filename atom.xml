<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Gai&#39;s Blog</title>
  
  <subtitle>A Zone for Knowledge</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://bluesmilery.github.io/"/>
  <updated>2018-12-01T07:38:58.000Z</updated>
  <id>https://bluesmilery.github.io/</id>
  
  <author>
    <name>Gai</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>使用Harbor搭建Docker私有仓库</title>
    <link href="https://bluesmilery.github.io/blogs/b6607682/"/>
    <id>https://bluesmilery.github.io/blogs/b6607682/</id>
    <published>2018-11-30T21:35:33.000Z</published>
    <updated>2018-12-01T07:38:58.000Z</updated>
    
    <content type="html"><![CDATA[<p>Docker官方提供了Docker Hub作为Docker公共仓库，大家可以上传或者下载镜像。但是对于企业来说将生产环境的镜像放在公共仓库是不安全的，所以企业有必要搭建自己的私有仓库。Docker官方提供了开源的registry镜像，可以用于搭建Docker私有仓库。但是它没有管理界面，缺少运维和管理功能，并不适用于企业搭建</p><p>由VMware中国研发团队负责开发的开源企业级Docker Registry——Harbor，以Docker官方提供的开源registry为基础，增加了Web UI、用户权限管理、日志等一系列便于运维和管理的功能，可以帮助企业快速搭建企业级的Docker私有仓库。所以Harbor成为了第一选择</p><p>本文的主要内容为如何使用Harbor搭建Docker私有仓库，并简单说明下使用中的注意事项</p><a id="more"></a><h3 id="系列文章"><a href="#系列文章" class="headerlink" title="系列文章"></a>系列文章</h3><p>本文为【Docker &amp; Kubernetes &amp; GPU】系列文章中的一篇。其他文章为：</p><ul><li><a href="https://bluesmilery.github.io/blogs/252e6902/">Docker安装指南以及使用GPU</a></li></ul><h3 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h3><p>本文实践的服务器环境为：</p><ul><li>CentOS Linux release 7.5.1804 (Core)</li><li>内核版本：3.10.0-862.3.2.el7.x86_64</li><li>Docker-CE版本：18.09.0</li><li>所安装的Harbor版本为：1.6.1</li></ul><h2 id="1、Harbor安装指南"><a href="#1、Harbor安装指南" class="headerlink" title="1、Harbor安装指南"></a>1、Harbor安装指南</h2><p>官方完整版安装指南：<a href="https://github.com/goharbor/harbor/blob/master/docs/installation_guide.md" target="_blank" rel="noopener">https://github.com/goharbor/harbor/blob/master/docs/installation_guide.md</a></p><h3 id="前置条件"><a href="#前置条件" class="headerlink" title="前置条件"></a>前置条件</h3><p>Harbor是使用Docker Compose进行快速部署，所以需要确保机器已经安装Docker Compose与Docker。相关安装指南可参见：<a href="https://bluesmilery.github.io/blogs/252e6902/">Docker安装指南以及使用GPU</a></p><p>更为具体的硬件、软件、端口要求可见下图。其中端口如果被占用的话，可以修改相应文件改变端口映射。下文会有具体说明</p><p>#todo：图片</p><h3 id="下载说明"><a href="#下载说明" class="headerlink" title="下载说明"></a>下载说明</h3><p>首先先从<a href="https://github.com/goharbor/harbor/releases" target="_blank" rel="noopener">GitHub</a>上下载安装文件，分为在线安装版和离线安装版</p><p>因为Harbor的所有组件运行在容器中，所以在线版就是安装的时候会联网下载相应的Docker镜像，而离线版就是安装文件中包含了相应的镜像</p><p>但是需要注意的是无论在线还是离线，安装文件都存放在Google相应的网址中（需自行翻墙解决），但是在线版所需要的镜像国内联网可正常下载</p><h3 id="安装配置"><a href="#安装配置" class="headerlink" title="安装配置"></a>安装配置</h3><p>解压后目录下会有一个名为<code>harbor.cfg</code>的文件，Harbor的所有安装配置都存在于这里。官方对每一个参数都有说明，主要包括网络、日志、邮件、CA、账号、数据库和LDAP、redis等相关参数设置，其中部分设置在安装后还可以在Web端修改</p><p>因为这里是公司内网使用，所以不需要做CA认证，直接http链接即可。所以唯一需要修改的就是hostname，需要修改为本机IP（如果不使用域名的话），其他的默认</p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>修改完配置以后，执行目录下的<code>install.sh</code>文件便开始安装，然后会依次启动相关服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[Step 3]: starting Harbor ...</span><br><span class="line">Creating network &quot;harbor_harbor&quot; with the default driver</span><br><span class="line">Creating harbor-log ... done</span><br><span class="line">Creating harbor-db          ... done</span><br><span class="line">Creating redis              ... done</span><br><span class="line">Creating harbor-adminserver ... done</span><br><span class="line">Creating registry           ... done</span><br><span class="line">Creating harbor-ui          ... done</span><br><span class="line">Creating harbor-jobservice  ... done</span><br><span class="line">Creating nginx              ... done</span><br><span class="line"></span><br><span class="line">✔ ----Harbor has been installed and started successfully.----</span><br><span class="line"></span><br><span class="line">Now you should be able to visit the admin portal at http://【所设置的hostname】.</span><br><span class="line">For more details, please visit https://github.com/goharbor/harbor .</span><br></pre></td></tr></table></figure><p>在浏览器中打开所设置的hostname，看到登陆界面的话即为安装成功</p><h3 id="转移数据目录"><a href="#转移数据目录" class="headerlink" title="转移数据目录"></a>转移数据目录</h3><p>转移数据目录有两种方式：一种是将默认数据目录软链至其他路径，另外一种是修改相应配置。目前使用的是第一种</p><ul><li>目录软链</li></ul><p>Harbor所有数据文件（包括镜像）默认存储在<code>/data</code>目录下，如果其所在的硬盘分区空间较小，可以将其转移到大的磁盘分区。例如我这里是根目录<code>/</code>挂载在小硬盘上，<code>/home</code>目录挂载在大硬盘上，所以将其转移到<code>/home</code>目录下。前往Harbor目录，执行以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker-compose stop</span><br><span class="line">mkdir /home/root_data</span><br><span class="line">mv /data /home/root_data</span><br><span class="line">ln -s /home/root_data/data /data</span><br><span class="line">docker-compose start</span><br></pre></td></tr></table></figure><ul><li>修改配置</li></ul><p>注：因未完全摸清所有配置，不知有没有地方漏修改了，所以该方法<strong>没有经过测试</strong></p><p>Harbor各个组件容器的启动配置在<code>docker-compose.yml</code>文件中，所以可以在这里修改</p><p>打开该文件后，搜索所有<code>volumes</code>关键字，可以在每个容器的volumes下面看到，数据在Host机器上的挂载只会在两个地方：<code>/data</code>和<code>./common</code>。将<code>/data</code>修改为所需目录即可，例如<code>/home/harborData</code></p><p>在Harbor的安装配置文件<code>harbor.cfg</code>中也有部分数据路径设置，主要为ssh相关设置，也作出相应修改即可</p><p>修改前使用<code>docker-compose down -v</code>停止并删除现有容器，修改完成后先执行<code>.prepare</code>使配置生效，然后使用<code>docker-compose up -d</code>重新创建容器并启动服务</p><h3 id="端口修改"><a href="#端口修改" class="headerlink" title="端口修改"></a>端口修改</h3><p>Harbor默认使用的端口是80（HTTP）、443和4443（HTTPS），如果存在端口占用冲突，可以进行修改</p><p>打开<code>docker-compose.yml</code>文件，在proxy服务下面的ports参数部分，可以看到端口映射关系，冒号左边为Host所用端口，冒号右边为容器所用端口。根据自己的需要对Host所用端口进行修改即可</p><p>然后再打开<code>harbor.cfg</code>文件，在hostname处IP（或域名）的后面增加所用端口</p><p>然后执行以下命令重启Harbor服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker-compose down -v</span><br><span class="line">./prepare</span><br><span class="line">docker-compose up -d</span><br></pre></td></tr></table></figure><p>之后需要使用IP加端口才能访问Harbor服务</p><h2 id="2、Harbor使用须知"><a href="#2、Harbor使用须知" class="headerlink" title="2、Harbor使用须知"></a>2、Harbor使用须知</h2><p>在使用Harbor的过程中经常会遇见网络连接相关或者认证相关的错误，一般是由于以下几点没有设置或者做对。所以挑出来一起说明一下</p><p>PS：如果修改了Harbor所使用的端口，那么下面的内容中在IP或者域名后面均需要加上所使用的端口</p><ul><li>修改Docker配置</li></ul><p>如果Harbor配置的是HTTP链接，那么在需要使用私有仓库的机器上编辑<code>/etc/docker/daemon.json</code>文件，在其中加上以下键值对。修改后需要重启Docker<code>systemctl restart docker</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;insecure-registries&quot; : [&quot;IP或者域名&quot;]</span><br></pre></td></tr></table></figure><p>HTTPS链接无需修改</p><ul><li>推送镜像</li></ul><p>所推送的镜像命名要符合规范。例如</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker push IP或者域名/library/镜像名:tag</span><br></pre></td></tr></table></figure><p>其中library为Harbor默认存在的项目。如果使用自建的项目，修改为相应的项目名即可</p><p>（Harbor必须有项目才可以使用，无项目名直接推送的话会报错）</p><p>推送前要先登录<code>docker login IP或者域名</code>。并且需要已经加入所要推送到的项目（例如默认的library），这一项需要联系Harbor管理员进行添加</p><ul><li>拉取镜像</li></ul><p>拉取镜像时代码与推送一样，只是将push换为pull</p><p>如果所要拉取的镜像所在的项目是公开的，那么可以直接拉取不需要登录。否则需要是项目成员且需要登录才可以拉取</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li>Harbor安装指南：<a href="https://github.com/goharbor/harbor/blob/master/docs/installation_guide.md" target="_blank" rel="noopener">https://github.com/goharbor/harbor/blob/master/docs/installation_guide.md</a></li><li>Harbor使用指南：<a href="https://github.com/goharbor/harbor/blob/master/docs/user_guide.md" target="_blank" rel="noopener">https://github.com/goharbor/harbor/blob/master/docs/user_guide.md</a></li><li>搭建Harbor的参考文章：<a href="https://blog.csdn.net/aixiaoyang168/article/details/73549898" target="_blank" rel="noopener">https://blog.csdn.net/aixiaoyang168/article/details/73549898</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Docker官方提供了Docker Hub作为Docker公共仓库，大家可以上传或者下载镜像。但是对于企业来说将生产环境的镜像放在公共仓库是不安全的，所以企业有必要搭建自己的私有仓库。Docker官方提供了开源的registry镜像，可以用于搭建Docker私有仓库。但是它没有管理界面，缺少运维和管理功能，并不适用于企业搭建&lt;/p&gt;
&lt;p&gt;由VMware中国研发团队负责开发的开源企业级Docker Registry——Harbor，以Docker官方提供的开源registry为基础，增加了Web UI、用户权限管理、日志等一系列便于运维和管理的功能，可以帮助企业快速搭建企业级的Docker私有仓库。所以Harbor成为了第一选择&lt;/p&gt;
&lt;p&gt;本文的主要内容为如何使用Harbor搭建Docker私有仓库，并简单说明下使用中的注意事项&lt;/p&gt;
    
    </summary>
    
      <category term="技术" scheme="https://bluesmilery.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="Docker" scheme="https://bluesmilery.github.io/tags/Docker/"/>
    
      <category term="Harbor" scheme="https://bluesmilery.github.io/tags/Harbor/"/>
    
  </entry>
  
  <entry>
    <title>Docker安装指南以及使用GPU</title>
    <link href="https://bluesmilery.github.io/blogs/252e6902/"/>
    <id>https://bluesmilery.github.io/blogs/252e6902/</id>
    <published>2018-11-25T14:26:25.000Z</published>
    <updated>2018-11-27T04:17:35.000Z</updated>
    
    <content type="html"><![CDATA[<p>随着公司GPU服务器数量的增加，深度学习开发环境的部署逐渐成为了负担。因为机器本身环境配置的差异，即使经验丰富的人也会遇到一些全新的问题，需要耗费时间去解决。而Docker与生俱来的<code>Build once, Run anywhere</code>特点使得多机器统一环境部署变得极为容易，所以使用Docker势在必行</p><p>本文的主要内容为在腾讯云的GPU服务器上如何安装Docker，并能支持GPU的使用</p><p>（用了Docker后，就可以跟<a href="https://bluesmilery.github.io/blogs/a687003b/">前文介绍的Anaconda</a>拜拜了，因为conda只做到了Python环境隔离，并且每个虚拟环境还是要手动配置）</p><a id="more"></a><p>Docker是什么，有什么好处，网上的介绍很多，相关图书也有很多，在此不做赘述。我是通过《循序渐进学Docker》这本书入门的，让我很清晰的知道了基本概念和基本原理，摆脱了之前一看就头大的困扰</p><p>如果在网上搜索Docker相关的内容，经常会看到这个四个词：<code>Docker、Moby、Docker-CE、Docker-EE</code>，所以在这里简单介绍一下这四个词之间有什么关系：</p><blockquote><p>Docker于2013年3月正式开源，然后由社区正常进行版本迭代，一直至2017年3月版本为v1.13.1。然后Docker公司将Docker项目改名为Moby项目（仍由社区维护），并发布两款自己维护的产品Docker-CE和Docker-EE。其中Docker-CE为开源免费的，提供给广大开发者和用户使用；Docker-EE为封闭收费的，提供给企业级用户使用。现在大家口中的Docker主要指的就是Docker-CE</p></blockquote><h3 id="系列文章"><a href="#系列文章" class="headerlink" title="系列文章"></a>系列文章</h3><p> 本文为【Docker &amp; Kubernetes &amp; GPU】系列文章中的一篇。其他篇正在编写中</p><h3 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h3><p>本文实践的服务器环境为：</p><ul><li>CentOS Linux release 7.5.1804 (Core)</li><li>内核版本：3.10.0-862.14.4.el7.x86_64</li><li>所安装的Docker-CE版本：18.06.1-ce</li></ul><h2 id="1、安装Docker"><a href="#1、安装Docker" class="headerlink" title="1、安装Docker"></a>1、安装Docker</h2><p>Docker需要使用root权限来安装。以下内容根据官方安装指南进行简化整理，完整版请移步：<a href="https://docs.docker.com/install/" target="_blank" rel="noopener">https://docs.docker.com/install/</a></p><p>官方指南中针对各个系统都有安装说明，此处使用的是CentOS。Docker CE要求使用CentOS 7版本，并且<code>centos-extras</code>库需要启用，不过正常来说是默认启用的</p><h3 id="卸载旧版本"><a href="#卸载旧版本" class="headerlink" title="卸载旧版本"></a>卸载旧版本</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">yum remove docker \</span><br><span class="line">           docker-client \</span><br><span class="line">           docker-client-latest \</span><br><span class="line">           docker-common \</span><br><span class="line">           docker-latest \</span><br><span class="line">           docker-latest-logrotate \</span><br><span class="line">           docker-logrotate \</span><br><span class="line">           docker-selinux \</span><br><span class="line">           docker-engine-selinux \</span><br><span class="line">           docker-engine</span><br></pre></td></tr></table></figure><h3 id="安装Docker-CE"><a href="#安装Docker-CE" class="headerlink" title="安装Docker CE"></a>安装Docker CE</h3><p>安装方式有许多种，这里选择使用添加yum库的方式来安装</p><p>安装相关依赖</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y yum-utils device-mapper-persistent-data lvm2</span><br></pre></td></tr></table></figure><p>添加Docker的yum库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo</span><br><span class="line"><span class="comment"># 若担心下载过慢，可以添加阿里的镜像源</span></span><br><span class="line"><span class="comment"># yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</span></span><br></pre></td></tr></table></figure><p>安装最新版Docker CE（安装时为18.06.1-ce）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install docker-ce</span><br></pre></td></tr></table></figure><p>如果遇见无法安装的问题（跟audit-lib相关），请参考下方’一些错误’部分</p><p>如果提示需要接受GPG key，看一下是不是<code>060A 61C5 1B55 8A7F 742B 77AA C52F EB6B 621E 9F35</code>，是的话就接受</p><p>如果要安装指定版本的，可以先执行<code>yum list docker-ce --showduplicates | sort -r</code>列举出来所有版本，例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker-ce.x86_64            3:18.09.0-3.el7                    docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:18.09.0-3.el7                    @docker-ce-stable</span><br><span class="line">docker-ce.x86_64            18.06.1.ce-3.el7                   docker-ce-stable</span><br><span class="line">docker-ce.x86_64            18.06.0.ce-3.el7                   docker-ce-stable</span><br><span class="line">docker-ce.x86_64            18.03.1.ce-1.el7.centos            docker-ce-stable</span><br></pre></td></tr></table></figure><p>然后执行<code>yum install docker-ce-&lt;VERSION STRING&gt;</code>安装。其中<code>&lt;VERSION STRING&gt;</code>指的是版本号中第一个破折号之前的内容，例如<code>yum install docker-ce-18.06.1.ce</code></p><h3 id="验证是否安装成功"><a href="#验证是否安装成功" class="headerlink" title="验证是否安装成功"></a>验证是否安装成功</h3><p>启动Docker服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl start docker</span><br><span class="line"><span class="comment"># service docker start  # 也可以</span></span><br></pre></td></tr></table></figure><p>如果报错，请参考下方’一些错误’部分</p><p>运行hello-world镜像</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run hello-world</span><br></pre></td></tr></table></figure><p>该命令会首先在本地寻找是否存在hello-world镜像，如果不存在就会去Docker官方镜像库Docker Hub中拉取下载，然后启动一个容器运行该镜像。如果成功运行，将会打印以下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Hello from Docker!</span><br><span class="line">This message shows that your installation appears to be working correctly.</span><br><span class="line"></span><br><span class="line">To generate this message, Docker took the following steps:</span><br><span class="line"> 1. The Docker client contacted the Docker daemon.</span><br><span class="line"> 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub.</span><br><span class="line">    (amd64)</span><br><span class="line"> 3. The Docker daemon created a new container from that image which runs the</span><br><span class="line">    executable that produces the output you are currently reading.</span><br><span class="line"> 4. The Docker daemon streamed that output to the Docker client, which sent it</span><br><span class="line">    to your terminal.</span><br><span class="line"></span><br><span class="line">To try something more ambitious, you can run an Ubuntu container with:</span><br><span class="line"> $ docker run -it ubuntu bash</span><br><span class="line"></span><br><span class="line">Share images, automate workflows, and more with a free Docker ID:</span><br><span class="line"> https://hub.docker.com/</span><br><span class="line"></span><br><span class="line">For more examples and ideas, visit:</span><br><span class="line"> https://docs.docker.com/get-started/</span><br></pre></td></tr></table></figure><h3 id="添加权限"><a href="#添加权限" class="headerlink" title="添加权限"></a>添加权限</h3><p>执行Docker需要用户具有sudo权限，所以可以将需要使用Docker的普通用户加入docker用户组</p><p>先查看下是否存在docker用户组<code>cat /etc/group | grep docker</code>（注意不是dockerroot），如果不存在则创建<code>groupadd docker</code>。然后将普通用户添加至docker用户组中<code>usermod -aG docker username</code></p><h3 id="修改Docker-Hub为国内镜像"><a href="#修改Docker-Hub为国内镜像" class="headerlink" title="修改Docker Hub为国内镜像"></a>修改Docker Hub为国内镜像</h3><p>参考自：<a href="https://www.docker-cn.com/registry-mirror" target="_blank" rel="noopener">https://www.docker-cn.com/registry-mirror</a></p><p>修改<code>/etc/docker/daemon.json</code>文件（不存在则创建），并添加上 registry-mirrors 键值</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;registry-mirrors&quot;: [&quot;https://registry.docker-cn.com&quot;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="转移数据目录"><a href="#转移数据目录" class="headerlink" title="转移数据目录"></a>转移数据目录</h3><p>Docker的数据目录默认位于<code>/var/lib/docker</code>，里面会存储着Docker镜像的数据。如果其所在的硬盘分区空间较小，可以将其转移到大的磁盘分区。例如我这里是根目录<code>/</code>挂载在小硬盘上，<code>/home</code>目录挂载在大硬盘上，所以将其转移到<code>/home</code>目录下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">service docker stop</span><br><span class="line">mkdir /home/dockerData</span><br><span class="line">mv /var/lib/docker /home/dockerData</span><br><span class="line">ln -s /home/dockerData/docker /var/lib/docker</span><br><span class="line">service docker start</span><br></pre></td></tr></table></figure><h3 id="安装Docker-Compose"><a href="#安装Docker-Compose" class="headerlink" title="安装Docker Compose"></a>安装Docker Compose</h3><p>Docker提倡的理念是一个容器一个进程，那么一个服务是由多个进程组成的，那么就需要启动多个容器。而容器之间肯定是有依赖关系的（比如需要先启动数据库容器），如果手动管理则太麻烦。幸好Docker提供了一个工具——Docker Compose来解决这个问题，它允许用户在一个模版（yaml格式的）中定义一组相关联的容器，通过配置可以实现多个容器依次创建和启动</p><p>执行以下命令进行安装。其中1.23.1为撰文时的最新版本。可以去<a href="https://github.com/docker/compose/releases" target="_blank" rel="noopener">GitHub</a>去查看最新版本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -L &quot;https://github.com/docker/compose/releases/download/1.23.1/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-compose</span><br><span class="line">chmod +x /usr/local/bin/docker-compose</span><br></pre></td></tr></table></figure><p>执行<code>docker-compose --version</code>验证是否安装成功</p><h3 id="一些错误"><a href="#一些错误" class="headerlink" title="一些错误"></a>一些错误</h3><ul><li>跟audit-libs相关的依赖问题，错误内容如下</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">错误：软件包：audit-libs-python-2.8.1-3.el7.x86_64 (lianjia-base)</span><br><span class="line">          需要：audit-libs(x86-64) = 2.8.1-3.el7</span><br><span class="line">          已安装: audit-libs-2.8.1-3.el7_5.1.x86_64 (@updates)</span><br><span class="line">              audit-libs(x86-64) = 2.8.1-3.el7_5.1</span><br><span class="line">          可用: audit-libs-2.8.1-3.el7.x86_64 (lianjia-base)</span><br><span class="line">              audit-libs(x86-64) = 2.8.1-3.el7</span><br><span class="line"> 您可以尝试添加 --skip-broken 选项来解决该问题</span><br><span class="line"> 您可以尝试执行：rpm -Va --nofiles --nodigest</span><br></pre></td></tr></table></figure><p>看错误内容，是因为Docker所要依赖的包已经安装了，但是太新了不支持。所以需要卸载已安装的，然后安装Docker的时候自动安装所需要的版本。因为有很多包依赖了audit-libs，如果直接用yum进行删除会导致依赖audit-libs的包也被删除，所以这里使用<code>rpm -e --nodeps xxx</code>的方式进行脱离依赖删除</p><p>执行<code>rpm -e --nodeps audit-libs-2.8.1-3.el7_5.1.x86_64</code>后再安装Docker可能会提示下面的内容。这是因为服务器上安装了针对两个平台的audit-libs，只卸载一个的话，另外一个会被错误识别</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">错误： Multilib version problems found. This often means that the root</span><br><span class="line">      cause is something else and multilib version checking is just</span><br><span class="line">      pointing out that there is a problem. Eg.:</span><br><span class="line"></span><br><span class="line">        1. You have an upgrade for audit-libs which is missing some</span><br><span class="line">           dependency that another package requires. Yum is trying to</span><br><span class="line">           solve this by installing an older version of audit-libs of the</span><br><span class="line">           different architecture. If you exclude the bad architecture</span><br><span class="line">           yum will tell you what the root cause is (which package</span><br><span class="line">           requires what). You can try redoing the upgrade with</span><br><span class="line">           --exclude audit-libs.otherarch ... this should give you an error</span><br><span class="line">           message showing the root cause of the problem.</span><br><span class="line"></span><br><span class="line">        2. You have multiple architectures of audit-libs installed, but</span><br><span class="line">           yum can only see an upgrade for one of those architectures.</span><br><span class="line">           If you don&apos;t want/need both architectures anymore then you</span><br><span class="line">           can remove the one with the missing update and everything</span><br><span class="line">           will work.</span><br><span class="line"></span><br><span class="line">        3. You have duplicate versions of audit-libs installed already.</span><br><span class="line">           You can use &quot;yum check&quot; to get yum show these errors.</span><br><span class="line"></span><br><span class="line">      ...you can also use --setopt=protected_multilib=false to remove</span><br><span class="line">      this checking, however this is almost never the correct thing to</span><br><span class="line">      do as something else is very likely to go wrong (often causing</span><br><span class="line">      much more problems).</span><br><span class="line"></span><br><span class="line">      保护多库版本：audit-libs-2.8.1-3.el7.x86_64 != audit-libs-2.8.1-3.el7_5.1.i686</span><br></pre></td></tr></table></figure><p>那解决办法就是把支持另外一个平台的audit-libs也删除，执行<code>rpm -e --nodeps audit-libs-2.8.1-3.el7_5.1.i686</code>，然后再<code>yum install docker-ce</code>安装Docker就没问题了</p><ul><li>执行<code>systemctl start docker</code>无法启动Docker服务，报错信息如下</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Job for docker.service failed because the control process exited with error code. See &quot;systemctl status docker.service&quot; and &quot;journalctl -xe&quot; for details.</span><br></pre></td></tr></table></figure><p>不论Docker服务因为什么原因无法启动都会报这个错误，具体的错误信息需要使用<code>systemctl status docker.service</code>或者<code>journalctl -xe</code>去查看，然后根据具体的错误再去解决（上网寻找资料）</p><p>执行<code>journalctl -xe &gt; error_info</code>将错误信息保存，便于查看。打开error_info，定位到Docker相关的部分。在这里我的最后的错误信息为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dockerd[3518]: Error starting daemon: Error initializing network controller: error obtaining controller instance: failed to create NAT chain DOCKER: iptables failed: iptables --wait -t nat -N DOCKER: iptables v1.4.21: can&apos;t initialize iptables table `nat&apos;: Table does not exist (do you need to insmod?)</span><br></pre></td></tr></table></figure><p>看起来是跟iptables和nat等网络有关的问题。再往上看会发现一堆<code>nf_nat_ipv4: Unknown symbol nf_nat_l3proto_register (err 0)</code>类似的错误，再往上会看到最相关的初始错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kernel: xt_conntrack: Unknown symbol nf_ct_l3proto_module_put (err 0)</span><br><span class="line">kernel: xt_conntrack: Unknown symbol nf_ct_l3proto_try_module_get (err 0)</span><br><span class="line">dockerd[3518]: time=&quot;2018-11-02T13:32:27.308082925+08:00&quot; level=warning msg=&quot;Running modprobe xt_conntrack failed with message: `modprobe: ERROR: could not insert &apos;xt_conntrack&apos;: Unknown symbol in module, or unknown parameter (see dmesg)\ninstall /bin/true \ninsmod /lib/modules/3.10.0-862.14.4.el7.x86_64/kernel/net/netfilter/xt_conntrack.ko.xz`, error: exit status 1&quot;</span><br></pre></td></tr></table></figure><p>分析一下是因为无法加载xt_conntrack内核模块，而无法加载的原因是找不到<code>nf_ct_l3proto_module_put</code>和<code>nf_ct_l3proto_try_module_get</code>这两个内核符号</p><p>使用<code>modinfo xt_conntrack</code>查看模块信息，可以发现其依赖于nf_conntrack。使用<code>modprobe -v nf_conntrack</code>尝试加载，然后<code>lsmod | grep nf_conntrack</code>检查是否加载成功，发现并没有输出相应信息。陷入困境。此处非常感谢 @武帅，指出可能是该模块被加入了黑名单，导致无法加载（自己完全没有考虑过还有模块加载黑名单的存在）。具体的解决办法为：</p><p>打开<code>/etc/modprobe.d/blacklist.conf</code>文件，会看到其中跟nf_conntrack相关的内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># I/O dynamic configuration support for s390x (bz #563228)</span><br><span class="line">blacklist chsc_sch</span><br><span class="line">blacklist nf_conntrack</span><br><span class="line">blacklist nf_conntrack_ipv6</span><br><span class="line">blacklist xt_conntrack</span><br><span class="line">blacklist nf_conntrack_ftp</span><br><span class="line">blacklist xt_state</span><br><span class="line">blacklist iptable_nat</span><br><span class="line">blacklist ipt_REDIRECT</span><br><span class="line">blacklist nf_nat</span><br><span class="line">blacklist nf_conntrack_ipv4</span><br></pre></td></tr></table></figure><p>可以看到使用一系列网络相关的模块被blacklist关键字给屏蔽了，解决办法为将从nf_conntrack开始到nf_conntrack_ipv4之间的行加上注释</p><p>打开<code>/etc/modprobe.d/connectiontracking.conf</code>，也会看到其中跟nf_conntrack相关的内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">install nf_nat /bin/true</span><br><span class="line">install xt_state  /bin/true</span><br><span class="line">install iptable_nat /bin/true</span><br><span class="line">install nf_conntrack /bin/true</span><br><span class="line">install nf_defrag_ipv4   /bin/true</span><br><span class="line">install nf_conntrack_ipv4 /bin/true</span><br><span class="line">install nf_conntrack_ipv6  /bin/true</span><br></pre></td></tr></table></figure><p>这里的含义是如果发生<code>install xxx</code>的行为，跳过加载过程直接返回<code>install /bin/true</code>，所以也起到了加载模块屏蔽的作用。解决办法为将文件的后缀名进行修改，此处将其重命名为<code>connectiontracking.conf.old</code>使其失效（或将文件内容全部加上注释）</p><p>然后再启动Docker服务<code>systemctl start docker</code>，拉取hello-world镜像，成功。问题解决</p><h2 id="2、安装Nvidia-Docker"><a href="#2、安装Nvidia-Docker" class="headerlink" title="2、安装Nvidia-Docker"></a>2、安装Nvidia-Docker</h2><p>安装好了普通的Docker以后，如果想在容器内使用GPU会非常麻烦（并不是不可行），好在Nvidia为了让大家能在容器中愉快使用GPU，基于Docker开发了Nvidia-Docker，使得在容器中深度学习框架调用GPU变得极为容易</p><p>以下内容根据官方安装指南进行简化整理，完成版请移步<a href="https://github.com/NVIDIA/nvidia-docker" target="_blank" rel="noopener">https://github.com/NVIDIA/nvidia-docker</a></p><p>官方指南中针对各个系统都有安装说明，因为使用的是CentOS 7，并且安装的Docker-CE，所以此处参照的是<code>CentOS 7 (docker-ce), RHEL 7.4/7.5 (docker-ce), Amazon Linux 1/2</code></p><h3 id="前置条件"><a href="#前置条件" class="headerlink" title="前置条件"></a>前置条件</h3><ol><li>GNU/Linux x86_64 with kernel version &gt; 3.10</li><li>Docker &gt;= 1.12</li><li>NVIDIA GPU with Architecture &gt; Fermi (2.1)</li><li>NVIDIA drivers ~= 361.93 (untested on older versions)</li></ol><p>一句话，只要安装了显卡驱动就可以（终于不用自己搞CUDA和cuDNN了）</p><p>显卡驱动下载链接：<a href="https://www.nvidia.com/Download/index.aspx" target="_blank" rel="noopener">https://www.nvidia.com/Download/index.aspx</a></p><p>这里根据本机环境，各个选项的选择如下：</p><ul><li>Product Type: Tesla</li><li>Product Series: P-Series</li><li>Product: Tesla P40</li><li>Operating System: Linux 64-bit</li><li>Windows Driver Type: Standard</li><li>CUDA Toolkit: 10.0</li><li>Language: English(US)</li></ul><p>其中CUDA版本选择最高的就行，因为支持高版本CUDA的驱动可以支持低版本CUDA，反过来不行</p><p>下载的文件名为：<code>NVIDIA-Linux-x86_64-410.72.run</code></p><p>关于显卡驱动的安装可以<a href="https://bluesmilery.github.io/blogs/77f7532c/#%E5%AE%89%E8%A3%85">参考之前的文章</a></p><h3 id="删除旧版本"><a href="#删除旧版本" class="headerlink" title="删除旧版本"></a>删除旧版本</h3><p>如果以前安装过nvidia-docker 1.0版本，需要先将其删除</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker volume ls -q -f driver=nvidia-docker | xargs -r -I&#123;&#125; -n1 docker ps -q -a -f volume=&#123;&#125; | xargs -r docker rm -f</span><br><span class="line">yum remove nvidia-docker</span><br></pre></td></tr></table></figure><h3 id="添加相关库"><a href="#添加相关库" class="headerlink" title="添加相关库"></a>添加相关库</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">distribution=$(. /etc/os-release;<span class="built_in">echo</span> <span class="variable">$ID</span><span class="variable">$VERSION_ID</span>)</span><br><span class="line">curl -s -L https://nvidia.github.io/nvidia-docker/<span class="variable">$distribution</span>/nvidia-docker.repo | tee /etc/yum.repos.d/nvidia-docker.repo</span><br></pre></td></tr></table></figure><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum install -y nvidia-docker2</span><br><span class="line">pkill -SIGHUP dockerd</span><br></pre></td></tr></table></figure><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi</span><br></pre></td></tr></table></figure><p>看到如下信息那便表示安装成功了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 410.72       Driver Version: 410.72       CUDA Version: 10.0     |</span><br><span class="line">|-------------------------------+----------------------+----------------------+</span><br><span class="line">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|===============================+======================+======================|</span><br><span class="line">|   0  Tesla P40           Off  | 00000000:00:06.0 Off |                  N/A |</span><br><span class="line">| N/A   23C    P0    48W / 250W |      0MiB / 22919MiB |      0%      Default |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line">|   1  Tesla P40           Off  | 00000000:00:07.0 Off |                  N/A |</span><br><span class="line">| N/A   23C    P0    44W / 250W |      0MiB / 22919MiB |      0%      Default |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line"></span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                       GPU Memory |</span><br><span class="line">|  GPU       PID   Type   Process name                             Usage      |</span><br><span class="line">|=============================================================================|</span><br><span class="line">|  No running processes found                                                 |</span><br><span class="line">+-----------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure><h3 id="一些问题"><a href="#一些问题" class="headerlink" title="一些问题"></a>一些问题</h3><ul><li>安装完nvidia-docker后，之前配置使用Docker Hub国内镜像的那个文件（<code>/etc/docker/daemon.json</code>）内容可能会发生改变，需要检查并如有必要重新添加Docker Hub国内镜像</li><li>docker: Error response from daemon: OCI runtime create failed: container_linux.go:348: starting container process caused “process_linux.go:402: container init caused \”process_linux.go:385: running prestart hook 1 caused \\”error running hook: exit status 1, stdout: , stderr: exec command: [/usr/bin/nvidia-container-cli –load-kmods configure --ldconfig=@/sbin/ldconfig –device=all –compute –utility –require=cuda&gt;=10.0 brand=tesla,driver&gt;=384,driver&lt;385 –pid=46031 /home/dockerData/docker/overlay2/2965080837ad6a78dd0013f677706eac50d147e457a719281750755f7aecbdb1/merged]\\nnvidia-container-cli: requirement error: unsatisfied condition: driver &lt; 385\\n\\”\””: unknown.</li></ul><p>这个是在运行<code>docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi</code>时的报错。报错的关键信息为最后的<code>requirement error: unsatisfied condition: driver &lt; 385</code>，是说驱动不满足要求。这是因为在使用CUDA镜像的时候没指定tag，那么就会使用最新的，而最新版本的CUDA为10.0。出问题的这台服务器上安装的显卡驱动是当时为使用CUDA9安装的。所以问题就是因为现有的显卡驱动不支持CUDA10导致的</p><p>两种解决办法，一种是升级显卡驱动，如果要使用CUDA10可以这么做；另外一种是指定CUDA镜像的tag，因为目前的显卡驱动是支持CUDA9.0的，而此处运行该镜像也只是为了验证Nvidia-Docker是否安装成功，所以可以执行<code>docker run --runtime=nvidia --rm nvidia/cuda:9.0-base nvidia-smi</code>，从而可以成功看到显卡信息</p><h2 id="3、使用容器运行TensorFlow"><a href="#3、使用容器运行TensorFlow" class="headerlink" title="3、使用容器运行TensorFlow"></a>3、使用容器运行TensorFlow</h2><p>接下来需要验证在容器中能否正常使用TensorFlow，参考自：<a href="https://www.tensorflow.org/install/docker" target="_blank" rel="noopener">https://www.tensorflow.org/install/docker</a></p><ul><li>CPU版</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run -it --rm tensorflow/tensorflow \</span><br><span class="line">    python -c <span class="string">"import tensorflow as tf; print(tf.__version__)"</span></span><br></pre></td></tr></table></figure><p>能输出TensorFlow的版本号便是成功，此处为1.11.0</p><ul><li>GPU版</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run --runtime=nvidia -it --rm tensorflow/tensorflow:latest-gpu \</span><br><span class="line">    python -c <span class="string">"import tensorflow as tf; print(tf.contrib.eager.num_gpus())"</span></span><br></pre></td></tr></table></figure><p>能输出TensorFlow调用GPU的信息以及GPU数量便是成功，此处输出为</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">2018-11-02 10:02:03.213647: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA</span><br><span class="line">2018-11-02 10:02:03.827049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class="line">2018-11-02 10:02:03.827656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties:</span><br><span class="line">name: Tesla P40 major: 6 minor: 1 memoryClockRate(GHz): 1.531</span><br><span class="line">pciBusID: 0000:00:06.0</span><br><span class="line">totalMemory: 22.38GiB freeMemory: 22.22GiB</span><br><span class="line">2018-11-02 10:02:03.916592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class="line">2018-11-02 10:02:03.917168: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 1 with properties:</span><br><span class="line">name: Tesla P40 major: 6 minor: 1 memoryClockRate(GHz): 1.531</span><br><span class="line">pciBusID: 0000:00:07.0</span><br><span class="line">totalMemory: 22.38GiB freeMemory: 22.22GiB</span><br><span class="line">2018-11-02 10:02:03.917248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0, 1</span><br><span class="line">2018-11-02 10:02:04.566762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:</span><br><span class="line">2018-11-02 10:02:04.566813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 1</span><br><span class="line">2018-11-02 10:02:04.566824: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N N</span><br><span class="line">2018-11-02 10:02:04.566831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 1:   N N</span><br><span class="line">2018-11-02 10:02:04.567666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21551 MB memory) -&gt; physical GPU (device: 0, name: Tesla P40, pci bus id: 0000:00:06.0, compute capability: 6.1)</span><br><span class="line">2018-11-02 10:02:04.960299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 21551 MB memory) -&gt; physical GPU (device: 1, name: Tesla P40, pci bus id: 0000:00:07.0, compute capability: 6.1)</span><br><span class="line">2</span><br></pre></td></tr></table></figure><p>PS：添加<code>-e NVIDIA_VISIBLE_DEVICES</code>参数可以指定容器使用哪一张（哪几张显卡），例如只使用第一张显卡</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run --runtime=nvidia -e NVIDIA_VISIBLE_DEVICES=0 -it --rm tensorflow/tensorflow:latest-gpu \</span><br><span class="line">    python -c <span class="string">"import tensorflow as tf; print(tf.contrib.eager.num_gpus())"</span></span><br></pre></td></tr></table></figure><h2 id="4、小结"><a href="#4、小结" class="headerlink" title="4、小结"></a>4、小结</h2><p>至此完成了Docker的安装以及在容器内使用GPU</p><p>容器使用GPU并不会对其独占，多个容器使用GPU就如同多个程序使用GPU一样，只要协调好显存与计算力的使用即可</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li>各类官方安装指南<ul><li>Docker：<a href="https://docs.docker.com/install/linux/docker-ce/centos/" target="_blank" rel="noopener">https://docs.docker.com/install/linux/docker-ce/centos/</a></li><li>Nvidia-Docker：<a href="https://github.com/NVIDIA/nvidia-docker" target="_blank" rel="noopener">https://github.com/NVIDIA/nvidia-docker</a></li><li>TensorFlow：<a href="https://www.tensorflow.org/install/docker" target="_blank" rel="noopener">https://www.tensorflow.org/install/docker</a></li></ul></li><li>内核模块屏蔽相关<ul><li><a href="https://www.linuxquestions.org/questions/linux-kernel-70/block-a-kernel-module-to-be-loaded-4175490812/" target="_blank" rel="noopener">https://www.linuxquestions.org/questions/linux-kernel-70/block-a-kernel-module-to-be-loaded-4175490812/</a></li><li><a href="https://www.cyberciti.biz/faq/linux-disable-mounting-of-uncommon-filesystem/" target="_blank" rel="noopener">https://www.cyberciti.biz/faq/linux-disable-mounting-of-uncommon-filesystem/</a></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;随着公司GPU服务器数量的增加，深度学习开发环境的部署逐渐成为了负担。因为机器本身环境配置的差异，即使经验丰富的人也会遇到一些全新的问题，需要耗费时间去解决。而Docker与生俱来的&lt;code&gt;Build once, Run anywhere&lt;/code&gt;特点使得多机器统一环境部署变得极为容易，所以使用Docker势在必行&lt;/p&gt;
&lt;p&gt;本文的主要内容为在腾讯云的GPU服务器上如何安装Docker，并能支持GPU的使用&lt;/p&gt;
&lt;p&gt;（用了Docker后，就可以跟&lt;a href=&quot;https://bluesmilery.github.io/blogs/a687003b/&quot;&gt;前文介绍的Anaconda&lt;/a&gt;拜拜了，因为conda只做到了Python环境隔离，并且每个虚拟环境还是要手动配置）&lt;/p&gt;
    
    </summary>
    
      <category term="技术" scheme="https://bluesmilery.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="Docker" scheme="https://bluesmilery.github.io/tags/Docker/"/>
    
      <category term="GPU" scheme="https://bluesmilery.github.io/tags/GPU/"/>
    
  </entry>
  
  <entry>
    <title>使用Python多进程Pool类时遇见的一些问题</title>
    <link href="https://bluesmilery.github.io/blogs/6d457259/"/>
    <id>https://bluesmilery.github.io/blogs/6d457259/</id>
    <published>2018-09-23T22:55:45.000Z</published>
    <updated>2018-09-23T15:09:23.000Z</updated>
    
    <content type="html"><![CDATA[<p>在学习Python多进程的过程中，虽然Process和Pool都能实现多进程的功能，但是侧重点各有不同：</p><ul><li><p>Process需要自己管理进程，起一个Process就是起一个新进程</p></li><li><p>Pool是进程池，它可以开启固定数量的进程，然后将任务放到一个池子里，系统来调度多进程执行池子里的任务</p></li></ul><p>所以从直观感受上更倾向于使用Pool，但是使用过程中却发现Pool存在一些问题（或者说与Process的差异），所以记录下来给大家分享一下</p><a id="more"></a><p>关于Process和Pool的基本使用可以参考<a href="https://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000/0013868323401155ceb3db1e2044f80b974b469eb06cb43000" target="_blank" rel="noopener">廖雪峰的Python教程</a>，非常简单直观，易于上手。<a href="https://thief.one/2016/11/23/Python-multiprocessing/" target="_blank" rel="noopener">这个博客</a>介绍的也不错</p><h2 id="Process和Pool的使用差异"><a href="#Process和Pool的使用差异" class="headerlink" title="Process和Pool的使用差异"></a>Process和Pool的使用差异</h2><h4 id="1-Pool使用全局变量的问题"><a href="#1-Pool使用全局变量的问题" class="headerlink" title="1.Pool使用全局变量的问题"></a>1.Pool使用全局变量的问题</h4><p>这个问题我不知道该如何定义，只是在开发过程中发现了并探索到相应的解决办法。问题简单描述就是无法使用可变的全局变量（比如for循环），可见如下代码示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multi_task</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">print</span> i, <span class="string">'|'</span>, global_var</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'start'</span></span><br><span class="line">    global_var = <span class="string">"I'm a global variable"</span></span><br><span class="line">    p_pool = Pool(<span class="number">5</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        p_pool.apply_async(func=multi_task)</span><br><span class="line">    p_pool.close()</span><br><span class="line">    p_pool.join()</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'end'</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">start</span></span><br><span class="line"><span class="string">end</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># def multi_task():</span></span><br><span class="line"><span class="comment">#     print global_var</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">start</span></span><br><span class="line"><span class="string">I'm a global variable</span></span><br><span class="line"><span class="string">I'm a global variable</span></span><br><span class="line"><span class="string">I'm a global variable</span></span><br><span class="line"><span class="string">I'm a global variable</span></span><br><span class="line"><span class="string">I'm a global variable</span></span><br><span class="line"><span class="string">end</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><p>执行后会发现输出内容只有start和end，从0-4的数字并没有打印出来，并且连不变的全局变量global_var都没有打印，看起来是没有执行multi_task（实际上是执行了multi_task。如果multi_task有返回值，并且在main中用Pool的get方法获取返回值时会报错<code>NameError: global name &#39;i&#39; is not defined</code>）。但是如果在multi_task里面删掉可变全局变量i，那么全局变量global_var还是能打印出来的</p><p>以上代码换用Process实现却不存在问题，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Process</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multi_task</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">print</span> i, <span class="string">'|'</span>, global_var</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'start'</span></span><br><span class="line">    global_var = <span class="string">"I'm a global variable"</span></span><br><span class="line">    p_list = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        p = Process(target=multi_task)</span><br><span class="line">        p.start()</span><br><span class="line">        p_list.append(p)</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> p_list:</span><br><span class="line">        p.join()</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'end'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">start</span></span><br><span class="line"><span class="string">0 | I'm a global variable</span></span><br><span class="line"><span class="string">1 | I'm a global variable</span></span><br><span class="line"><span class="string">2 | I'm a global variable</span></span><br><span class="line"><span class="string">3 | I'm a global variable</span></span><br><span class="line"><span class="string">4 | I'm a global variable</span></span><br><span class="line"><span class="string">end</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><p>不变的全局变量global_var和变化的全局变量i都能正确的打印出来</p><p>至于原因，因为能力有限没有找到相关解释。但是解决办法有两种：其一就是如上换用Process，缺点是失去了进程池的功能（不过放心，后文会有Process实现进程池功能）；其二是在使用Pool的apply_async方法时将i作为参数传递进去，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multi_task</span><span class="params">(var)</span>:</span></span><br><span class="line">    <span class="keyword">print</span> var, <span class="string">'|'</span>, global_var</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'start'</span></span><br><span class="line">    global_var = <span class="string">"I'm a global variable"</span></span><br><span class="line">    p_pool = Pool(<span class="number">5</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        p_pool.apply_async(func=multi_task, args=(i,))</span><br><span class="line">    p_pool.close()</span><br><span class="line">    p_pool.join()</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'end'</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">start</span></span><br><span class="line"><span class="string">0 | I'm a global variable</span></span><br><span class="line"><span class="string">1 | I'm a global variable</span></span><br><span class="line"><span class="string">3 | I'm a global variable</span></span><br><span class="line"><span class="string">2 | I'm a global variable</span></span><br><span class="line"><span class="string">4 | I'm a global variable</span></span><br><span class="line"><span class="string">end</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><p>可见结果与使用Process一致，两种变量都能正确输出</p><p>注：因是多进程，所以输出顺序并不一定是01234；以及apply_async方法的args参数形式是tuple，所以如果只有一个入参的话要加个逗号</p><h4 id="2-类内使用Pool的问题"><a href="#2-类内使用Pool的问题" class="headerlink" title="2.类内使用Pool的问题"></a>2.类内使用Pool的问题</h4><p>如果在类内直接使用Pool来使用多进程的话，一般会出现问题。示例代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"></span><br><span class="line">global_var = <span class="string">"I'm a global variable"</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SomeClass</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">some_method</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">print</span> <span class="string">'start'</span></span><br><span class="line">        p_pool = Pool(<span class="number">5</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">            p_pool.apply_async(func=self.multi_task, args=(i,))</span><br><span class="line">        p_pool.close()</span><br><span class="line">        p_pool.join()</span><br><span class="line">        <span class="keyword">print</span> <span class="string">'end'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">multi_task</span><span class="params">(self, var)</span>:</span></span><br><span class="line">        <span class="keyword">print</span> var, <span class="string">'|'</span>, global_var</span><br><span class="line"></span><br><span class="line">cls = SomeClass()</span><br><span class="line">cls.some_method()</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">start</span></span><br><span class="line"><span class="string">end</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><p>虽然参数传入了，但从结果上来看很像第一个问题（不传入也是这个结果），看起来是没有执行multi_task（实际上是执行了multi_task。如果multi_task有返回值，并且在main中用Pool的get方法获取返回值时会报错<code>cPickle.PicklingError: Can&#39;t pickle &lt;type &#39;instancemethod&#39;&gt;: attribute lookup __builtin__.instancemethod failed</code>，这也是解释该问题的原因，实例无法序列化）</p><p>通过在网上搜索答案，发现这是因为Pool中使用Queue来进行通信，所有进入队列的数据必须可序列化，包括实例方法。而在python2.7中，实例方法并不能被序列化，从而出现该问题（python3中实例方法可以被序列化了，所以这么使用就没问题了）</p><p>解决方法有多种，最简单的就是把multi_task放到类外，问题迎刃而解</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"></span><br><span class="line">global_var = <span class="string">"I'm a global variable"</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SomeClass</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">some_method</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">print</span> <span class="string">'start'</span></span><br><span class="line">        p_pool = Pool(<span class="number">5</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">            p_pool.apply_async(func=multi_task, args=(i,))</span><br><span class="line">        p_pool.close()</span><br><span class="line">        p_pool.join()</span><br><span class="line">        <span class="keyword">print</span> <span class="string">'end'</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multi_task</span><span class="params">(var)</span>:</span></span><br><span class="line">    <span class="keyword">print</span> var, <span class="string">'|'</span>, global_var</span><br><span class="line"></span><br><span class="line">cls = SomeClass()</span><br><span class="line">cls.some_method()</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">start</span></span><br><span class="line"><span class="string">0 | I'm a global variable</span></span><br><span class="line"><span class="string">1 | I'm a global variable</span></span><br><span class="line"><span class="string">3 | I'm a global variable</span></span><br><span class="line"><span class="string">2 | I'm a global variable</span></span><br><span class="line"><span class="string">4 | I'm a global variable</span></span><br><span class="line"><span class="string">end</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><p>其他的解决办法以及更为细致解释和实验，可以参见以下两篇文章：</p><ul><li><a href="http://xiaorui.cc/2016/01/18/python-multiprocessing%e9%81%87%e5%88%b0cant-pickle-instancemethod%e9%97%ae%e9%a2%98/" target="_blank" rel="noopener">python multiprocessing遇到Can’t pickle instancemethod问题</a></li><li><a href="https://strcpy.me/index.php/archives/318/" target="_blank" rel="noopener">今天遇到的Python多线程、多进程中的几个坑</a></li></ul><h2 id="其他事项"><a href="#其他事项" class="headerlink" title="其他事项"></a>其他事项</h2><h4 id="1-多进程之间的数据共享"><a href="#1-多进程之间的数据共享" class="headerlink" title="1.多进程之间的数据共享"></a>1.多进程之间的数据共享</h4><p>多进程之间不能使用普通的Python数据类型，比如平常使用的list或者dict由父进程传递给子进程后，子进程只可读，写无效。但是Python在multiprocessing模块中给我们提供了一些用于多进程的数据类型：</p><ul><li>multiprocessing.Queue，是多进程安全的队列（FIFO），其主要有put和get两个方法，分别是存储数据和取出数据。使用方法很简单。更多信息可自行搜索</li><li>multiprocessing.Manager，封装了可用于多进程的常用数据类型，例如list和dict。使用Manager创建一个list以后，该list的使用和普通list一致。更多信息可自行搜索</li><li>还有一些其他的，例如multiprocessing.Array，multiprocessing.Pipe等，因为没有使用过所以在此不再赘述，感兴趣可自行搜索</li></ul><p>在实践中发现，使用Manager的list时，虽然使用方法上和普通list一样，但可能因为多进程之间通信的缘故，list中每个元素大小存在限制。因为将之前单进程的代码修改为多进程后，出现报错<code>OverflowError: cannot serialize a string larger than 2 GiB multiprocessing</code>，经查看发现这是在某处给Manager的list添加元素时发生的，并且该元素的确很大，而原来单进程的时候却没有出现过该问题，所以遂产生刚才的猜想（因为没有找到相关解释）。我的解决办法就是把该超大元素切分后再分别添加到Manager的list中</p><h4 id="2-Pool子进程不显示报错信息"><a href="#2-Pool子进程不显示报错信息" class="headerlink" title="2.Pool子进程不显示报错信息"></a>2.Pool子进程不显示报错信息</h4><p>在最一开始使用Pool的时候，发现子进程中如果有错误的话并不会抛出异常，而是该子进程直接死掉然后继续去取任务执行。诸如本文上述的<code>NameError</code>、<code>cPickle.PicklingError</code>、<code>OverflowError</code>都没有抛出，这在查找问题的时候颇费了一番功夫，而使用Process的时候这些异常均能正常抛出。经实践发现，只有多进程执行的任务有返回值，并且在主进程用get方法来获取返回值时，子进程中的异常才会正常抛出，原因是：<code>apply_async</code>返回的是<code>AsyncResult</code>，其中出现的异常只有在调用<code>AsyncResult.get()</code>的时候才会被重新引发（来源于<a href="https://strcpy.me/index.php/archives/318/" target="_blank" rel="noopener">今天遇到的Python多线程、多进程中的几个坑</a>）</p><h4 id="3-使用Process实现进程池功能"><a href="#3-使用Process实现进程池功能" class="headerlink" title="3.使用Process实现进程池功能"></a>3.使用Process实现进程池功能</h4><p>用Queue和while True来模拟进程池，具体可见代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Process, Queue, Manager</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multi_task</span><span class="params">(param, r)</span>:</span></span><br><span class="line">    <span class="string">"""多进程需要执行的任务"""</span></span><br><span class="line">    t = random.random() * <span class="number">5</span></span><br><span class="line">    pid = os.getpid()</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Task %s(pid is %s) will run %s seconds'</span> % (param, pid, t)</span><br><span class="line">    time.sleep(t)</span><br><span class="line">    res = param * param</span><br><span class="line">    r.append(res)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Task %s\' result is %s'</span> % (param, res)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_pool</span><span class="params">(q, r)</span>:</span></span><br><span class="line">    <span class="string">"""模拟进程池"""</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            param = q.get(<span class="keyword">False</span>)</span><br><span class="line">            multi_task(param, r)</span><br><span class="line">        <span class="keyword">except</span> Exception:</span><br><span class="line">            <span class="keyword">if</span> q.empty():</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">your_code</span><span class="params">()</span>:</span></span><br><span class="line">    p_list = []</span><br><span class="line">    <span class="comment"># 先将多进程所要执行的任务的所有参数放入队列中</span></span><br><span class="line">    all_task = Queue()</span><br><span class="line">    <span class="keyword">for</span> task_param <span class="keyword">in</span> range(<span class="number">7</span>):</span><br><span class="line">        all_task.put(task_param)</span><br><span class="line">    <span class="comment"># 结果存储</span></span><br><span class="line">    result = Manager().list()</span><br><span class="line">    <span class="comment"># 启动多进程</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">        p = Process(target=process_pool, args=(all_task, result))</span><br><span class="line">        p.start()</span><br><span class="line">        p_list.append(p)</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> p_list:</span><br><span class="line">        p.join()</span><br><span class="line">    <span class="keyword">print</span> result</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'start'</span></span><br><span class="line">    your_code()</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'end'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">start</span></span><br><span class="line"><span class="string">Task 0(pid is 14642) will run 3.78 seconds</span></span><br><span class="line"><span class="string">Task 1(pid is 14643) will run 4.08 seconds</span></span><br><span class="line"><span class="string">Task 2(pid is 14644) will run 4.62 seconds</span></span><br><span class="line"><span class="string">Task 0' result is 0</span></span><br><span class="line"><span class="string">Task 3(pid is 14642) will run 1.58 seconds</span></span><br><span class="line"><span class="string">Task 1' result is 1</span></span><br><span class="line"><span class="string">Task 4(pid is 14643) will run 1.20 seconds</span></span><br><span class="line"><span class="string">Task 2' result is 4</span></span><br><span class="line"><span class="string">Task 5(pid is 14644) will run 2.74 seconds</span></span><br><span class="line"><span class="string">Task 4' result is 16</span></span><br><span class="line"><span class="string">Task 6(pid is 14643) will run 2.07 seconds</span></span><br><span class="line"><span class="string">Task 3' result is 9</span></span><br><span class="line"><span class="string">Task 6' result is 36</span></span><br><span class="line"><span class="string">Task 5' result is 25</span></span><br><span class="line"><span class="string">[0, 1, 4, 16, 9, 36, 25]</span></span><br><span class="line"><span class="string">end</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在学习Python多进程的过程中，虽然Process和Pool都能实现多进程的功能，但是侧重点各有不同：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Process需要自己管理进程，起一个Process就是起一个新进程&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pool是进程池，它可以开启固定数量的进程，然后将任务放到一个池子里，系统来调度多进程执行池子里的任务&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所以从直观感受上更倾向于使用Pool，但是使用过程中却发现Pool存在一些问题（或者说与Process的差异），所以记录下来给大家分享一下&lt;/p&gt;
    
    </summary>
    
      <category term="技术" scheme="https://bluesmilery.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="Python" scheme="https://bluesmilery.github.io/tags/Python/"/>
    
      <category term="多进程" scheme="https://bluesmilery.github.io/tags/%E5%A4%9A%E8%BF%9B%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>多版本CUDA和TensorFlow共存</title>
    <link href="https://bluesmilery.github.io/blogs/a687003b/"/>
    <id>https://bluesmilery.github.io/blogs/a687003b/</id>
    <published>2018-06-17T18:57:03.000Z</published>
    <updated>2018-11-25T06:42:17.000Z</updated>
    
    <content type="html"><![CDATA[<p>直奔主题，为什么要做这件事情？</p><ul><li>服务器目前安装了CUDA 8.0、TensorFlow 1.3.0，并已有线上服务基于该版本环境配置开发，无法轻易修改本机环境版本</li><li>TensorFlow版本迭代很快，新版本使用更高版本的CUDA，不仅计算速度上更快，而且还有一些更加丰富的API可供使用，例如TensorFlow 1.8.0版本中的eager模式</li></ul><p>所以通过查资料以及实验完成了这件事情，并记录下来以供有同样需求的朋友参考</p><p>文章修改记录：</p><table><thead><tr><th>时间</th><th>修改内容</th></tr></thead><tbody><tr><td>2018.11.25</td><td>统一路径信息、修改部分链接、增加全局pip源设置以及一些小问题修复</td></tr><tr><td></td></tr></tbody></table><a id="more"></a><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>文章开头说了为什么要做这件事，所以其实本质就是为了在不改变原有版本的基础上能够使用新版本，那么便需要调研多版本共存的可行性</p><p>经过调研，CUDA和TensorFlow均可以做到多版本共存，障碍只剩一个——显卡驱动。因为目前的显卡驱动384.66是支持CUDA 8.0的最新版本驱动，然而其不支持更高版本的CUDA。所以考虑使用支持更高版本CUDA的显卡驱动，并测试其是否向下兼容低版本CUDA。测试结果是可行的，这也才有了这篇文章</p><p>整篇文章分为两部分：使用方法、安装过程</p><ul><li>使用方法是将安装过程中创建虚拟环境并配置的过程使用脚本实现，以达到一步到位的便捷使用。脚本内容可以见文后</li><li>安装过程叙述了为了达成多版本共存的目的所做的一些操作或者配置</li></ul><p>本文所使用的Anaconda软件以root权限安装在<code>/home/anaconda3</code>目录下，可根据需要自行修改。安装后服务器上所有用户均可使用Anaconda来创建虚拟环境</p><p>本文由 @李君阳 与我一同完成</p><h1 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h1><p>若要创建指定CUDA版本的虚拟环境，请运行 <strong>create_virtual_env.sh</strong> 脚本（见文末），根据提示输入虚拟环境名称、Python版本、CUDA版本即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Enter the name of Anaconda virtual environment: </span><br><span class="line">Enter the version of Python (default is 3.6.5):</span><br><span class="line">Enter the version of CUDA (8.0/9.0, default is 9.0):</span><br><span class="line">Start to create the environment......</span><br></pre></td></tr></table></figure><p>创建完成后会显示 Everything is DONE!</p><p>随后使用<code>source activate XXX(或者conda activate XXX)</code>命令进入虚拟环境，在虚拟环境中可以使用创建时指定的Python版本和CUDA版本，可自由使用pip安装自己所需要的Python包，不影响本机环境</p><p>要回到本机环境使用<code>source deactivate(或者conda deactivate)</code>退出虚拟环境</p><h1 id="安装过程"><a href="#安装过程" class="headerlink" title="安装过程"></a>安装过程</h1><h2 id="1、安装显卡驱动"><a href="#1、安装显卡驱动" class="headerlink" title="1、安装显卡驱动"></a>1、安装显卡驱动</h2><p>目前服务器上安装的显卡驱动版本为384.66，是支持CUDA 8.0的最新版本驱动。目前已知该驱动无法支持更高版本的CUDA，所以需要验证安装支持高版本CUDA的显卡驱动是否能正常向下兼容低版本CUDA</p><h3 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h3><p>去 <a href="http://www.nvidia.cn/Download/index.aspx" target="_blank" rel="noopener">http://www.nvidia.cn/Download/index.aspx</a> 这里寻找对应的显卡驱动即可，这里选择：</p><ul><li>Product Type: Tesla</li><li>Product Series: M-Series</li><li>Product: M40 24GB</li><li>Operating System: Linux 64-bit</li><li>CUDA Toolkit: 9.1</li><li>Language: English(US)</li></ul><p>这里下载的文件名是：NVIDIA-Linux-x86_64-390.46.run</p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>首先先卸载旧版本驱动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./NVIDIA-Linux-x86_64-384.66.run --uninstall</span><br></pre></td></tr></table></figure><p>然后对新驱动添加可执行权限，安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod u+x NVIDIA-Linux-x86_64-390.46.run</span><br><span class="line">./NVIDIA-Linux-x86_64-390.46.run</span><br></pre></td></tr></table></figure><p>进入安装界面后一路同意就可以</p><h3 id="验证是否支持低版本CUDA"><a href="#验证是否支持低版本CUDA" class="headerlink" title="验证是否支持低版本CUDA"></a>验证是否支持低版本CUDA</h3><p>使用CUDA自带的samples进行验证</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/cuda-8.0/samples/1_Utilities/deviceQuery</span><br><span class="line">make clean</span><br><span class="line">make</span><br><span class="line">./deviceQuery</span><br></pre></td></tr></table></figure><p>如果在最后看到以下信息，则证明该驱动能够支持低版本CUDA</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 9.1, CUDA Runtime Version = 8.0, NumDevs = 2, Device0 = Tesla M40 24GB, Device1 = Tesla M40 24GB</span><br><span class="line">Result = PASS</span><br></pre></td></tr></table></figure><h3 id="一些错误"><a href="#一些错误" class="headerlink" title="一些错误"></a>一些错误</h3><ul><li>ERROR: An NVIDIA kernel module ‘nvidia-uvm’ appears to already be loaded in your kernel.  This may be because it is in use (for example, by an X server, a CUDA program, or the NVIDIA Persistence Daemon), but this may also happen if your kernel was configured without support for module unloading.  Please be sure to exit any programs that may be using the GPU(s) before attempting to upgrade your driver.  If no GPU-based programs are running, you know that your kernel supports module unloading, and you still receive this message, then an error may have occured that has corrupted an NVIDIA kernel module’s usage count, for which the simplest remedy is to reboot your computer.</li></ul><p>使用nvidia-smi命令查看有哪些程序在使用显卡，kill掉。如果没有使用显卡的程序依旧报错，可以重启服务器</p><ul><li>其他错误参考另外两篇文章<a href="https://bluesmilery.github.io/blogs/9ef0e127/">文章1</a>、<a href="https://bluesmilery.github.io/blogs/77f7532c/">文章2</a></li></ul><h2 id="2、安装CUDA-9-0-amp-cuDNN-7-1"><a href="#2、安装CUDA-9-0-amp-cuDNN-7-1" class="headerlink" title="2、安装CUDA-9.0 &amp; cuDNN-7.1"></a>2、安装CUDA-9.0 &amp; cuDNN-7.1</h2><h3 id="CUDA下载"><a href="#CUDA下载" class="headerlink" title="CUDA下载"></a>CUDA下载</h3><p>注意：目前CUDA最新版本为9.1，但是TensorFlow的最新版本1.8还未支持CUDA 9.1，所以只能安装CUDA 9.0</p><p>去 <a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-downloads</a> 这里寻找对应平台的文件下载即可。进入后默认显示最新版本CUDA，选择靠右侧的Legacy Releases，进入后选择<a href="https://developer.nvidia.com/cuda-90-download-archive" target="_blank" rel="noopener">CUDA Toolkit 9.0</a> (Sept 2017)</p><p>这里一些选项的选择为：</p><ul><li>Operating System: Linux</li><li>Architecture: x86_64</li><li>Distribution: CentOS</li><li>Version: 7</li><li>Installer Type: runfile(local)</li></ul><p>下面会显示两个安装文件，一个 Base Installer ，两个Patch。安装完Base后再安装Patch即可</p><p>直接复制下载链接，在服务器上wget下载即可</p><h3 id="CUDA安装"><a href="#CUDA安装" class="headerlink" title="CUDA安装"></a>CUDA安装</h3><p>添加可执行权限，安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod u+x cuda_9.0.176_384.81_linux.run</span><br><span class="line">./cuda_9.0.176_384.81_linux.run</span><br></pre></td></tr></table></figure><p>接下来会有一系列选择</p><ul><li>是否安装显卡驱动：选no，因为已经装了最新的驱动</li><li>是否安装CUDA 9.0：选yes</li><li>输入安装位置：默认即可，因为默认位置就区分开了不同的CUDA版本</li><li>是否创建软链：选no，因为要实现多版本CUDA共存。如果以前安装过CUDA并创建过软链，需要删除</li><li>是否安装sample：选no，因为在安装路径下会有一份sample，这个是问是否要在自己的目录下再安装一份</li></ul><p>然后再安装一下两个补丁即可：cuda_9.0.176.1_linux.run、cuda_9.0.176.2_linux.run</p><h3 id="CUDA测试"><a href="#CUDA测试" class="headerlink" title="CUDA测试"></a>CUDA测试</h3><p>进入samples目录，选择第一个例子进行测试</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/cuda-9.0/samples/1_Utilities/deviceQuery</span><br><span class="line">make</span><br><span class="line">./deviceQuery</span><br></pre></td></tr></table></figure><p>如果在最后看到以下信息，则证明CUDA 9.0安装成功</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 9.1, CUDA Runtime Version = 9.0, NumDevs = 2</span><br><span class="line">Result = PASS</span><br></pre></td></tr></table></figure><h3 id="cuDNN下载"><a href="#cuDNN下载" class="headerlink" title="cuDNN下载"></a>cuDNN下载</h3><p>去 <a href="https://developer.nvidia.com/rdp/cudnn-download" target="_blank" rel="noopener">https://developer.nvidia.com/rdp/cudnn-download</a> 选择对应的版本下载即可。不过需要先注册开发者账号后才可以下载</p><p>刚才安装的CUDA是9.0，所以我们选择</p><ul><li>Download cuDNN v7.1.3 (April 17, 2018), for CUDA 9.0</li><li>cuDNN v7.1.3 Library for Linux</li></ul><h3 id="cuDNN安装"><a href="#cuDNN安装" class="headerlink" title="cuDNN安装"></a>cuDNN安装</h3><p>执行解压操作</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir cuda-9.0_cudnn</span><br><span class="line">tar zxvf cudnn-9.0-linux-x64-v7.1.tgz -C ./cuda-9.0_cudnn</span><br><span class="line"><span class="built_in">cd</span> cuda-9.0_cudnn</span><br></pre></td></tr></table></figure><p>解压后的文件夹是cuda。执行以下操作把文件复制到相应的位置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cp cuda/include/cudnn.h /usr/<span class="built_in">local</span>/cuda-9.0/include/</span><br><span class="line">cp cuda/lib64/libcudnn* /usr/<span class="built_in">local</span>/cuda-9.0/lib64/</span><br><span class="line">chmod a+r /usr/<span class="built_in">local</span>/cuda-9.0/include/cudnn.h</span><br><span class="line">chmod a+r /usr/<span class="built_in">local</span>/cuda-9.0/lib64/libcudnn*</span><br></pre></td></tr></table></figure><h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><p>以上过程基于服务器上已经有一个CUDA版本。如果服务器上未安装过CUDA，按照这个过程依次安装两个版本CUDA即可，只需注意不要创建软链。如果打算在本机环境使用CUDA，则需要配置环境变量。如果打算都在虚拟python环境下运行，则本机不需要配置相关环境变量，在之后启动虚拟环境时配置</p><h2 id="3、安装Anaconda"><a href="#3、安装Anaconda" class="headerlink" title="3、安装Anaconda"></a>3、安装Anaconda</h2><h3 id="下载安装"><a href="#下载安装" class="headerlink" title="下载安装"></a>下载安装</h3><p>使用该<a href="https://repo.anaconda.com/archive/Anaconda3-5.1.0-Linux-x86_64.sh" target="_blank" rel="noopener">下载链接</a>下载Anaconda安装脚本，默认Python版本是3.6.5</p><p>添加可执行权限后，执行安装</p><p>安装完一堆包之后会询问是否要添加环境变量，这里选择no，稍后再添加。然后会继续询问是否安装VSCode，选择no。此时安装完成</p><p>因为我们使用Anaconda只是作为虚拟python环境管理，不需要用到其自带的python以及相关包，所以需要将Anaconda的bin放到path后，否则系统的python会失效。所以将下面这一行添加到需要使用Anaconda账户的<code>.bashrc</code>文件中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export PATH=$PATH:/home/anaconda3/bin</span><br></pre></td></tr></table></figure><p>source一下或者退出重新登陆</p><p>然后执行<code>ln -s /home/anaconda3/etc/profile.d/conda.sh /etc/profile.d/conda.sh</code>，这是为了服务器上所有用户都可以使用<code>conda activate</code>和<code>conda deactivate</code>来进入和退出虚拟环境（即source和conda都可以）</p><h3 id="修改国内镜像仓库"><a href="#修改国内镜像仓库" class="headerlink" title="修改国内镜像仓库"></a>修改国内镜像仓库</h3><p>因为Anaconda默认使用国外的下载地址，为了提高国内的下载速度，可以使用国内的清华镜像源或者中科大镜像源，二选一即可，执行三条命令</p><p>加入–system是配置全局的conda config。若个人账户想修改，则取消–system即可，此时在个人账户下会覆盖全局配置（类似 git config ）。个人账号相关配置会写在 ~/.condarc 里面</p><ul><li>清华Anaconda镜像</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda config --system --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">conda config --system --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span><br><span class="line">conda config --system --<span class="built_in">set</span> show_channel_urls yes</span><br></pre></td></tr></table></figure><ul><li>中科大Anaconda镜像</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda config --system --add channels http://mirrors.ustc.edu.cn/anaconda/pkgs/free/</span><br><span class="line">conda config --system --add channels http://mirrors.ustc.edu.cn/anaconda/pkgs/main/</span><br><span class="line">conda config --system --<span class="built_in">set</span> show_channel_urls yes</span><br></pre></td></tr></table></figure><h3 id="创建虚拟环境"><a href="#创建虚拟环境" class="headerlink" title="创建虚拟环境"></a>创建虚拟环境</h3><p>基本命令为 <code>conda create --name XXX</code>，–name可缩写为-n</p><p>如果在创建虚拟环境时报错（跟urls.txt或者environments.txt有关，一般是第一次使用Anaconda时会出现），参考下方一些错误部分</p><p>关于创建虚拟环境，分以下三种情况陈述</p><ul><li><code>conda create --name test1</code></li></ul><p>此时虚拟环境并不起作用，进入test1后会发现实际上使用的依旧是本机Python环境，并且在test1中使用pip安装或者删除包均是对本机环境操作</p><p>所以不要使用这种方式</p><ul><li><code>conda create --name test2 python=X.X.X</code></li></ul><p>该命令会创建一个名为test2的纯净虚拟环境，并且会安装python X.X.X版本，无其他Python包。进入test2后，使用<code>python -V</code>和<code>pip -V</code>可以看到使用的的确是该虚拟环境，并且在test2中使用pip安装或者删除包均是对该虚拟环境进行操作，不会影响本机环境</p><p>这里python的入参有多种</p><p>如果只写python，则安装的是Anaconda自带的Python版本（在这里是3.6.5）</p><p>如果是python=2或者python=3，则安装的是python2或者python3的最新版本（目前是2.7.15和3.6.5）</p><p>如果写了二级版本号，例如python=2.6或者python=3.5，则安装的是python2.6或者python3.5分支的最新版本（目前是2.6.9和3.5.5）</p><p>如果写了最小的版本号，例如python=2.7.8或者python=3.6.2，则安装的便是输入的特定版本</p><ul><li><code>conda create --name test3 python=2.7 numpy</code></li></ul><p>该命令会创建一个名为test3的纯净虚拟环境，并且会安装python2.7的最新版本，在此基础上还会额外安装numpy包。当然numpy包也可以类似Python一样指定版本，例如numpy=1.10，则会安装1.10.4版本。不指定则安装的就是最新版本。其他同上</p><ul><li><code>conda create --name test4 numpy</code></li></ul><p>该命令会创建一个名为test4的纯净虚拟环境，并且会安装Anaconda自带的Python版本（在这里是3.6.5），在此基础上还会额外安装numpy包。其他同上</p><h3 id="启动和退出虚拟环境"><a href="#启动和退出虚拟环境" class="headerlink" title="启动和退出虚拟环境"></a>启动和退出虚拟环境</h3><ul><li>启动</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> activate XXX(或者conda activate XXX)</span><br></pre></td></tr></table></figure><p>启动成功后在命令行前会有虚拟环境的名字（XXX）</p><ul><li>退出</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> deactivate(或者conda deactivate)</span><br></pre></td></tr></table></figure><h3 id="其他常用指令"><a href="#其他常用指令" class="headerlink" title="其他常用指令"></a>其他常用指令</h3><p>查看有哪些虚拟环境：<code>conda env list</code></p><p>删除虚拟环境：<code>conda env remove -n XXX</code></p><h3 id="多版本TensorFlow共存"><a href="#多版本TensorFlow共存" class="headerlink" title="多版本TensorFlow共存"></a>多版本TensorFlow共存</h3><p>在不同的虚拟环境中安装不同版本的TensorFlow即可。若不同版本的TensorFlow依赖的CUDA版本不同，则参照下一小节“给虚拟环境指定CUDA版本”来操作</p><h3 id="设置全局pip源配置"><a href="#设置全局pip源配置" class="headerlink" title="设置全局pip源配置"></a>设置全局pip源配置</h3><p>进入到虚拟环境后，使用pip安装Python包的时候会发现其从原始的国外地址进行下载。有时下载速度较慢，所以我们要更新pip源为国内源</p><p>编辑 <code>/etc/pip.conf</code>文件（若无则创建），添加以下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[global]</span><br><span class="line">trusted-host = mirrors.aliyun.com</span><br><span class="line">index-url = http://mirrors.aliyun.com/pypi/simple</span><br></pre></td></tr></table></figure><p>该方式可以进行全局pip源配置，便于服务器上所有用户均可使用。若只要某些用户使用，那么编辑的就是<code>~/.pip/pip.conf</code>文件</p><h3 id="一些错误-1"><a href="#一些错误-1" class="headerlink" title="一些错误"></a>一些错误</h3><ul><li><p>NotWritableError: The current user does not have write permissions to a required path.</p><p>path: /home/username/.conda/pkgs/urls.txt</p><p>uid: 1001</p><p>gid: 1002</p><p>If you feel that permissions on this path are set incorrectly, you can manually change them by executing</p><p>$ sudo chown 1001:1002 /home/username/.conda/pkgs/urls.txt</p><p>In general, it’s not advisable to use ‘sudo conda’.</p></li></ul><p>自己创建该文件。类似的还有 <code>/home/username/.conda/environments.txt</code></p><h2 id="4、给虚拟环境指定CUDA版本"><a href="#4、给虚拟环境指定CUDA版本" class="headerlink" title="4、给虚拟环境指定CUDA版本"></a>4、给虚拟环境指定CUDA版本</h2><h3 id="处理环境变量"><a href="#处理环境变量" class="headerlink" title="处理环境变量"></a>处理环境变量</h3><p>假设此时服务器上安装了两个CUDA版本，分别为CUDA 8.0（/usr/local/cuda-8.0）和CUDA 9.0（/usr/local/cuda-9.0），并且在本机上配置了一个版本（例如8.0）的环境变量（$CUDA_HOME, $LD_LIBRARY_PATH）。没配置更好，这里配置是为了验证切换虚拟环境时指定的CUDA版本是否发生相应的改变</p><p>首先创建一个虚拟环境，让这个虚拟环境使用CUDA 9.0的lib</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n cuda_test python=3</span><br></pre></td></tr></table></figure><p>然后设定启动虚拟环境以及退出时需要执行的脚本，为了使虚拟环境启动时环境变量不同于本机环境，退出时又恢复为本机环境</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /home/username/.conda/envs/cuda_test/etc/conda/activate.d</span><br><span class="line">mkdir -p /home/username/.conda/envs/cuda_test/etc/conda/deactivate.d</span><br></pre></td></tr></table></figure><ul><li>编辑启动脚本 <code>vim /home/username/.conda/envs/cuda_test/etc/conda/activate.d/activate.sh</code></li></ul><p>输入以下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ORIGINAL_CUDA_HOME=$CUDA_HOME</span><br><span class="line">ORIGINAL_LD_LIBRARY_PATH=$LD_LIBRARY_PATH</span><br><span class="line">export CUDA_HOME=/usr/local/cuda-9.0</span><br><span class="line">export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH</span><br></pre></td></tr></table></figure><p>添加执行权限 <code>chmod +x /home/username/.conda/envs/cuda_test/etc/conda/activate.d/activate.sh</code></p><ul><li>编辑退出脚本 <code>vim /home/username/.conda/envs/cuda_test/etc/conda/deactivate.d/deactivate.sh</code></li></ul><p>输入以下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export CUDA_HOME=$ORIGINAL_CUDA_HOME</span><br><span class="line">export LD_LIBRARY_PATH=$ORIGINAL_LD_LIBRARY_PATH</span><br><span class="line">unset ORIGINAL_CUDA_HOME</span><br><span class="line">unset ORIGINAL_LD_LIBRARY_PATH</span><br></pre></td></tr></table></figure><p>添加执行权限 <code>chmod +x /home/username/.conda/envs/cuda_test/etc/conda/deactivate.d/deactivate.sh</code></p><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>首先先在本机查看环境变量 CUDA_HOME 和 LD_LIBRARY_PATH</p><p><code>echo $CUDA_HOME</code> 结果为/usr/local/cuda-8.0</p><p><code>echo $LD_LIBRARY_PATH</code> 结果为/usr/local/cuda-8.0/lib64</p><p>进入虚拟环境</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> activate cuda_test</span><br></pre></td></tr></table></figure><p>查看此时的环境变量 </p><p><code>echo $CUDA_HOME</code> 结果为/usr/local/cuda-9.0</p><p><code>echo $LD_LIBRARY_PATH</code> 结果为/usr/local/cuda-9.0/lib64:/usr/local/cuda-8.0/lib64</p><p>退出虚拟环境</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> deactivate</span><br></pre></td></tr></table></figure><p>再次查看环境变量，会发现恢复为/usr/local/cuda-8.0和/usr/local/cuda-8.0/lib64，证明在虚拟环境中指定CUDA版本lib成功</p><p>PS：也可以通过在虚拟环境中安装1.8.0版本的tensorflow-gpu来验证，因为其必须使用CUDA 9.0才能正确import，否则会报错ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory</p><h2 id="5、小结"><a href="#5、小结" class="headerlink" title="5、小结"></a>5、小结</h2><p>至此，完成以上操作后，在服务器上即可达到多版本CUDA、TensorFlow共存的目的</p><h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><p>通过第四小节可以看到，可以通过编写虚拟环境的启动脚本和退出脚本来管理虚拟环境中的环境变量，使其启动时不同于本机环境，退出时又恢复为本机环境。但是如果需要在脚本中对PATH环境变量做改变的话，会发生一些问题——退出时没有正确恢复PATH环境变量</p><p>在对虚拟环境启动和退出的过程做了梳理后，可以有办法解决这个问题（使用上述ORIGINAL_的方式做中间转换是无法解决的）</p><p>假设启动前PATH内容为/usr/bin</p><p>启动过程：</p><ol><li>在启动时，conda会先修改PATH，在本机PATH的前面加上<code>/home/username/.conda/envs/cuda_test/bin:</code> ，这是为了在虚拟环境中能够使用虚拟环境自己的Python。此时PATH内容为<code>PATH=/home/username/.conda/envs/cuda_test/bin:/usr/bin</code>，然后export该PATH</li><li>然后执行添加的启动脚本activate.sh。假如在其中对PATH做了添加，此时PATH内容为<code>PATH=/xxxx/bin:/home/username/.conda/envs/cuda_test/bin:/usr/bin</code>。进入虚拟环境中echo $PATH得到的内容也是<code>/xxxx/bin:/home/username/.conda/envs/cuda_test/bin:/usr/bin</code></li><li>然后conda会将此时的PATH在某个地方备份记录一下，并且将其在第一步添加的内容删掉。也就是说这时conda会保留一个内容为<code>/xxxx/bin:/usr/bin</code>的PATH</li></ol><p>退出过程：</p><ol><li>首先执行添加的退出脚本deactivate.sh</li><li>将启动第三步存下来的PATH拿出来直接export。所以在deactivate.sh中使用ORIGINAL_的方式做恢复是无效的</li></ol><p>当恢复到本机环境时，PATH的内容为<code>/xxxx/bin:/usr/bin</code>的PATH，与启动前不一样</p><p>启动过程主要执行的是<code>/home/anaconda3/etc/profile.d/conda.sh</code>中的<code>_conda_activate()</code>方法。退出过程主要执行的是<code>/home/anaconda3/etc/profile.d/conda.sh</code>中的<code>_conda_deactivate()</code>方法</p><p>目前想到的一种解决方案如下：（未尝试）</p><ol><li>在退出脚本deactivate.sh的头部添加<code>FIX_PATH=$CUDA_HOME</code></li><li>在<code>/home/anaconda3/etc/profile.d/conda.sh</code>的<code>_conda_deactivate()</code>方法中的<code>eval &quot;$ask_conda&quot;</code>之后，在PATH中正则匹配FIX_PATH的内容并将其删掉，再重新export PATH</li></ol><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://www.jianshu.com/p/4ac737fd6b45" target="_blank" rel="noopener">https://www.jianshu.com/p/4ac737fd6b45</a></p><p><a href="https://blog.kovalevskyi.com/multiple-version-of-cuda-libraries-on-the-same-machine-b9502d50ae77" target="_blank" rel="noopener">https://blog.kovalevskyi.com/multiple-version-of-cuda-libraries-on-the-same-machine-b9502d50ae77</a></p><p><a href="https://www.digitalocean.com/community/tutorials/how-to-install-the-anaconda-python-distribution-on-ubuntu-16-04" target="_blank" rel="noopener">https://www.digitalocean.com/community/tutorials/how-to-install-the-anaconda-python-distribution-on-ubuntu-16-04</a></p><h1 id="create-virtual-env-sh"><a href="#create-virtual-env-sh" class="headerlink" title="create_virtual_env.sh"></a>create_virtual_env.sh</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Used to create a Anaconda virtual environment <span class="keyword">for</span> CUDA and TensorFlow.</span></span><br><span class="line"></span><br><span class="line">echo ""</span><br><span class="line">echo "This script is used to create a Anaconda virtual environment for CUDA and TensorFlow."</span><br><span class="line"></span><br><span class="line">echo ""</span><br><span class="line">read -p "Enter the name of Anaconda virtual environment: " ENV_NAME</span><br><span class="line">if [ -z $&#123;ENV_NAME&#125; ]; then</span><br><span class="line">    echo ""</span><br><span class="line">    echo "Need a virtual environment name!" &gt;&amp;2</span><br><span class="line">    exit 1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">echo ""</span><br><span class="line">read -p "Enter the version of Python (default is 3.6.5): " PYTHON_VERSION</span><br><span class="line">if [ -z $&#123;PYTHON_VERSION&#125; ]; then</span><br><span class="line">    PYTHON_VERSION="3.6.5"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">echo ""</span><br><span class="line">read -p "Enter the version of CUDA (8.0/9.0, default is 9.0): " CUDA_VERSION</span><br><span class="line">if [ -z $&#123;CUDA_VERSION&#125; ]; then</span><br><span class="line">    CUDA_VERSION="9.0"</span><br><span class="line">elif [ $&#123;CUDA_VERSION&#125; != "8.0" ] &amp;&amp; [ $&#123;CUDA_VERSION&#125; != "9.0" ]; then</span><br><span class="line">    echo ""</span><br><span class="line">    echo "CUDA version must be 8.0 or 9.0!" &gt;&amp;2</span><br><span class="line">    exit 1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">echo ""</span><br><span class="line">echo "Start to create the environment......"</span><br><span class="line">echo ""</span><br><span class="line"></span><br><span class="line">cmd_create="conda create -n $&#123;ENV_NAME&#125; python=$&#123;PYTHON_VERSION&#125;"</span><br><span class="line">eval $&#123;cmd_create&#125;</span><br><span class="line"></span><br><span class="line">ACTIVATE_PATH=$HOME/.conda/envs/$&#123;ENV_NAME&#125;/etc/conda/activate.d</span><br><span class="line">DEACTIVATE_PATH=$HOME/.conda/envs/$&#123;ENV_NAME&#125;/etc/conda/deactivate.d</span><br><span class="line"></span><br><span class="line">activate_string="</span><br><span class="line">ORIGINAL_CUDA_HOME=\$CUDA_HOME</span><br><span class="line">ORIGINAL_LD_LIBRARY_PATH=\$LD_LIBRARY_PATH</span><br><span class="line"></span><br><span class="line">export CUDA_HOME=/usr/local/cuda-$CUDA_VERSION</span><br><span class="line">export LD_LIBRARY_PATH=\$CUDA_HOME/lib64:\$LD_LIBRARY_PATH"</span><br><span class="line"></span><br><span class="line">deactivate_string='</span><br><span class="line">export CUDA_HOME=$ORIGINAL_CUDA_HOME</span><br><span class="line">export LD_LIBRARY_PATH=$ORIGINAL_LD_LIBRARY_PATH</span><br><span class="line"></span><br><span class="line">unset ORIGINAL_CUDA_HOME</span><br><span class="line">unset ORIGINAL_LD_LIBRARY_PATH'</span><br><span class="line"></span><br><span class="line">mkdir -p $&#123;ACTIVATE_PATH&#125;</span><br><span class="line">echo "$&#123;activate_string&#125;" &gt; $&#123;ACTIVATE_PATH&#125;/activate.sh</span><br><span class="line">chmod +x $&#123;ACTIVATE_PATH&#125;/activate.sh</span><br><span class="line"></span><br><span class="line">mkdir -p $&#123;DEACTIVATE_PATH&#125;</span><br><span class="line">echo "$&#123;deactivate_string&#125;" &gt; $&#123;DEACTIVATE_PATH&#125;/deactivate.sh</span><br><span class="line">chmod +x $&#123;DEACTIVATE_PATH&#125;/deactivate.sh</span><br><span class="line"></span><br><span class="line">echo "Everything is DONE!"</span><br><span class="line">echo ""</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;直奔主题，为什么要做这件事情？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;服务器目前安装了CUDA 8.0、TensorFlow 1.3.0，并已有线上服务基于该版本环境配置开发，无法轻易修改本机环境版本&lt;/li&gt;
&lt;li&gt;TensorFlow版本迭代很快，新版本使用更高版本的CUDA，不仅计算速度上更快，而且还有一些更加丰富的API可供使用，例如TensorFlow 1.8.0版本中的eager模式&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所以通过查资料以及实验完成了这件事情，并记录下来以供有同样需求的朋友参考&lt;/p&gt;
&lt;p&gt;文章修改记录：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;时间&lt;/th&gt;
&lt;th&gt;修改内容&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;2018.11.25&lt;/td&gt;
&lt;td&gt;统一路径信息、修改部分链接、增加全局pip源设置以及一些小问题修复&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
    
    </summary>
    
      <category term="技术" scheme="https://bluesmilery.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="TensorFlow" scheme="https://bluesmilery.github.io/tags/TensorFlow/"/>
    
      <category term="CUDA" scheme="https://bluesmilery.github.io/tags/CUDA/"/>
    
  </entry>
  
  <entry>
    <title>腾讯云GPU服务器搭建TensorFlow开发环境</title>
    <link href="https://bluesmilery.github.io/blogs/77f7532c/"/>
    <id>https://bluesmilery.github.io/blogs/77f7532c/</id>
    <published>2018-05-13T16:12:56.000Z</published>
    <updated>2018-07-14T10:58:35.000Z</updated>
    
    <content type="html"><![CDATA[<p>日前，我司开始使用腾讯云的GPU服务器，那自然需要在其上配置TF的开发环境。之前写过在CentOS6上进行源码编译安装TF（<a href="https://bluesmilery.github.io/blogs/9ef0e127/">传送门</a>），所以这篇也是在以前的基础上修改而来，不过因为系统版本换为CentOS7，许多步骤都可以省略了，方便不少。不过仍有部分操作或问题与之前不一致，在此也会对其说明</p><p>最终配置的环境为CUDA-8.0 + cuDNN-6.0 + TensorFlow-1.3.0</p><a id="more"></a><h1 id="总览"><a href="#总览" class="headerlink" title="总览"></a>总览</h1><p>目前总共配置过两种腾讯云GPU服务器，以下内容主要为第一种的安装过程。第二种在此基础上，降低了系统内核版本以便安装显卡驱动</p><p>第一种：CentOS Linux release 7.4.1708 (Core)</p><p>第二种：CentOS Linux release 7.5.1804 (Core)</p><p>目前内核版本均为：3.10.0-693.21.1.el7.x86_64（第二种默认为3.10.0-862.el7.x86_64，已降级）</p><p>目前的降级是建立在使用CUDA-8.0的基础上，如果是使用更高版本的CUDA和TF，或许不需要降级。具体可参考是否出现本文安装Nvidia驱动部分的第一个错误</p><p>配置过程主要分为以下几个部分：</p><ul><li>Python相关</li><li>安装NVIDIA Driver、CUDA、cuDNN</li><li>安装TensorFlow</li></ul><p>因为本次能够通过pip来安装TensorFlow，并且gcc版本符合安装要求，所以省略了之前安装Java8、Bazel，升级gcc的步骤</p><p>但安装过程中仍有部分操作或者问题与之前的不一致，不同之处主要在于：</p><ul><li>编译安装Python时需指定UCS编码方式</li><li>pip需要使用9.0.1版本</li><li>使用yum需要额外修改/usr/libexec/urlgrabber-ext-down文件</li><li>下载显卡驱动时需下载.run格式文件</li></ul><p>之后对安装过程进行简述，所有环境变量添加在/etc/profile.d/path.sh文件中（没有可创建）</p><h1 id="安装过程"><a href="#安装过程" class="headerlink" title="安装过程"></a>安装过程</h1><h2 id="1、Python更新"><a href="#1、Python更新" class="headerlink" title="1、Python更新"></a>1、Python更新</h2><p>通过 <code>python -V</code> 可以看到服务器上python的版本为2.7.5，已经符合TensorFlow安装要求。如果没有特殊需求，不建议再安装更(四声)新的2.7版本，例如2.7.14，因为父版本相同的情况下差别很小，只是一些Bug修复和性能改进，而安装2.7.14所要付出的代价（繁琐程度）是蛮大的，因为父版本相同会产生一些问题。但是如果需要安装Python3的话，那完全没问题</p><p>以下为安装Python2.7.14的过程。Python3类似</p><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><p>安装一些系统依赖</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">yum groupinstall -y <span class="string">'development tools'</span></span><br><span class="line">yum install -y zlib-devel bzip2-devel openssl-devel xz-libs wget</span><br><span class="line">yum install readline* rlwrap</span><br><span class="line">yum install sqlite-devel tk-devel</span><br></pre></td></tr></table></figure><p>下载Python2.7源码<br><a href="https://www.python.org/ftp/python/2.7.14/Python-2.7.14.tar.xz" target="_blank" rel="noopener">https://www.python.org/ftp/python/2.7.14/Python-2.7.14.tar.xz</a></p><p>解压</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xz -d Python-2.7.14.tar.xz</span><br><span class="line">tar -xvf Python-2.7.14.tar</span><br></pre></td></tr></table></figure><p>进入目录<code>cd Python-2.7.14</code>，接下来的操作都在这个目录下进行</p><h3 id="配置编译安装"><a href="#配置编译安装" class="headerlink" title="配置编译安装"></a>配置编译安装</h3><p>配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./configure --prefix=/usr/<span class="built_in">local</span> --<span class="built_in">enable</span>-unicode=ucs4</span><br></pre></td></tr></table></figure><p>关于–enable-unicode=ucs4参数后续会说明其作用</p><p>编译，用时几分钟。然后安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make &amp;&amp; make install</span><br></pre></td></tr></table></figure><h3 id="使新版本生效"><a href="#使新版本生效" class="headerlink" title="使新版本生效"></a>使新版本生效</h3><p>两种方式：将路径加入PATH环境变量、软连接</p><ul><li>加入PATH</li></ul><p>将以下代码加入/etc/profile.d/path.sh文件中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export PATH=/usr/local/bin:$PATH</span><br></pre></td></tr></table></figure><ul><li>软连接</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mv /usr/bin/python /usr/bin/python2.7.5</span><br><span class="line">ln -s /usr/<span class="built_in">local</span>/bin/python2.7 /usr/bin/python</span><br></pre></td></tr></table></figure><p>此时通过 <code>python -V</code> 可以查看Python版本已经为2.7.14。如果还想使用Python2.7.5，那么执行<code>python2.7.5</code>即可</p><h3 id="解决yum失效问题"><a href="#解决yum失效问题" class="headerlink" title="解决yum失效问题"></a>解决yum失效问题</h3><p>因为yum依赖的是原来的Python版本，所以做以下修改</p><p>将/usr/bin/yum以及/usr/libexec/urlgrabber-ext-down的第一行均改为</p><p>#!/usr/bin/python2.7.5</p><p>修改前者是为了升级Python后能够运行yum，修改后者的原因是使用yum安装软件会报以下错误</p><p>ImportError:No module nameed urlgrabber.grabber</p><p>修改后错误消失</p><h3 id="更新pip"><a href="#更新pip" class="headerlink" title="更新pip"></a>更新pip</h3><p>从<a href="https://bootstrap.pypa.io/get-pip.py" target="_blank" rel="noopener">该地址</a>﻿下载get-pip.py文件</p><p>然后执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python get-pip.py</span><br></pre></td></tr></table></figure><p>该文件会安装pip以及setuptools等工具</p><h3 id="一些错误"><a href="#一些错误" class="headerlink" title="一些错误"></a>一些错误</h3><ul><li>pkg_resources.DistributionNotFound: The ‘pip==9.0.1’ distribution was not found and is required by the application</li></ul><p>如果遇到该错误，那么就安装指定的pip版本，下载地址为：<a href="https://github.com/pypa/pip/releases" target="_blank" rel="noopener">https://github.com/pypa/pip/releases</a> ，下载指定的版本。这里下载的是pip-9.0.1版本，然后安装问题解决</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">unzip 9.0.1.zip</span><br><span class="line"><span class="built_in">cd</span> pip-9.0.1</span><br><span class="line">python setup.py install</span><br></pre></td></tr></table></figure><h2 id="2、NVIDIA-Driver"><a href="#2、NVIDIA-Driver" class="headerlink" title="2、NVIDIA Driver"></a>2、NVIDIA Driver</h2><h3 id="准备工作-1"><a href="#准备工作-1" class="headerlink" title="准备工作"></a>准备工作</h3><p>安装一些系统依赖</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install kernel-devel-xxx</span><br></pre></td></tr></table></figure><p>xxx是内核版本号，可以通过 uname -r 查看</p><h3 id="下载驱动程序"><a href="#下载驱动程序" class="headerlink" title="下载驱动程序"></a>下载驱动程序</h3><p>去 <a href="http://www.nvidia.cn/Download/index.aspx" target="_blank" rel="noopener">http://www.nvidia.cn/Download/index.aspx</a> 这里寻找对应的显卡驱动即可，这里选择：</p><ul><li>Product Type: Tesla</li><li>Product Series: M-Series</li><li>Product: M40</li><li>Operating System: Linux 64-bit</li><li>CUDA Toolkit: 8.0</li><li>Language: English(US)</li></ul><p>这里下载的文件名是：NVIDIA-Linux-x86_64-384.66.run</p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>添加可执行权限，安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod +x NVIDIA-Linux-x86_64-384.66.run</span><br><span class="line">./NVIDIA-Linux-x86_64-384.66.run</span><br></pre></td></tr></table></figure><p>进入安装界面后一路同意就可以，是否安装32位的库，我选择的同意。对于最后出现的warning我选择了忽略</p><p>安装完成后执行<code>nvidia-smi</code>后能看到一些显卡信息，若运行错误可重启服务器<code>reboot</code></p><h3 id="一些错误-1"><a href="#一些错误-1" class="headerlink" title="一些错误"></a>一些错误</h3><ul><li>NVIDIA-SMI has failed because it couldn’t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.</li></ul><p>如果安装完驱动后执行<code>nvidia-smi</code>后可能会出现该错误，重启后消失</p><p>如果在下载驱动的时候操作系统选择Linux 64-bit RHEL7，则会下载.rpm格式的安装包，安装后重启仍会出现该错误检测不到显卡驱动。所以操作系统需要选择Linux 64-bit 以下载.run格式安装包</p><ul><li>ERROR: An error occurred while performing the step: “Building kernel modules”. See /var/log/nvidia-installer.log for details.</li></ul><p>这个错误出现的原因之一是系统内核版本过高（或者过低）与驱动程序不匹配。问题发生在给第二种服务器安装时出现的，其系统版本升级为CentOS 7.5，默认内核版本为3.10.0-862.el7.x86_64（对比第一种为3.10.0-693.21.1.el7.x86_64），遂考虑切换内核版本</p><p>使用<code>cat /boot/grub2/grub.cfg | grep menuentry</code>查看目前有哪些内核版本，使用<code>grub2-set-default</code>命令切换，使用<code>grub2-editenv list</code>验证是否配置成功。在这里需要设置的内核版本为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grub2-set-default <span class="string">"CentOS Linux (3.10.0-693.21.1.el7.x86_64) 7 (Core)"</span></span><br></pre></td></tr></table></figure><p>如果切换完成后仍报该错误可以重启服务器</p><ul><li>WARNING: nvidia-installer was forced to guess the X library path ‘/usr/lib64’ and X module path ‘/usr/lib64/xorg/modules’; these paths were not queryable from the system. If X fails to find the NVIDIA X driver module, please install the pkg-config utility and the X.Org SDK/development package for your distribution and reinstall the driver.</li></ul><p>这是安装完后出现的warning，目前没发现有什么问题</p><h2 id="3、CUDA-8-0-amp-cuDNN-6-0"><a href="#3、CUDA-8-0-amp-cuDNN-6-0" class="headerlink" title="3、CUDA-8.0 &amp; cuDNN-6.0"></a>3、CUDA-8.0 &amp; cuDNN-6.0</h2><h3 id="CUDA下载"><a href="#CUDA下载" class="headerlink" title="CUDA下载"></a>CUDA下载</h3><p>去 <a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-downloads</a> 这里寻找对应平台的文件下载即可。这里有一份详尽官方的 <a href="http://docs.nvidia.com/cuda/cuda-installation-guide-linux/" target="_blank" rel="noopener">说明文档</a></p><p>这里一些选项的选择为：</p><ul><li>Operating System: Linux</li><li>Architecture: x86_64</li><li>Distribution: CentOS</li><li>Version: 7</li><li>Installer Type: runfile(local)</li></ul><p>下面会显示两个安装文件，一个 Base Installer ，一个Patch。安装完Base后再安装Patch即可</p><h3 id="CUDA安装"><a href="#CUDA安装" class="headerlink" title="CUDA安装"></a>CUDA安装</h3><p>添加可执行权限，安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod +x cuda_8.0.61_375.26_linux.run</span><br><span class="line">./cuda_8.0.61_375.26_linux.run</span><br></pre></td></tr></table></figure><p>接下来会有一系列提示需要确认，其中在询问是否要安装显卡驱动时选 n ，因为我们之前已经安装了最新版本的驱动。其他的一路同意即可</p><p>最后再打一下补丁即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod +x cuda_8.0.61.2_linux.run</span><br><span class="line">./cuda_8.0.61.2_linux.run</span><br></pre></td></tr></table></figure><h3 id="CUDA测试"><a href="#CUDA测试" class="headerlink" title="CUDA测试"></a>CUDA测试</h3><p>进入samples目录，选择第一个例子进行测试</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/cuda/samples/1_Utilities/deviceQuery</span><br><span class="line">make</span><br></pre></td></tr></table></figure><p>编译完成后执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./deviceQuery</span><br></pre></td></tr></table></figure><p>会看到一系列显卡参数信息，只要最后显示 Result = PASS 即说明CUDA安装成功</p><h3 id="添加环境变量"><a href="#添加环境变量" class="headerlink" title="添加环境变量"></a>添加环境变量</h3><p>编辑 /etc/profile.d/path.sh 文件，添加以下内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># cuda-8.0</span><br><span class="line">export CUDA_HOME=/usr/local/cuda</span><br><span class="line">export PATH=/usr/local/cuda-8.0/bin:$PATH</span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64:$LD_LIBRARY_PATH</span><br></pre></td></tr></table></figure><h3 id="cuDNN下载"><a href="#cuDNN下载" class="headerlink" title="cuDNN下载"></a>cuDNN下载</h3><p>去 <a href="https://developer.nvidia.com/rdp/cudnn-download" target="_blank" rel="noopener">https://developer.nvidia.com/rdp/cudnn-download</a> 选择对应的版本下载即可。不过需要先注册开发者账号后才可以下载</p><p>之前安装的CUDA是8.0，所以我们选择</p><ul><li>Download cuDNN v6.0 (April 27, 2017), for CUDA 8.0</li><li>cuDNN v6.0 Library for Linux</li></ul><h3 id="cuDNN安装"><a href="#cuDNN安装" class="headerlink" title="cuDNN安装"></a>cuDNN安装</h3><p>执行解压操作</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf cudnn-8.0-linux-x64-v6.0.tgz</span><br></pre></td></tr></table></figure><p>解压后的文件夹是cuda。执行以下操作把文件复制到相应的位置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cp cuda/include/cudnn.h /usr/<span class="built_in">local</span>/cuda/include/</span><br><span class="line">cp cuda/lib64/libcudnn* /usr/<span class="built_in">local</span>/cuda/lib64/</span><br><span class="line">chmod a+r /usr/<span class="built_in">local</span>/cuda/include/cudnn.h</span><br><span class="line">chmod a+r /usr/<span class="built_in">local</span>/cuda/lib64/libcudnn*</span><br></pre></td></tr></table></figure><h3 id="一些错误-2"><a href="#一些错误-2" class="headerlink" title="一些错误"></a>一些错误</h3><ul><li>g++: No such file or directory</li></ul><p>CUDA编译上述例子时报以上错误，这是因为没有安装g++，使用命令 <code>yum install gcc-c++</code> 来安装g++即可解决</p><h2 id="4、TensorFlow-1-3-0"><a href="#4、TensorFlow-1-3-0" class="headerlink" title="4、TensorFlow-1.3.0"></a>4、TensorFlow-1.3.0</h2><h3 id="准备工作-2"><a href="#准备工作-2" class="headerlink" title="准备工作"></a>准备工作</h3><p>安装一些系统依赖</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install python-devel</span><br></pre></td></tr></table></figure><h3 id="pip安装TensorFlow"><a href="#pip安装TensorFlow" class="headerlink" title="pip安装TensorFlow"></a>pip安装TensorFlow</h3><p>有两种方法</p><p>a）通过pip安装指定版本（1.3.0），注意要安装GPU版本</p><p>pip install tensorflow-gpu==1.3.0</p><p>b）若提示找不到对应版本（第一天安装是这样，第二天测试了下第一种方法又好使了），可执行以下命令</p><p>pip install –upgrade tfBinaryURL</p><p>其中，tfBinaryURL替换为 <a href="https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.3.0-cp27-none-linux_x86_64.whl" target="_blank" rel="noopener">https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.3.0-cp27-none-linux_x86_64.whl</a></p><p>如果不指定版本的话那默认安装的是最新版本的，而最新版本的TensorFlow已经不支持CUDA8.0了，可以去 <a href="https://www.tensorflow.org/install/install_sources" target="_blank" rel="noopener">https://www.tensorflow.org/install/install_sources</a> 查看TF与CUDA版本的对应关系，在最下面。如果该链接打不开（被墙），可以访问 <a href="https://tensorflow.google.cn/install/install_sources" target="_blank" rel="noopener">https://tensorflow.google.cn/install/install_sources</a> 。这是TF给中国大陆单独开的域名</p><p>安装其他版本CUDA与上述步骤类似</p><h3 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h3><p>进入python，进行 <code>import tensorflow</code>，没有错误那就代表安装成功</p><h3 id="一些错误-3"><a href="#一些错误-3" class="headerlink" title="一些错误"></a>一些错误</h3><ul><li>ImportError: /usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: PyUnicodeUCS4_AsUTF8String</li></ul><p>出现这个错误的原因是Python和某个你用的库编译时指定的UCS编码方式不对。编译Python时，可以通过指定–enable-unicode=ucs2或者ucs4来选择使用UCS2或者UCS4</p><p>如果你的错误是undefined symbol: PyUnicodeUCS2_AsUTF8String，说明你的Python编译时使用的是UCS4，反之亦然</p><p>解决方案就是重新编译Python或者重新编译库，但这里我们使用的是pip安装库（不想编译TensorFlow），所以选择重新编译Python</p><p>因为报错是PyUnicodeUCS4_AsUTF8String，说明TensorFlow是用UCS4编译的，而Python是UCS2编译的，所以重新编译Python时设置unicode为ucs4</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./configure --prefix=/usr/<span class="built_in">local</span> --<span class="built_in">enable</span>-unicode=ucs4</span><br></pre></td></tr></table></figure><ul><li>ImportError: Importing the multiarray numpy extension module failed. Most likely you are trying to import a failed build of numpy. If you’re working with a numpy git repo, try <code>git clean -xdf</code> (removes all files not under version control). Otherwise reinstall numpy. </li></ul><p>这是由于之前的旧版本Python安装的numpy与新编译的Python不兼容的缘故，卸载numpy重新安装即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip uninstall numpy</span><br><span class="line">pip install numpy</span><br></pre></td></tr></table></figure><ul><li>Cannot uninstall ‘Markdown’. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.</li></ul><p>如果先把pip从9升级到10后再安装TensorFlow会出现该问题。该问题的具体解释可见<a href="https://github.com/pypa/pip/issues/5247" target="_blank" rel="noopener">链接</a>。解决的方法为手动删除该包相关文件，在这里主要删除了一个文件夹以及一个info文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rm -rf /usr/lib/python2.7/site-packages/markdown</span><br><span class="line">rm Markdown-2.4.1-py2.7.egg-info</span><br></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>官方安装指南</p><ul><li><a href="https://www.tensorflow.org/install/install_sources" target="_blank" rel="noopener">https://www.tensorflow.org/install/install_sources</a></li><li><a href="https://www.tensorflow.org/install/install_linux" target="_blank" rel="noopener">https://www.tensorflow.org/install/install_linux</a></li></ul><p>腾讯云官方文档</p><ul><li><p>安装 NVIDIA 驱动指引<a href="https://cloud.tencent.com/document/product/560/8048" target="_blank" rel="noopener">https://cloud.tencent.com/document/product/560/8048</a></p></li><li><p>安装 CUDA 驱动指引<a href="https://cloud.tencent.com/document/product/560/8064" target="_blank" rel="noopener">https://cloud.tencent.com/document/product/560/8064</a></p></li></ul><p>参考的Blog</p><ul><li><a href="https://blog.csdn.net/qq708986022/article/details/77896791" target="_blank" rel="noopener">https://blog.csdn.net/qq708986022/article/details/77896791</a></li><li><a href="https://www.jianshu.com/p/b0b8ab2be12c" target="_blank" rel="noopener">https://www.jianshu.com/p/b0b8ab2be12c</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;日前，我司开始使用腾讯云的GPU服务器，那自然需要在其上配置TF的开发环境。之前写过在CentOS6上进行源码编译安装TF（&lt;a href=&quot;https://bluesmilery.github.io/blogs/9ef0e127/&quot;&gt;传送门&lt;/a&gt;），所以这篇也是在以前的基础上修改而来，不过因为系统版本换为CentOS7，许多步骤都可以省略了，方便不少。不过仍有部分操作或问题与之前不一致，在此也会对其说明&lt;/p&gt;
&lt;p&gt;最终配置的环境为CUDA-8.0 + cuDNN-6.0 + TensorFlow-1.3.0&lt;/p&gt;
    
    </summary>
    
      <category term="技术" scheme="https://bluesmilery.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="CentOS" scheme="https://bluesmilery.github.io/tags/CentOS/"/>
    
      <category term="TensorFlow" scheme="https://bluesmilery.github.io/tags/TensorFlow/"/>
    
      <category term="腾讯云" scheme="https://bluesmilery.github.io/tags/%E8%85%BE%E8%AE%AF%E4%BA%91/"/>
    
  </entry>
  
  <entry>
    <title>CentOS6源码编译安装TensorFlow</title>
    <link href="https://bluesmilery.github.io/blogs/9ef0e127/"/>
    <id>https://bluesmilery.github.io/blogs/9ef0e127/</id>
    <published>2017-09-30T20:59:17.000Z</published>
    <updated>2018-01-31T03:45:37.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章首发于微信公众号：链家产品技术团队，欢迎搜索关注~</p><p>公司前一阵搞了两台GPU服务器，终于有“玩具”可以玩了～用的卡是最新的P100（好吧，真正最新的是V100，不过还没铺货）。本着爱折腾的精神，自然就开始了折腾它们的征程（结果是我被折腾了。。。）</p><p>以前自己也搭建过TensorFlow的开发环境（<a href="https://bluesmilery.github.io/blogs/9a018dfc/">见链接</a>），所以一开始以为这次也不会难，结果。。。咳咳，还是要正视自己的水平的。因为服务器上装的是系统是CentOS，以前自己捣鼓的时候用的是Ubuntu，差别还是不少的，所以特此记录自己踩过的坑，也给其他人一些经验帮助。</p><p>这次折腾总共分为两个阶段：最初打算直接通过pip下载TensorFlow安装包安装，结果完全失败，不过在此期间摸清了许多限制条件，也为第二个阶段——通过编译的方式安装TensorFlow打下了基础。</p><p>系统版本：CentOS release 6.8 (Final)    64位</p><a id="more"></a><h1 id="追根溯源"><a href="#追根溯源" class="headerlink" title="追根溯源"></a>追根溯源</h1><p>首先，目标是安装TensorFlow，所以查阅TensorFlow官网的源码安装教程</p><p>发现需要使用Bazel来进行编译TensorFlow源码，进而生成pip安装包，通过pip来安装</p><p>那下一步就是安装Bazel。去Bazel官网查看，发现主要提供了Ubuntu、MacOS、Windows的安装方式，对于其他平台来说，需要通过源码编译的方式来安装。所需要的环境条件主要是JDK8、Python、跟C相关的编译工具等</p><p>CentOS 6.8自带的gcc版本为4.4.7，不支持C++11，而TensorFlow和Bazel的编译需要C++11的支持，所以要将gcc升级为支持C++11的版本，经过网上查找，gcc版本跨度不是很大的情况下，可以使用低版本的来编译安装高版本的gcc。gcc4.9.4可以通过4.4.7编译安装</p><p>CentOS 6.8自带的Python版本为2.6.6，而TensorFlow和Bazel的编译所需要的Python版本最低为2.7</p><p>因为要使用GPU，所以关于TensorFlow部分还需要一些额外的条件，在编译之前需要具备显卡驱动、Cuda Toolkit以及cuDNN</p><p>所以整个安装流程如下：</p><p>Java8、gcc4.9.4、Python2.7、pip、Bazel、NVIDIA Driver、CUDA、cuDNN、TensorFlow</p><h1 id="安装过程"><a href="#安装过程" class="headerlink" title="安装过程"></a>安装过程</h1><h2 id="1、Java-8"><a href="#1、Java-8" class="headerlink" title="1、Java-8"></a>1、Java-8</h2><p>去Oracle官网下载最新的Java8版本即可（8u144）</p><p><a href="http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html" target="_blank" rel="noopener">http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html</a></p><p>下载.tar.gz格式的压缩包后，解压缩，将bin目录所在的路径添加到PATH当中即可</p><h2 id="2、gcc-4-9-4"><a href="#2、gcc-4-9-4" class="headerlink" title="2、gcc-4.9.4"></a>2、gcc-4.9.4</h2><h3 id="2-1-准备工作"><a href="#2-1-准备工作" class="headerlink" title="2.1 准备工作"></a>2.1 准备工作</h3><p>先检查系统中是否安装了g++，如果没有的话在编译安装gcc时会报错</p><blockquote><p>make[1]: *** [stage1-bubble] Error 2</p></blockquote><p>使用yum安装g++</p><p><code>yum install gcc-c++</code></p><h3 id="2-2-下载gcc源码"><a href="#2-2-下载gcc源码" class="headerlink" title="2.2 下载gcc源码"></a>2.2 下载gcc源码</h3><p><a href="http://ftp.gnu.org/gnu/gcc/gcc-4.9.4/gcc-4.9.4.tar.bz2" target="_blank" rel="noopener">http://ftp.gnu.org/gnu/gcc/gcc-4.9.4/gcc-4.9.4.tar.bz2</a></p><p>解压</p><p><code>tar -jxvf gcc-4.9.4.tar.bz2</code></p><p>进入目录，接下来的操作都在这个目录下进行</p><p><code>cd gcc-4.9.4</code></p><h3 id="2-3-下载所需要的依赖库"><a href="#2-3-下载所需要的依赖库" class="headerlink" title="2.3 下载所需要的依赖库"></a>2.3 下载所需要的依赖库</h3><p>正常只需要执行</p><p><code>./contrib/download_prerequisites</code></p><p>但是因为服务器无法访问外网（后来就算可以访问了，但是连接依旧有限，比如这步自动下载一些依赖包的操作，就无法正常进行），所以我们采用先把所需要的依赖包下载下来后，放到所需要的目录（后来发现是ftp服务器无法请求= =）</p><p>查看下载操作的脚本</p><p><code>vim contrib/download_prerequisites</code></p><p>可以发现脚本通过wget下载了五个依赖包，分别是</p><ul><li><a href="http://ftp.gnu.org/gnu/mpfr/mpfr-2.4.2.tar.bz2" target="_blank" rel="noopener">mpfr-2.4.2</a></li><li><a href="http://ftp.gnu.org/gnu/gmp/gmp-4.3.2.tar.bz2" target="_blank" rel="noopener">gmp-4.3.2</a></li><li><a href="http://www.multiprecision.org/mpc/download/mpc-0.8.1.tar.gz" target="_blank" rel="noopener">mpc-0.8.1</a></li><li><a href="http://isl.gforge.inria.fr/isl-0.12.2.tar.bz2" target="_blank" rel="noopener">isl-0.12.2</a></li><li><a href="https://www.bastoul.net/cloog/pages/download/cloog-0.18.1.tar.gz" target="_blank" rel="noopener">cloog-0.18.1</a></li></ul><p>也可以去 <a href="ftp://gcc.gnu.org/pub/gcc/infrastructure/" target="_blank" rel="noopener">ftp://gcc.gnu.org/pub/gcc/infrastructure/</a> 下载</p><p>下载完成后将它们放到gcc-4.9.4目录下即可，然后再执行</p><p><code>./contrib/download_prerequisites</code></p><h3 id="2-4-配置编译安装"><a href="#2-4-配置编译安装" class="headerlink" title="2.4 配置编译安装"></a>2.4 配置编译安装</h3><p>官方建议新建一个目录用于编译，所以</p><p><code>mkdir build</code></p><p><code>cd build</code></p><p>配置</p><p><code>../gcc-4.9.4/configure --prefix=/opt/gcc-4.9.4/ --enable-checking=release --enable-languages=c,c++ --disable-multilib</code></p><p>关于具体的参数设置可以参照 <a href="https://gcc.gnu.org/install/configure.html" target="_blank" rel="noopener">https://gcc.gnu.org/install/configure.html</a></p><p>编译</p><p><code>make -j4</code></p><p>-j4指的是使用四个线程，服务器的CPU大约使用了不到20分钟。不过有人不建议使用多线程编译，说是可能会失败。</p><p>安装</p><p><code>make install</code></p><p>添加路径，打开 .bashrc，添加如下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">export PATH=/opt/gcc-4.9.4/bin:$PATH  </span><br><span class="line">export CXX=/opt/gcc-4.9.4/bin/c++  </span><br><span class="line">export CC=/opt/gcc-4.9.4/bin/gcc  </span><br><span class="line">export LDFLAGS=&quot;-L/opt/gcc-4.9.4/lib -L/opt/gcc-4.9.4/lib64&quot;  </span><br><span class="line">export CXXFLAGS=&quot;-L/opt/gcc-4.9.4/lib -L/opt/gcc-4.9.4/lib64&quot;  </span><br><span class="line">export C_INCLUDE_PATH=/opt/gcc-4.9.4/include  </span><br><span class="line">export CXX_INCLUDE_PATH=$C_INCLUDE_PATH  </span><br><span class="line">export LD_RUN_PATH=/opt/gcc-4.9.4/lib/:/opt/gcc-4.9.4/lib64/  </span><br><span class="line">export LD_LIBRARY_PATH=/opt/gcc-4.9.4/lib/:/opt/gcc-4.9.4/lib64/:$LD_LIBRARY_PATH</span><br></pre></td></tr></table></figure><p>一般添加第一行就可以，但是在Bazel的编译过程中会出现一些错误，加上后面的部分可以解决这些错误</p><p>最后在source一下 .bashrc 就可以了～</p><p>通过 <code>gcc -v</code> 和 <code>g++ -v</code> 可以看到版本已经变成了4.9.4：gcc version 4.9.4 (GCC)</p><h2 id="3、Python-2-7"><a href="#3、Python-2-7" class="headerlink" title="3、Python-2.7"></a>3、Python-2.7</h2><p>Python也需要通过编译源码的方式安装，使用刚才升级过的gcc来编译。之所以这样做是因为可以在后续操作中避免一些错误</p><h3 id="3-1-准备工作"><a href="#3-1-准备工作" class="headerlink" title="3.1 准备工作"></a>3.1 准备工作</h3><p>安装一些系统依赖</p><p><code>yum groupinstall -y &#39;development tools&#39;</code><br><code>yum install -y zlib-devel bzip2-devel openssl-devel xz-libs wget</code></p><h3 id="3-2-下载Python2-7源码"><a href="#3-2-下载Python2-7源码" class="headerlink" title="3.2 下载Python2.7源码"></a>3.2 下载Python2.7源码</h3><p><a href="https://www.python.org/ftp/python/2.7.14/Python-2.7.14.tar.xz" target="_blank" rel="noopener">https://www.python.org/ftp/python/2.7.14/Python-2.7.14.tar.xz</a></p><p>解压</p><p><code>xz -d Python-2.7.14.tar.xz</code><br><code>tar -xvf Python-2.7.14.tar</code></p><p>进入目录，接下来的操作都在这个目录下进行</p><p><code>cd Python-2.7.14</code></p><h3 id="3-3-配置编译安装"><a href="#3-3-配置编译安装" class="headerlink" title="3.3 配置编译安装"></a>3.3 配置编译安装</h3><p>配置</p><p><code>./configure --prefix=/usr/local</code></p><p>编译，用时几分钟</p><p><code>make</code></p><p>安装</p><p><code>make install</code></p><h3 id="3-4-使新版本生效"><a href="#3-4-使新版本生效" class="headerlink" title="3.4 使新版本生效"></a>3.4 使新版本生效</h3><p>两种方式：加入PATH、软连接</p><p>加入PATH</p><p><code>export PATH=/usr/local/bin:$PATH</code></p><p>软连接</p><p><code>mv /usr/bin/python /usr/bin/python2.6</code><br><code>ln -s /usr/local/bin/python2.7 /usr/bin/python</code></p><p>此时通过 <code>python -V</code> 可以查看Python版本已经为2.7</p><h3 id="3-5-解决yum失效问题"><a href="#3-5-解决yum失效问题" class="headerlink" title="3.5 解决yum失效问题"></a>3.5 解决yum失效问题</h3><p>因为yum依赖的是原来Python2.6版本，所以做以下修改</p><p><code>vim /usr/bin/yum</code></p><p>将第一行 <code>#!/usr/bin/python</code> 改为 <code>#!/usr/bin/python2.6</code></p><h3 id="3-6-更新setuptools和pip"><a href="#3-6-更新setuptools和pip" class="headerlink" title="3.6 更新setuptools和pip"></a>3.6 更新setuptools和pip</h3><p>分别下载 <a href="https://pypi.python.org/pypi/setuptools" target="_blank" rel="noopener">setuptools</a> 和 <a href="https://pypi.python.org/pypi/pip" target="_blank" rel="noopener">pip</a> 的安装包</p><p>解压后进到相应的目录，执行</p><p><code>python setup.py install</code></p><p>此时通过 <code>pip -V</code> 可以查看pip的版本，已经是对应于Python2.7的了</p><p>顺手更新pip源，毕竟还是国内的快</p><p><code>vim ~/.pip/pip.conf</code></p><p>然后写入如下内容并保存</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[global]  </span><br><span class="line">trusted-host = mirrors.aliyun.com  </span><br><span class="line">index-url = http://mirrors.aliyun.com/pypi/simple</span><br></pre></td></tr></table></figure><h2 id="4、Bazel-0-5-3"><a href="#4、Bazel-0-5-3" class="headerlink" title="4、Bazel-0.5.3"></a>4、Bazel-0.5.3</h2><p>接下来是编译安装Bazel，主要参照 <a href="https://docs.bazel.build/versions/master/install-compile-source.html" target="_blank" rel="noopener">官方教程</a> 即可</p><h3 id="4-1-下载Bazel源码"><a href="#4-1-下载Bazel源码" class="headerlink" title="4.1 下载Bazel源码"></a>4.1 下载Bazel源码</h3><p>去GitHub上下载 bazel-\&lt;VERSION>-dist.zip 格式的源码，下载最新版或者特定版本均可。此处下载的是0.5.3版本（最新的为0.5.4）</p><p><a href="https://github.com/bazelbuild/bazel/releases/download/0.5.3/bazel-0.5.3-dist.zip" target="_blank" rel="noopener">https://github.com/bazelbuild/bazel/releases/download/0.5.3/bazel-0.5.3-dist.zip</a></p><p>解压，最好指定目录，因为Bazel所有文件都放在根目录</p><p><code>unzip bazel-0.5.3-dist.zip -d bazel-0.5.3</code></p><p>进入目录，接下来的操作都在这个目录下进行</p><p><code>cd bazel-0.5.3</code></p><h3 id="4-2-编译"><a href="#4-2-编译" class="headerlink" title="4.2 编译"></a>4.2 编译</h3><p>运行</p><p><code>./compile.sh</code></p><p>编译出的结果放在 <code>output/bazel</code> 当中，将其复制到PATH路径下即可</p><p><code>cp output/bazel /usr/local/bin</code></p><p>或者</p><p><code>cp output/bazel /usr/bin</code></p><p>可以执行 <code>bazel</code> 验证是否安装成功</p><h3 id="4-3-一些错误"><a href="#4-3-一些错误" class="headerlink" title="4.3 一些错误"></a>4.3 一些错误</h3><ul><li>第一个</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ERROR: /root/gg/bazel/third_party/BUILD:106:1: Extracting interface //third_party:apache_commons_collections failed (Exit 1): ijar failed: error executing command</span><br></pre></td></tr></table></figure><p>参照这个 <a href="https://github.com/bazelbuild/bazel/issues/760" target="_blank" rel="noopener">issue</a> 解决：</p><p>升级gcc后，需要添加CXX, CC, LDFLAGS and CXXFLAGS等环境变量</p><ul><li>第二个 </li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Executing genrule //src:embedded_tools failed (Exit 1): bash failed: error executing command</span><br></pre></td></tr></table></figure><p>参照这两个 <a href="https://github.com/bazelbuild/bazel/issues/2738" target="_blank" rel="noopener">issue</a>、<a href="https://github.com/bazelbuild/bazel/issues/673" target="_blank" rel="noopener">issue</a> 解决：</p><p><code>mkdir /root/tmp</code><br><code>export TMPDIR=/root/tmp</code> </p><ul><li>第三个</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">AttributeError: ZipFile instance has no attribute &apos;__exit__&apos;</span><br></pre></td></tr></table></figure><p>当初升级gcc后直接编译Bazel报的错误，经查找是Python版本的问题（当时Python还是2.6），使用gcc4.9.4编译安装Python2.7后问题解决</p><h2 id="5、NVIDIA-Driver"><a href="#5、NVIDIA-Driver" class="headerlink" title="5、NVIDIA Driver"></a>5、NVIDIA Driver</h2><h3 id="5-1-准备工作"><a href="#5-1-准备工作" class="headerlink" title="5.1 准备工作"></a>5.1 准备工作</h3><p>安装一些系统依赖</p><p><code>yum install kernel-source</code></p><h3 id="5-2-下载驱动程序"><a href="#5-2-下载驱动程序" class="headerlink" title="5.2 下载驱动程序"></a>5.2 下载驱动程序</h3><p>去 <a href="http://www.nvidia.cn/Download/index.aspx" target="_blank" rel="noopener">http://www.nvidia.cn/Download/index.aspx</a> 这里寻找对应的显卡驱动即可，这里选择：</p><ul><li>Product Type: Tesla</li><li>Product Series: P-Series</li><li>Product: Tesla P100</li><li>Operating System: Linux 64-bit</li><li>CUDA Toolkit: 8.0</li><li>Language: English(US)</li></ul><p>这里下载的文件名是：NVIDIA-Linux-x86_64-384.66.run</p><h3 id="5-3-安装"><a href="#5-3-安装" class="headerlink" title="5.3 安装"></a>5.3 安装</h3><p>添加可执行权限，安装</p><p><code>chmod +x NVIDIA-Linux-x86_64-384.66.run</code><br><code>./NVIDIA-Linux-x86_64-384.66.run</code></p><p>进入安装界面后一路同意就可以，是否安装32位的库，我选择的同意。对于最后出现的warning我选择了忽略</p><p>安装完成后需要重启服务器，驱动才会生效</p><p><code>reboot</code></p><h3 id="5-4-验证"><a href="#5-4-验证" class="headerlink" title="5.4 验证"></a>5.4 验证</h3><p>执行</p><p><code>nvidia-smi</code></p><p>后可以看到一些显卡信息，包括驱动版本、风扇转速、温度、显卡型号、已用功率/总功率、已用显存/总显存、GPU计算力利用率等</p><h3 id="5-5-一些错误"><a href="#5-5-一些错误" class="headerlink" title="5.5 一些错误"></a>5.5 一些错误</h3><ul><li>第一个</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ERROR: Unable to find the kernel source tree for the currently running kernel. Please make sure you have installed the kernel source files for your kernel and that they are properly configured; on Red Hat Linux systems, for example, be sure you have the &apos;kernel-source&apos; or &apos;kernel-devel&apos; RPM installed. If you know the correct kernel source files are installed, you may specify the kernel source path with the &apos;--kernel-source-path&apos; command line option.</span><br></pre></td></tr></table></figure><p>如果没做最一开始的准备工作，可能会出现这个错误</p><ul><li>第二个</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">WARNING: nvidia-installer was forced to guess the X library path &apos;/usr/lib64&apos; and X module path &apos;/usr/lib64/xorg/modules&apos;; these paths were not queryable from the system.  If X fails to find the NVIDIA X driver module, please install the `pkg-config` utility and the X.Org SDK/development package for your distribution and reinstall the driver.</span><br></pre></td></tr></table></figure><p>安装完成后的warning，目前没发现有什么问题</p><h2 id="6、CUDA-8-0"><a href="#6、CUDA-8-0" class="headerlink" title="6、CUDA-8.0"></a>6、CUDA-8.0</h2><p>CUDA是英伟达开发的一款针对于使用GPU来加速计算的工具包，所以机器学习或者深度学习想要使用GPU来加速计算的话，就必须使用CUDA</p><h3 id="6-1-下载"><a href="#6-1-下载" class="headerlink" title="6.1 下载"></a>6.1 下载</h3><p>去 <a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-downloads</a> 这里寻找对应平台的文件下载即可。这里有一份详尽官方的 <a href="http://docs.nvidia.com/cuda/cuda-installation-guide-linux/" target="_blank" rel="noopener">说明文档</a></p><p>目前CUDA已经出了9.0，但是TenforFlow官方推荐的是8.0，所以我们安装的还是8.0版本</p><p>这里一些选项的选择为：</p><ul><li>Operating System: Linux</li><li>Architecture: x86_64</li><li>Distribution: CentOS</li><li>Version: 6</li><li>Installer Type: runfile(local)</li></ul><p>下面会显示两个安装文件，一个 Base Installer ，一个Patch。安装完Base后再安装Patch即可</p><h3 id="6-2-安装"><a href="#6-2-安装" class="headerlink" title="6.2 安装"></a>6.2 安装</h3><p>添加可执行权限，安装</p><p><code>chmod +x cuda_8.0.61_375.26_linux.run</code><br><code>./cuda_8.0.61_375.26_linux.run</code></p><p>接下来会有一系列提示需要确认，其中在询问是否要安装显卡驱动时选 n ，因为我们之前已经安装了最新版本的驱动。其他的一路同意即可</p><p>安装完成后会出现以下内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">Installing the CUDA Toolkit in /usr/local/cuda-8.0 ...  </span><br><span class="line">Missing recommended library: libGLU.so  </span><br><span class="line">Missing recommended library: libX11.so  </span><br><span class="line">Missing recommended library: libXi.so  </span><br><span class="line">Missing recommended library: libXmu.so  </span><br><span class="line"></span><br><span class="line">Installing the CUDA Samples in /home/gaixindong ...  </span><br><span class="line">Copying samples to /home/gaixindong/NVIDIA_CUDA-8.0_Samples now...  </span><br><span class="line">Finished copying samples.  </span><br><span class="line"></span><br><span class="line">= Summary =   </span><br><span class="line"></span><br><span class="line">Driver:   Not Selected  </span><br><span class="line">Toolkit:  Installed in /usr/local/cuda-8.0  </span><br><span class="line">Samples:  Installed in /home/gaixindong, but missing recommended libraries  </span><br><span class="line"></span><br><span class="line">Please make sure that  </span><br><span class="line">-   PATH includes /usr/local/cuda-8.0/bin  </span><br><span class="line">-   LD_LIBRARY_PATH includes /usr/local/cuda-8.0/lib64, or, add /usr/local/cuda-8.0/lib64 to /etc ld.so.conf and run ldconfig as root  </span><br><span class="line"></span><br><span class="line">To uninstall the CUDA Toolkit, run the uninstall script in /usr/local/cuda-8.0/bin  </span><br><span class="line"></span><br><span class="line">Please see CUDA_Installation_Guide_Linux.pdf in /usr/local/cuda-8.0/doc/pdf for detailed information on setting up CUDA.  </span><br><span class="line"></span><br><span class="line">***WARNING: Incomplete installation! This installation did not install the CUDA Driver. A driver of version at least 361.00 is required for CUDA 8.0 functionality to work.  </span><br><span class="line">To install the driver using this installer, run the following command, replacing &lt;CudaInstaller&gt; with the name of this run file:  </span><br><span class="line">    sudo &lt;CudaInstaller&gt;.run -silent -driver  </span><br><span class="line"></span><br><span class="line">Logfile is /tmp/cuda_install_132117.log</span><br></pre></td></tr></table></figure><p>根据以上提示内容发现，缺少了一些推荐的库，但是这些库可能是跟运行它提供的samples有关，所以现在并没有安装。日后如果有问题，可以依照这些提示去安装一下</p><p>并且samples也可以选择不安装，因为在cuda的目录下有一份samples</p><p>最后再打一下补丁即可</p><p><code>chmod +x cuda_8.0.61.2_linux.run</code><br><code>./cuda_8.0.61.2_linux.run</code></p><h3 id="6-3-测试"><a href="#6-3-测试" class="headerlink" title="6.3 测试"></a>6.3 测试</h3><p>进入samples目录，选择第一个例子进行测试</p><p><code>cd /usr/local/cuda/samples/1_Utilities/deviceQuery</code><br><code>make</code></p><p>编译完成后执行</p><p><code>./deviceQuery</code></p><p>会看到一系列显卡参数信息，只要最后显示 <code>Result = PASS</code> 即说明CUDA安装成功</p><h2 id="7、cuDNN-6-0"><a href="#7、cuDNN-6-0" class="headerlink" title="7、cuDNN-6.0"></a>7、cuDNN-6.0</h2><p>在CUDA之外，还有个库叫做cuDNN(CUDA Deep Neural Network library)，是专门给深度神经网络针对GPU调优的，也是TensorFlow官方要求必须安装的</p><h3 id="7-1-下载"><a href="#7-1-下载" class="headerlink" title="7.1 下载"></a>7.1 下载</h3><p>去 <a href="https://developer.nvidia.com/rdp/cudnn-download" target="_blank" rel="noopener">https://developer.nvidia.com/rdp/cudnn-download</a> 选择对应的版本下载即可。不过需要先注册开发者账号后才可以下载</p><p>目前cuDNN最新版本是7.0，因为担心兼容性问题所以没有选择最新版本。因为之前安装的CUDA是8.0，所以我们选择</p><p>Download cuDNN v6.0 (April 27, 2017), for CUDA 8.0</p><p>cuDNN v6.0 Library for Linux</p><h3 id="7-2-安装"><a href="#7-2-安装" class="headerlink" title="7.2 安装"></a>7.2 安装</h3><p>执行解压操作</p><p><code>tar -zxvf cudnn-8.0-linux-x64-v6.0-tgz</code></p><p>解压后的文件夹是cuda。执行以下操作把文件复制到相应的位置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cp cuda/include/cudnn.h /usr/local/cuda/include/  </span><br><span class="line">cp cuda/lib64/libcudnn* /usr/local/cuda/lib64/  </span><br><span class="line">chmod a+r /usr/local/cuda/include/cudnn.h  </span><br><span class="line">chmod a+r /usr/local/cuda/lib64/libcudnn*</span><br></pre></td></tr></table></figure><h2 id="8、TensorFlow-1-3-0"><a href="#8、TensorFlow-1-3-0" class="headerlink" title="8、TensorFlow-1.3.0"></a>8、TensorFlow-1.3.0</h2><p>终于到最后一步了，整个过程主要参照 <a href="https://www.tensorflow.org/install/install_sources" target="_blank" rel="noopener">官方教程</a></p><h3 id="8-1-准备工作"><a href="#8-1-准备工作" class="headerlink" title="8.1 准备工作"></a>8.1 准备工作</h3><p>安装一些系统依赖</p><p><code>yum install python-devel</code><br><code>pip install numpy wheel</code></p><h3 id="8-2-下载TensorFlow源码"><a href="#8-2-下载TensorFlow源码" class="headerlink" title="8.2 下载TensorFlow源码"></a>8.2 下载TensorFlow源码</h3><p>最一开始是直接通过pip下载官方编译好的安装包进行安装，但因为CentOS的原因，并不能通过这种方式安装（在网上找到的CentOS安装TensorFlow大部分都是通过编译源码来安装的）。所以这也是导致这次安装如此繁琐的原因</p><p>目前TensorFlow的最新版本为1.3.0，源码可以通过git clone下来，也可以直接去releases界面下载</p><ul><li>git clone</li></ul><p><code>git clone https://github.com/tensorflow/tensorflow</code><br><code>git checkout r1.3</code></p><ul><li>releases界面</li></ul><p><a href="https://github.com/tensorflow/tensorflow/archive/v1.3.0.tar.gz" target="_blank" rel="noopener">TensorFlow 1.3.0</a></p><h3 id="8-3-配置"><a href="#8-3-配置" class="headerlink" title="8.3 配置"></a>8.3 配置</h3><p>进入TensorFlow源码根目录后，执行</p><p><code>./configure</code></p><p>接下来会有一系列提示需要确认，其中</p><p>Do you wish to use jemalloc as the malloc implementation? [Y/n] 选择 n，因为选择y后编译不通过</p><p>Do you wish to build TensorFlow with CUDA support? [y/N] 选择 y，因为我们要使用GPU</p><p>其他的默认就可以（大写字母是默认选择）</p><p>以下是具体的配置选项</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">.........  </span><br><span class="line">You have bazel 0.5.3- installed.  </span><br><span class="line">Please specify the location of python. [Default is /usr/local/bin/python]:  </span><br><span class="line">Found possible Python library paths:  </span><br><span class="line">/usr/local/lib/python2.7/site-packages  </span><br><span class="line">Please input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/site-packages]  </span><br><span class="line"></span><br><span class="line">Using python library path: /usr/local/lib/python2.7/site-packages  </span><br><span class="line">Do you wish to build TensorFlow with MKL support? [y/N]  </span><br><span class="line">No MKL support will be enabled for TensorFlow  </span><br><span class="line">Please specify optimization flags to use during compilation when bazel option &quot;--config=opt&quot; is specified [Default is -march=native]:  </span><br><span class="line">Do you wish to use jemalloc as the malloc implementation? [Y/n] n</span><br><span class="line">jemalloc disabled  </span><br><span class="line">Do you wish to build TensorFlow with Google Cloud Platform support? [y/N]  </span><br><span class="line">No Google Cloud Platform support will be enabled for TensorFlow  </span><br><span class="line">Do you wish to build TensorFlow with Hadoop File System support? [y/N]  </span><br><span class="line">No Hadoop File System support will be enabled for TensorFlow  </span><br><span class="line">Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N]  </span><br><span class="line">No XLA support will be enabled for TensorFlow  </span><br><span class="line">Do you wish to build TensorFlow with VERBS support? [y/N]  </span><br><span class="line">No VERBS support will be enabled for TensorFlow  </span><br><span class="line">Do you wish to build TensorFlow with OpenCL support? [y/N]  </span><br><span class="line">No OpenCL support will be enabled for TensorFlow  </span><br><span class="line">Do you wish to build TensorFlow with CUDA support? [y/N] y</span><br><span class="line">CUDA support will be enabled for TensorFlow  </span><br><span class="line">Do you want to use clang as CUDA compiler? [y/N]  </span><br><span class="line">nvcc will be used as CUDA compiler  </span><br><span class="line">Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]:  </span><br><span class="line">Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:  </span><br><span class="line">Please specify which gcc should be used by nvcc as the host compiler. [Default is /opt/gcc-4.9.4/bin/gcc]:  </span><br><span class="line">Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 6.0]:  </span><br><span class="line">Please specify the location where cuDNN 6 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:  </span><br><span class="line">Please specify a list of comma-separated Cuda compute capabilities you want to build with.  </span><br><span class="line">You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.  </span><br><span class="line">Please note that each additional compute capability significantly increases your build time and binary size.  </span><br><span class="line">[Default is: &quot;6.0&quot;]:  </span><br><span class="line">Do you wish to build TensorFlow with MPI support? [y/N]  </span><br><span class="line">MPI support will not be enabled for TensorFlow  </span><br><span class="line">Configuration finished</span><br></pre></td></tr></table></figure><h3 id="8-4-编译打包安装"><a href="#8-4-编译打包安装" class="headerlink" title="8.4 编译打包安装"></a>8.4 编译打包安装</h3><p>接下来进行编译操作</p><p><code>bazel build --config=opt --config=cuda//tensorflow/tools/pip_package:build_pip_package</code></p><p>如果顺利的话就不会出现什么ERROR提示，整个编译过程大约十分钟。如果出现错误可以加入 <code>--verbose_failures</code> 参数，会提供更丰富的出错信息</p><p>编译完成后，接下来就是打包成 .whl 文件供pip安装使用，文件会放到/tmp/tensorflow_pkg目录下</p><p><code>bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg</code></p><p>安装</p><p><code>pip install /tmp/tensorflow_pkg/tensorflow-1.3.0-py2-none-any.whl</code></p><p>安装过程会自动联网下载一些依赖的包，如果无法连外网的话，那就需要自己下载所需要的包，然后上传到服务器后用pip离线安装</p><p>可以使用 <a href="https://pypi.python.org/pypi/" target="_blank" rel="noopener">官方pip源</a> 或者 <a href="http://mirrors.aliyun.com/pypi/simple/" target="_blank" rel="noopener">阿里pip源</a> 来搜索安装，推荐后者，国内快</p><p>依赖的包有（可能不全）：</p><ul><li>backports.weakref-1.0</li><li>bleach-1.5.0</li><li>funcsigs-1.0.2</li><li>html5lib-0.9999999</li><li>Markdown-2.6.9</li><li>mock-2.0.0</li><li>numpy-1.13.1</li><li>pbr-3.1.1</li><li>protobuf-3.4.0</li><li>six-1.11.0</li><li>tensorflow_tensorboard-0.1.6</li><li>webencodings-0.5.1</li><li>Werkzeug-0.12.2</li><li>wheel-0.30.0</li></ul><h3 id="8-5-验证"><a href="#8-5-验证" class="headerlink" title="8.5 验证"></a>8.5 验证</h3><p>进入python，进行 <code>import tensorflow</code>，没有错误那就代表大功告成了！</p><h3 id="8-6-一些错误"><a href="#8-6-一些错误" class="headerlink" title="8.6 一些错误"></a>8.6 一些错误</h3><p>整个编译过程还是比较坎坷的</p><p>1、第一个错误  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">ERROR: /root/gg/tensorflow/tensorflow/tensorflow/tools/pip_package/BUILD:100:1: no such package &apos;@boringssl//&apos;: Traceback (most recent call last):  </span><br><span class="line">File &quot;/root/gg/tensorflow/tensorflow/tensorflow/workspace.bzl&quot;, line 116  </span><br><span class="line">_apply_patch(repo_ctx, repo_ctx.attr.patch_file)  </span><br><span class="line">File &quot;/root/gg/tensorflow/tensorflow/tensorflow/workspace.bzl&quot;, line 107, in _apply_patch  </span><br><span class="line">_execute_and_check_ret_code(repo_ctx, cmd)  </span><br><span class="line">File &quot;/root/gg/tensorflow/tensorflow/tensorflow/workspace.bzl&quot;, line 91, in _execute_and_check_ret_code  </span><br><span class="line">fail(&quot;Non-zero return code(&#123;1&#125;) when ...))  </span><br><span class="line">Non-zero return code(256) when executing &apos;patch -p1 -d /root/.cache/bazel/_bazel_root/9abfd3cc56b23f8500d978612da34f89/external/boringssl -i /root/gg/tensorflow/tensorflow/third_party/boringssl/add_boringssl_s390x.patch&apos;:  </span><br><span class="line">Stdout:  </span><br><span class="line">Stderr: java.io.IOException: Cannot run program &quot;patch&quot; (in directory &quot;/root/.cache/bazel/_bazel_root/9abfd3cc56b23f8500d978612da34f89/external/boringssl&quot;): error=2, No such file or directory and referenced by &apos;//tensorflow/tools/pip_package:licenses&apos;.  </span><br><span class="line">ERROR: Analysis of target &apos;//tensorflow/tools/pip_package:build_pip_package&apos; failed; build aborted.  </span><br><span class="line">INFO: Elapsed time: 1.401s</span><br></pre></td></tr></table></figure><p>这个错误出现的时候@boringssl和@protobuf是交替出现，<code>no such package &#39;@boringssl</code>。一开始以为是这两个包没有下载成功，因为在info信息中有连接下载链接失败的提示，并且错误信息中是说没有找到对应的包，所以去 <code>tensorflow/workspace.bzl</code> 中查找对应的下载链接，下载到具体的包后在服务器上做了个微型本地服务器，将workspace.bzl中的下载链接改成本地的下载链接。然而这个错误继续出现，后来经过白银指点，发现是 <code>Cannot run program &quot;patch&quot;</code> 这里的问题，遂去 <code>yum install patch</code> 后该错误不再出现。原来的思路一直局限在下载不成功上，没有仔细观察后面的错误信息</p><p>2、第二个错误</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">C++ compilation of rule &apos;@nanopb_git//: nanopb&apos; failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command</span><br></pre></td></tr></table></figure><p>后续内容还有一堆，其中@nanopb_git会变成其他包的名字，所以查找的重点就放在了后面 <code>crosstool_wrapper_driver_is_not_gcc failed: error executing command</code></p><p>3、经过了一顿google，查阅了无数github上的issus，都没有发现类似的问题（有这部分相同的报错，但是后续错误内容不同），问题没有解决，我决定先编译TensorFlow提供的example，看看小范围编译是否存在问题</p><p><code>bazel build --config=opt --config=cuda //tensorflow/cc:tutorials_example_trainer</code></p><p>此时报错：</p><blockquote><p>error: ‘MADV_NOHUGEPAGE’ undeclared</p></blockquote><p>参照这个 <a href="https://github.com/tensorflow/tensorflow/issues/7572" target="_blank" rel="noopener">issus</a> 解决：</p><p>在进行configure配置时，将 Do you wish to use jemalloc as the malloc implementation? [Y/n] 选择 n</p><p>4、在对example编译成功后，又继续回来尝试编译源码，但是依旧出问题。不过，有时候命运就是捉弄人，尝到了‘踏破铁鞋无觅处，得来全不费功夫’的滋味</p><p>看到白银在看 <a href="http://www.jianshu.com/p/fdb7b54b616e" target="_blank" rel="noopener">这篇文章</a>，突然发现在文章的前部有这样一句话：</p><p>“CentOS 6 上glibc最多到2.12，强行使用高版本的glibc会导致程序意外崩溃”</p><p>这篇文章之前看过，但是注意力从来没有留意这句话。突然想到会不会真的是glibc的问题。因为在之前的尝试过程中因为某些报错需要安装glibc2.14版本，而系统里自带的版本只到2.12，所以就去下载编译了glibc2.14版本，并且加入LD_LIBRARY_PATH中</p><p>那么在 .bashrc 中将glibc添加到LD_LIBRARY_PATH中这一行注释掉，结果竟然顺利编译成功！真是惊喜啊</p><p>感谢白银同学，最后两个关键问题都是在他的帮助下解决的～</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li>官方安装指南</li></ul><p><a href="https://www.tensorflow.org/install/install_sources" target="_blank" rel="noopener">https://www.tensorflow.org/install/install_sources</a>  </p><p><a href="https://docs.bazel.build/versions/master/install-compile-source.html" target="_blank" rel="noopener">https://docs.bazel.build/versions/master/install-compile-source.html</a>  </p><p><a href="https://gcc.gnu.org/install/configure.html" target="_blank" rel="noopener">https://gcc.gnu.org/install/configure.html</a>  </p><p><a href="http://docs.nvidia.com/cuda/cuda-installation-guide-linux/" target="_blank" rel="noopener">http://docs.nvidia.com/cuda/cuda-installation-guide-linux/</a>  </p><ul><li>gcc安装</li></ul><p><a href="http://caosiyang.github.io/2016/05/04/installing-gcc/" target="_blank" rel="noopener">http://caosiyang.github.io/2016/05/04/installing-gcc/</a>  </p><p><a href="http://www.cjjjs.com/paper/czxt/2017222114137150.html" target="_blank" rel="noopener">http://www.cjjjs.com/paper/czxt/2017222114137150.html</a>  </p><ul><li>Python以及pip安装</li></ul><p><a href="http://farwmarth.com/%E5%8D%87%E7%BA%A7centos%E4%B8%8A%E7%9A%84python/" target="_blank" rel="noopener">http://farwmarth.com/%E5%8D%87%E7%BA%A7centos%E4%B8%8A%E7%9A%84python/</a>  </p><p><a href="http://blog.csdn.net/tiantuanzi/article/details/50475718" target="_blank" rel="noopener">http://blog.csdn.net/tiantuanzi/article/details/50475718</a>  </p><p><a href="http://www.itdadao.com/articles/c15a1317443p0.html" target="_blank" rel="noopener">http://www.itdadao.com/articles/c15a1317443p0.html</a></p><ul><li>显卡驱动及CUDA相关（文章里关于禁用nouveau、重新建立initramfs image文件的操作在本服务器上不操作也可以正常安装显卡驱动）</li></ul><p><a href="http://www.gpusever.com/show.php?cid=33&amp;id=27" target="_blank" rel="noopener">http://www.gpusever.com/show.php?cid=33&amp;id=27</a>  </p><p><a href="http://www.centoscn.com/image-text/install/2015/0921/6199.html" target="_blank" rel="noopener">http://www.centoscn.com/image-text/install/2015/0921/6199.html</a>  </p><ul><li>TensorFlow安装</li></ul><p><a href="http://www.jianshu.com/p/fdb7b54b616e" target="_blank" rel="noopener">http://www.jianshu.com/p/fdb7b54b616e</a>  </p><p><a href="http://ju.outofmemory.cn/entry/311727" target="_blank" rel="noopener">http://ju.outofmemory.cn/entry/311727</a>  </p><p><a href="https://blog.abysm.org/2016/06/building-tensorflow-centos-6/" target="_blank" rel="noopener">https://blog.abysm.org/2016/06/building-tensorflow-centos-6/</a>  </p><p><a href="http://blog.csdn.net/may0324/article/details/53413024" target="_blank" rel="noopener">http://blog.csdn.net/may0324/article/details/53413024</a>  </p><p><a href="http://www.alfrednanwu.com/machine-learning/-gtx-1080ubuntu1604cuda8cudnn51tensorflow" target="_blank" rel="noopener">http://www.alfrednanwu.com/machine-learning/-gtx-1080ubuntu1604cuda8cudnn51tensorflow</a>  </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;文章首发于微信公众号：链家产品技术团队，欢迎搜索关注~&lt;/p&gt;
&lt;p&gt;公司前一阵搞了两台GPU服务器，终于有“玩具”可以玩了～用的卡是最新的P100（好吧，真正最新的是V100，不过还没铺货）。本着爱折腾的精神，自然就开始了折腾它们的征程（结果是我被折腾了。。。）&lt;/p&gt;
&lt;p&gt;以前自己也搭建过TensorFlow的开发环境（&lt;a href=&quot;https://bluesmilery.github.io/blogs/9a018dfc/&quot;&gt;见链接&lt;/a&gt;），所以一开始以为这次也不会难，结果。。。咳咳，还是要正视自己的水平的。因为服务器上装的是系统是CentOS，以前自己捣鼓的时候用的是Ubuntu，差别还是不少的，所以特此记录自己踩过的坑，也给其他人一些经验帮助。&lt;/p&gt;
&lt;p&gt;这次折腾总共分为两个阶段：最初打算直接通过pip下载TensorFlow安装包安装，结果完全失败，不过在此期间摸清了许多限制条件，也为第二个阶段——通过编译的方式安装TensorFlow打下了基础。&lt;/p&gt;
&lt;p&gt;系统版本：CentOS release 6.8 (Final)    64位&lt;/p&gt;
    
    </summary>
    
      <category term="技术" scheme="https://bluesmilery.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="CentOS" scheme="https://bluesmilery.github.io/tags/CentOS/"/>
    
      <category term="TensorFlow" scheme="https://bluesmilery.github.io/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>增强学习 Reinforcement learning part 4 - Model-Free Prediction</title>
    <link href="https://bluesmilery.github.io/blogs/a6aaca4e/"/>
    <id>https://bluesmilery.github.io/blogs/a6aaca4e/</id>
    <published>2017-05-16T20:56:22.000Z</published>
    <updated>2018-01-31T03:45:13.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文是在学习David Silver所教授的Reinforcement learning课程过程中所记录的笔记。因为个人知识的不足以及全程啃生肉，难免会有理解偏差的地方，欢迎一起交流。</p><p>课程资料：<a href="http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html" target="_blank" rel="noopener">http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html</a></p><h2 id="1、Introduction"><a href="#1、Introduction" class="headerlink" title="1、Introduction"></a>1、Introduction</h2><p>上一节讲的东西是基于已知的MPD，也就是有模型学习，而实际中很多情况下MDP是未知的，各个状态之间的转移概率以及reward很难得知，所以这种环境称为model free。</p><p>首先先讲model free prediction，类似于DP中的policy evaluation，去估计这个未知的MDP中各个状态的value function。]</p><p>下一节会讲model free control，类似于DP中的policy iteration，去最优化这个未知的MDP中各个状态的value function。</p><a id="more"></a><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/2jKbEh0B4d.png" alt=""></p><h2 id="2、Monte-Carlo-Learning"><a href="#2、Monte-Carlo-Learning" class="headerlink" title="2、Monte-Carlo Learning"></a>2、Monte-Carlo Learning</h2><p>在有模型的时候给定一个policy pi后对这个policy进行评估很简单，但是在model free的情况下，不知道MDP的状态转移概率及reward（但是知道state），就无法用原来的公式进行计算。所以MC采用的学习方式是learn directly from episodes of experience。这里episode理解为MDP中的某个序列，比如在之前的student例子中，C1 C2 C3 Pass Sleep就可理解为一个episode。MC的思想就是在MDP中，依据给定的policy pi去行动，会依次经过一些state并且得到reward。如果不断的尝试（尝试一次就是一个episode），那么就可以大体估计出MDP的状态转移概率和reward。</p><p>需要注意的是MC学习用的episode必须是完整的和有终止状态的。（完整可以这么理解，假如从C1 state开始，在policy pi下会依次经过C2 C3 Pass Sleep这些state，其中sleep是终止状态，那么C1到sleep就被称为一个完整的episode，要使用MC方法来计算C1的state value function，那必须使用这个完整的episode。与之相对的是下面要讲的TD方法是使用不完整的episode，比如计算C1的state value function ，只需使用C1 C2就可以）</p><p>使用Monte-Carlo进行policy evaluation。回忆下原来计算state value function的时候是求从状态s开始对未来的期望收益，而在MC中，收益的计算方法依旧一样，因为MC使用完整的episode，所以对于状态s后面的reward都知道。但是不再是对未来的收益求期望了（因为转移概率不知道），我们采用最简单的方法，求平均value = mean return</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/kdmeEgec7H.png" alt=""></p><p>在一个episode，如果经过了状态s，那么N(s)就累加一，S(s)累加上这次的return。然后状态s的V(s)就等于S/N，如果尝试的episode次数足够多，那么求出来的V(s)就是Vpi(s)。（原因是尝试的次数足够多后，整个policy的所有情况都会遍历到，从状态s往后的各种情况的return的概率用频率逼近，所以也就和求期望是一样的了）</p><p>可以看到first标记为红色，意思是在一个episode中，第一次经过状态s才计数，如果一个episode中经过了状态s好几次（循环），那么后面几次都不算的。与之相对的是every，就是每次经过状态s都计数。</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/0CJcGbjE24.png" alt=""></p><p>下面讲了一个例子，玩blackjack 21点</p><p>简单描述下建模过程，每个state由三个因素构成，agent手中牌的点数和，dealer显示的一张牌的点数，agent手中有没有ace（也就是A，这张牌既可以当1点，也可以当10点），这是agent能观察到的environment。agent的action有两种，stick和twist。reward按照胜负来表示，赢了+1，平手为0，输了-1</p><p>后面是使用MC来评估value function，在经过10000次episode（也就是10000局比赛），可以发现图像并不平整还有起伏，说明还有噪声，也就是还没收敛到Vpi，在经过50000局比才后，图像很平整，就可以认为已经收敛到Vpi了</p><table><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/KKdmKJ0l5B.jpg" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/I4LJ5C9EGb.jpg" alt=""></td></tr></table><p>有个问题可以改进，那就是求平均值那里，原来是把所有的值都累加完再求，而其实<strong>均值是可以增量求的</strong>。k个值的均值等于前k-1值的均值加上后面那部分，后面那部分可以看作是一个误差，什么与什么的误差呢，就是前k-1个求出来的均值是uk-1，预计下一个值也是uk-1，但实际上第k个值是xk，所以就产生了个误差，所以就要把这部分误差加上去修正平均值。</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/mh6b7i82G2.png" alt=""></p><p>所以在MC中更新V(s)可以用增量的方式，每完成一个episode，便可以用Gt去增量更新V(s)</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/3Db7lddldb.png" alt=""></p><p>后面误差前的系数虽然是跟次数有关，但实际上也可以用一个固定值代替，因为在实际情况中，记录次数需要额外开销，并且如果是一个动态的系统，也没法从头到尾一直记录次数。可以证明的是，用固定值代替也不影响结果。</p><p>In non-stationary problems, it can be useful to track a running mean, i.e. forget old episodes.</p><p>In real world, don’t want to remember everything.</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/68fe1CC2Ib.png" alt=""></p><h2 id="3、Temporal-Difference-Learning"><a href="#3、Temporal-Difference-Learning" class="headerlink" title="3、Temporal-Difference Learning"></a>3、Temporal-Difference Learning</h2><p>TD称为时序差分学习。MC因为要完成一个episode后才能更新V(s)，效率比较低，所以TD便改进了这一点，learn directly from incomplete episodes of experience，TD使用的是不完整的episode。</p><p>MC使用的是真实的return来更新V(s)，但是TD不是。TD使用一步真实的reward加上对下一个状态的state value function的猜测，所以TD updates a guess towards a guess。这种方法的好处就是走出一步就可以更新V(s)，而不用等整个episode完成。</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/1AgbFc8ADi.png" alt=""></p><p>下面举个例子，下班回家时对耗时的估计。可以看到MC要等整个路程都完成后才能更新各个状态的值，而TD在进行到下一状态后就可以更新上一状态的值。</p><table><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/GmajgKga07.png" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/LI3eBgh2k5.jpg" alt=""></td></tr></table><p>所以MC与TD相比较</p><ul><li>TD can learn before knowing the final outcome<br>TD在每步结束后就可以实时学习（learn online），也就是实时更新state value function。但是MC必须整个episode完成后才能求出Gt，从而更新state value function</li><li>TD can learn without the final outcome<br>TD可以使用不完整的episode学习，MC只能使用完整的episode学习。这样的话，TD可以用于continuing (non-terminating)的environment中，而MC只能用于episodic (terminating)的environment中</li></ul><p>再来讨论下Bias/Variance平衡的问题</p><p>bias：MC中的Gt是属于无偏估计的，因为用到的reward都是真实的没有偏差的，但是TD target是有偏差的估计，因为V(st+1)是猜的，不是真实的。</p><p>variance：因为MC的Gt要加很多项，每一项都会存在随机的噪声，所以MC的variance会很高，而TD只有两项，所以受到的随机因素影响很小，从而TD的variance就小。</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/b9CCaLibjh.png" alt=""></p><p>所以这是MC与TD第二个区别</p><ul><li>MC has high variance, zero bias<ul><li>Good convergence properties</li><li>(even with function approximation)</li><li>Not very sensitive to initial value</li><li>Very simple to understand and use</li></ul></li><li>TD has low variance, some bias<ul><li>Usually more efficient than MC</li><li>TD(0) converges to vpi(s)</li><li>(but not always with function approximation)</li><li>More sensitive to initial value</li></ul></li></ul><p>下面再讲个例子，有一条直线，左右两边是终止状态，中间是ABCDE五个状态，action是向左或者向右，只有在E向右的时候才会得到reward+1，左边是对各个状态的state value function的估计，可以看到经过100次episode后，已经比较接近真实的value了。右图是统计了估计的value与真实的value之间的RMS误差，上半部分是MC，下半部分是TD，分别使用了不同的alpha值，可以看到TD整体是优于MC的，并且随着episode次数的增加，误差逐渐减少并趋于稳定，在平稳部分还有波动那就是噪声引起的，这也可以看出TD的variance比MC的小很多。</p><table><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/54HDEafKDg.jpg" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/jBFgmgFlJg.jpg" alt=""></td></tr></table><p>之前说道MC和TD在experience趋向无穷的时候，V(s)才会收敛于Vpi(s)，那如果experience是有限的呢？</p><p>比如这个例子，总共只有八次episode，求AB两个状态的state value function。可以先求B的，8次中有两次得到的reward是0，6次得到的reward是1，所以V(B) = 6/8 = 0.75。那A的呢，如果以MC的角度去看，出现A的episode只有第一个，并且在AB收到的reward都为0，所以Gt = 0，从而V(A) = 0。但是从TD的角度看，我们先用得到的episode构建出MDP，所以V(A) = 0 + V(B) = 0.75</p><table><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/hBgA214Af3.png" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/A163FgDI4J.jpg" alt=""></td></tr></table><p>从这个例子中也可以看出在有限的experience下，MC和TD的计算方法是不一样的</p><p>MC：是求最小均方误差，就是使用出现过A的episode来的计算Gt，然后更新state value function</p><p>TD：是使用最大似然马尔可夫模型，根据所有的episode计算出状态转移概率和reward，从而求出state value function</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/jdjIBL6A94.png" alt=""></p><p>通过这个例子可以看出MC与TD的第三点区别，TD能利用马尔可夫性，所以在马尔可夫环境下更有效率，而MC无法利用马尔可夫性，所以在非马尔可夫环境下更有效率</p><ul><li>TD exploits Markov property<ul><li>Usually more efficient in Markov environments</li></ul></li><li>MC does not exploit Markov property<ul><li>Usually more effiective in non-Markov environments</li></ul></li></ul><p>下面总结一下MC、TD、DP的区别，MC是使用一个完整的episode来更新V(s)，TD是使用不完整的episode来更新V(s)，而DP是使用状态s后面下一步所有的状态来计算V(s)</p><table><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/gc6h0AkGKa.png" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/2JL8I4e6b4.png" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/71J69Gjde6.png" alt=""></td></tr></table><p>而且还有两个概念前面没有提及，bootstrapping和sampling</p><p>sampling可以理解成抽样，就是使用一个episode，可以看到MC和TD是使用sampling的，而DP是使用遍历下一层所有的情况，所以不使用sampling</p><p>bootstrapping可以理解成是不是需要使用完整的episode，像TD和DP都是使用下一层来计算上一层的，而MC是使用一个完整的episode，一直到terminate</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/8G3Imk6g58.png" alt=""></p><p>下面这张图从bootstrapping和sampling两个维度把之前讲的方法进行了分类，一目了然</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/712FL9gdBI.jpg" alt=""></p><h2 id="4、TD-λ"><a href="#4、TD-λ" class="headerlink" title="4、TD(λ)"></a>4、TD(λ)</h2><p>之前讲的TD都是只考虑了未来的一步，那可不可以考虑未来的两步呢，甚至是三步四步？当然可以，见下图，这就称作n-step TD，如果n无穷，那么就是MC方法了</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/iDHcgib9L4.jpg" alt=""></p><p>在n-step TD中Gt的计算方法如下</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/jBmfE4d9F0.png" alt=""></p><p>那么n-step TD的V(s)更新公式变为</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/KfG3jKDlIF.png" alt=""></p><p>下图是Large Random Walk Example，统计了经过10次episode后的RMS误差，横坐标为选择不同的alpha，图中不同的曲线代表了n的不同取值。</p><p>上面的是online的，immediately update value function。下面的是offline，delay update until the end of episode</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/0L8d8I2CcB.jpg" alt=""></p><p>可以看到online与offline误差最小的位置不一样，如果我们换了一个MDP或者做了一些其他的变化，那这个最优点就会变化，选的n就会变，如果每次都要画这种图来比较的话就太麻烦了，有没有什么办法可以解决一下？</p><p>答案是有的，那就是把不同的n-step组合一下，比如把n=4和n=2按照一半一半的比例组合一下。但是这么组合很傻瓜，有没有什么更有效的组合方法？</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/IFCa31G44j.png" alt=""></p><p>答案也是有的，把所有的n-step都综合起来，各自的比例分别是(1-λ)λn-1，所以新的Gtλ就是把Gt(n)按照各自的比例加起来，然后用Gtλ来更新V(s)，所以这种TD称作Forward-view TD(λ)</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/gdhelHEg0b.jpg" alt=""></p><p>关于n-step各自的比例(1-λ)λn-1是服从指数分布的</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/HmckfdB5f2.jpg" alt=""></p><p>Forward-view TD(λ)如下图所示，在更新状态St时需要用到未来的值，所以被称作像前看forward view。但是需要注意的是，因为需要用到所有的n-step，所以它也需要完整的episode的才能计算，这一点与MC一样</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/a2e76f9GLe.jpg" alt=""></p><p>这是Forward-view TD(λ) on Large Random Walk，图中不同的曲线代表着选择了不同的λ</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/8fLKcJHem0.jpg" alt=""></p><p>既然有forward view，那么肯定有backward view</p><ul><li>Forward view provides theory</li><li>Backward view provides mechanism</li><li><strong>Update online, every step, from incomplete sequences</strong></li></ul><p>讲述一个新概念，Eligibility Traces，字面理解是资格迹或者传导径迹，其实质是判断过去经历过的状态中哪些对现在的状态有影响。比如依次经过了三次响铃，一次灯亮后，你遭到了电击，那么是三次响铃导致了电击，还是说灯亮后会导致电击。当然都有可能，所以我们从把这两方面都考虑进去，考虑频率的称为frequency heuristic，考虑最近的称为recency heuristic，然后把它们俩结合起来。</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/1L1I92h7af.jpg" alt=""></p><p>下面是backward view TD(λ)的主要含义，对于每个state来说都有自己的eligibility trace E(s)，在更新V(s)的时候把E(s)也考虑进去了，乘在TD error部分</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/Ihdf4HLdfg.jpg" alt=""></p><p>E(s)初始化为0，在第一次经过状态s的时候便会+1，并且每一时刻E(s)都会按照</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/8l7BhAe1Ci.jpg" alt=""></p><p>进行指数级衰减，如果再次经过了状态s，那么再+1。这个+1就是下图中突然向上的那一下。</p><p>假设下图描述的就是状态s的eligibility trace，在最一开始的时候为0，然后第一次经过状态s后+1，然后开始衰减，但是过了很短的时间又经过了状态s，所以又向上冲了一下。所以图中前面一部分就是很短的时间内经过了状态s四次，所以此时E(s)就很高，但是在接下来很长的时间内没有在经过状态s，所以E(s)快速衰减，接近于0。此时再次连续两次经过状态s，又很长时间没经过，然后再次经过状态s，然后就结束了。所以图片就是状态s的E(s)的值的变化情况。</p><table><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/2l20ahiH1f.jpg" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/68916dFA16.png" alt=""></td></tr></table><p>backward view TD(λ)的伪代码，看着比较容易理解这个算法是怎么样的，以及eligibility trace是如何更新的</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/IAkd36Ld6e.jpg" alt=""></p><p>Forward view和backward view的本质是一样的，只是从两个角度去描述了同一件事情。Forward view是向未来看，用未来的reward来更新当前状态，而backward view是向过去看，在当前状态收到reward了以后，它会把reward传递回过去，去更新过去的点（以过去状态的角度看来，这不也就是用未来的reward来更新自己吗）。并且forward view和backward view都是离当前状态近的占的比例高，如果两个状态离的很远（假设为A,B，以时间为顺序A在过去B在未来），从forward view的角度看，B对A的影响很小，从backward view的角度看，B往回传递给A的影响也很小，是统一的。</p><p>当λ=0时，便是最一开始讲的TD(0)</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/3aBdBHGecG.png" alt=""></p><p>而当λ=1时，是与MC是等价的，下面几张图就说的是这件事</p><ul><li>When λ = 1, credit is deferred until end of episode</li><li>Consider episodic environments with offline updates</li><li>Over the course of an episode, total update for TD(1) is the</li><li>same as total update for MC</li></ul><table><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/fk3H2D6l76.png" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/9hhAF66270.png" alt=""></td></tr></table><ul><li>TD(1) is roughly equivalent to every-visit Monte-Carlo</li><li>Error is accumulated online, step-by-step</li><li>If value function is only updated offline at end of episode</li><li>Then total update is exactly the same as MC</li></ul><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/69I12jlK8I.png" alt=""></p><p>实质上forward-view TD(λ)和backward view TD(λ)是等价的，是一个算法从两个角度去看</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/H3HLj6deh9.png" alt=""></p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/ImFcjGIgDd.png" alt=""></p><p>Offline updates</p><ul><li>Updates are accumulated within episode</li><li>but applied in batch at the end of episode</li></ul><p>Online updates</p><ul><li>TD(λ) updates are applied online at each step within episode</li><li>Forward and backward-view TD(λ) are slightly different</li><li>NEW: Exact online TD(λ) achieves perfect equivalence</li><li>By using a slightly dierent form of eligibility trace</li><li>Sutton and von Seijen, ICML 2014</li></ul><p>总结：</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/lkKf7hkl2l.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是在学习David Silver所教授的Reinforcement learning课程过程中所记录的笔记。因为个人知识的不足以及全程啃生肉，难免会有理解偏差的地方，欢迎一起交流。&lt;/p&gt;
&lt;p&gt;课程资料：&lt;a href=&quot;http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;1、Introduction&quot;&gt;&lt;a href=&quot;#1、Introduction&quot; class=&quot;headerlink&quot; title=&quot;1、Introduction&quot;&gt;&lt;/a&gt;1、Introduction&lt;/h2&gt;&lt;p&gt;上一节讲的东西是基于已知的MPD，也就是有模型学习，而实际中很多情况下MDP是未知的，各个状态之间的转移概率以及reward很难得知，所以这种环境称为model free。&lt;/p&gt;
&lt;p&gt;首先先讲model free prediction，类似于DP中的policy evaluation，去估计这个未知的MDP中各个状态的value function。]&lt;/p&gt;
&lt;p&gt;下一节会讲model free control，类似于DP中的policy iteration，去最优化这个未知的MDP中各个状态的value function。&lt;/p&gt;
    
    </summary>
    
      <category term="笔记" scheme="https://bluesmilery.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement Learning" scheme="https://bluesmilery.github.io/tags/Reinforcement-Learning/"/>
    
  </entry>
  
  <entry>
    <title>增强学习 Reinforcement learning part 3 - Planning by Dynamic Programming</title>
    <link href="https://bluesmilery.github.io/blogs/b96003ba/"/>
    <id>https://bluesmilery.github.io/blogs/b96003ba/</id>
    <published>2017-04-10T20:04:48.000Z</published>
    <updated>2018-01-31T03:45:02.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文是在学习David Silver所教授的Reinforcement learning课程过程中所记录的笔记。因为个人知识的不足以及全程啃生肉，难免会有理解偏差的地方，欢迎一起交流。</p><p>课程资料：<a href="http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html" target="_blank" rel="noopener">http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html</a></p><h2 id="1、Introduction"><a href="#1、Introduction" class="headerlink" title="1、Introduction"></a>1、Introduction</h2><p>这一部分是用动态规划来解决MDP问题，所以先介绍下什么是动态规划</p><p><mark>Dynamic Programming</mark>（动态规划），是用来解决一些复杂的问题，将复杂的问题分解为一些子问题，然后分别解决这些子问题，最后再把解决方案合并得到复杂问题的解</p><ul><li>Dynamic：代表这个问题是连续的或者是跟时间相关的</li><li>Programming：不是编程的意思，而指的是优化一个程序</li></ul><a id="more"></a><p>想要使用DP来解决一个问题，那么这个问题需要具有以下两种性质：</p><ul><li>Optimal substructure：满足最优化原理，最优解分解后针对子问题也是最优解</li><li>Overlapping subproblems：子问题重复出现，对每一种子问题只计算一次，然后将结果存起来，下次直接使用</li></ul><p>而MDP刚好满足这两条性质，bellman equation具有递归分解性，说明可以分解为子问题并且会重复出现（类似于斐波那契数列）；value function就相当于能够存储并且重复利用的解</p><p><strong>要使用DP来解决MDP问题，假设条件是对MDP是充分认知（full knowledge）的</strong></p><p>对于MDP来说，还是从两个方面来说，prediction and control</p><p>prediction的目的是给你policy，然后去预测使用这个policy的话各个state的value function，也就是预测使用这个policy的话未来的收益</p><p>control的目的是不给你policy，自己去寻找最优的policy以及未来最优的收益</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/J028IlH8i7.png" alt=""></p><p>扩展：动态规划还可以解决其他许多问题</p><ul><li>Scheduling algorithms</li><li>String algorithms (e.g. sequence alignment)</li><li>Graph algorithms (e.g. shortest path algorithms)</li><li>Graphical models (e.g. Viterbi algorithm)</li><li>Bioinformatics (e.g. lattice models)</li></ul><h2 id="2、Policy-Evaluation"><a href="#2、Policy-Evaluation" class="headerlink" title="2、Policy Evaluation"></a>2、Policy Evaluation</h2><p>策略评估</p><p>需要解决的问题：评估一个给定的policy</p><p>解决方案：以迭代次数为维度（或者理解成时间，一次迭代相当于过去了一个时刻，agent执行了一次action）不断去迭代Bellman Expectation Equation</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/eLDB0cbdGh.png" alt=""></p><p>可以证明，<strong>最终value function会收敛到vpi</strong></p><p>采用同步更新的方式，在新的时刻所有的state同时更新</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/7F8m89m2L7.png" alt=""></p><p>计算方法如下：</p><p>套用Bellman Expectation Equation，加上迭代次数维度，依据马尔可夫性，未来只与现在有关，所以求state s的k+1时刻的value function，就要使用k时刻state s’的value function，其中s’是s的后续state</p><table><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/Ll2i3G7E1c.png" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/IFA6cI4gEA.png" alt=""></td></tr></table><p>上面的是RL part2中的计算方程，相当于是下面的计算方程迭代到最后已经收敛的形式</p><table><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/0HJ54KeD14.png" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/ejd0leDJCF.png" alt=""></td></tr></table><p>下面举个例子gridworld</p><p>这是个4x4的方格，那两个灰色的方格是特殊的state，可以看作是终点，其他14个白色的方格是普通的state，在白色方格时有四种action选择，向上下左右移动，各自的概率为0.25。每移动一步reward减1，直到灰色的方格</p><p>所以这个例子可以看作你随机出现在某个方格上，然后用最少的步数走到灰色方格。</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/4KkGKDG329.png" alt=""></p><p>下面是进行policy evaluation的迭代过程，k代表第几次迭代</p><p>先说左边的，选择的policy是random policy，就是在任何一个state，选择四个方向的概率都为0.25。初始v0的时候所有state的value都是0。然后v1时，利用上面vk+1(s)的公式进行计算（0.25x(-1+0)x4）。下面以v2中两个state来说明具体的计算过程</p><p><u>先计算state6</u>（就是第二行第三个，这里对应公式的话就是s）在这次迭代中（对应公式的话就是k+1）的state value：如果向上走的话会移动到state2（对应公式的话就是s’），那这个action value就是reward(-1)加上state2上一时刻（对应公式的话就是k）的state value(-1)，所以向上移动是0.25x(-1-1)。向其他三个方向移动的计算方式和向上移动类似，结果也都是0.25x(-1-1)。所以state6在v2时的state value就是0.25x(-1-1)x4=-2</p><p><u>再计算state1</u>（第一行第二个）的state value：如果选择向上走的话，因为上面没有格子了，所以会停在原地，那这次action value就是移动的reward(-1)加上state1上一时刻的state value(-1)，所以向上移动是0.25x(-1-1)。向下向右类似。向左移动的话是reward(-1)加上灰色格子上一时刻的state value(0)，所以向左移动就是0.25x(-1+0)。所以state1在v2时的state value就是0.25x(-1-1)x3+0.25x(-1+0)=-1.75。图中显示是-1.7，那是因为只能显示两位数。</p><p>如果一直迭代下去，当k为无穷的时候，各个state的value就已经收敛不会变了。</p><table><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/ilhLcGJKm0.png" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/JAB7kbLJbK.png" alt=""></td></tr></table><p>再说下右边的，显示的是每次迭代根据左边计算出来的state value选择出来的greedy policy。greedy policy指的是选出各个action value中最高的所对应的action。比如在v2时，state1的四个action value中，向左移动的action value是最高的，所以greedy policy在state1处只选择向左移动</p><p>当random policy经过许多步迭代后state value收敛到vpi时，这时候选出的greedy policy就是random policy的optimal policy</p><p>注意：evaluate任何一个policy（这里是random policy）都能得到greedy policy</p><p>总结：进行策略评估的实质就是计算在使用policy pi时各个state的value function，但是因为reword和action的存在，随着时间的推移（action次数的增加），每个state的value function会变化，但是最终会收敛到一个固定值vpi，然后使用greedy policy选出policy pi的optimal policy</p><h2 id="3、Policy-Iteration"><a href="#3、Policy-Iteration" class="headerlink" title="3、Policy Iteration"></a>3、Policy Iteration</h2><p>策略迭代</p><p>在策略评估中，讲的是在给定一个policy pi后，先对其进行evaluate得到vpi，然后使用greedy policy来improve policy，得到新的policy pi’</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/101a9hKiaC.png" alt=""></p><p>在上面gridworld的例子中，因为这个例子很简单，所以经过一次improve后得到的pi’就已经是optimal policy pi*</p><p>但是通常说来，一般需要经过多次evaluation/improvement的过程才能得到pi*</p><p>这种不断evaluation/improvement的过程就称作策略迭代</p><p><strong>注意：类似于policy evaluation中state value会收敛到vpi，在policy iteration中policy肯定会收敛到pi*</strong></p><table><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/BFA2CI6G5m.jpg" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/Hel0eHdai0.png" alt=""></td></tr></table><p>evaluation和improvement过程中的算法可以是任意的</p><p>example：</p><table><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/8CeJK6GFi3.jpg" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/I21d5hiche.jpg" alt=""></td></tr></table><p>下面是具体讲一下improve过程，证明了为什么improve后vpi’(s)&gt;=vpi(s)</p><table><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/kJ4B7BlhCJ.png" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/97b7kJ4dJ2.png" alt=""></td></tr></table><p>modified：在evaluation部分，一定要等state value达到vpi吗？</p><p>事实上，这并不是必须的，就好比上面的gridworld例子，在第三次迭代的时候其实就已经是optimal policy了，所以没必要继续迭代下去</p><p>所以我们可以通过一些方法提前停止evaluation，比如在state value收敛的过程中设置epsilon，小于则停止迭代；或者是设置迭代k次就停止</p><p>如果设置evaluation只迭代一次，即k=1，那么这就是下面要讲的value iteration</p><p>总结：policy iteration相比于policy evaluation而言，后者是一次evaluation/improvement过程，只能得到比policy pi更好的policy pi’，而前者是多次evaluation/improvement过程，最终能得到该MDP的optimal policy pi*</p><h2 id="4、Value-Iteration"><a href="#4、Value-Iteration" class="headerlink" title="4、Value Iteration"></a>4、Value Iteration</h2><p>值迭代</p><p>上面说过能用DP解决的问题需要具有一个特点，满足最优化原理（Principle of Optimality），就是最优解分解之后针对每个子问题也是最优解，对于MDP来说，就是optimal policy可以分为两部分，当前state的optimal action A*，后续state的policy也是optimal policy<br>Any optimal policy can be subdivided into two components:</p><ul><li>An optimal first action A*</li><li>Followed by an optimal policy from successor state S0</li></ul><p>所以，若要满足一个policy pi使的state s的value为optimal value，当且仅当policy pi使得s的后续state s’的value为optimal value</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/f9GH2k4901.png" alt=""></p><p>所以知道了s’的optimal value便可以倒推出来s的optimal value</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/chHIB2ahGI.jpg" alt=""></p><p>所以这便是value iteration的思想所在，迭代重复上面这个公式</p><p>直观上看来，是从最后一个state开始，倒推到第一个state，下面以一个例子来说明下</p><p>这个例子的目的是每个state找出到达左上角的最短路径（步数），其他方面类似于之前gridworld的例子，action是向四个方向移动，每移动一步reward减1</p><p>V1初始时所有的state的value都是0，然后在V2时，每个state从四个action value中选出最高的那个作为自己的state value（在policy iteration中是将四个action value求期望作为自己的state value），以第二行第二个为例，它在V1V2V3的时候四个action value都是一样的，所以没有相对的optimal，但是从V4开始，向上和向左比向下向右好，所以它的state value就一直为-2，拿V6来说，它的state value = max(-2, -2, -4, -4) = -2</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/888CK5hL90.png" alt=""></p><p>在这个例子中需要注意这么几个问题：</p><ul><li>每一次迭代中所有的state都要计算value，只不过有些state每次max的时候都是那些值（先达到了optimal value）所以看起来就不变了</li><li>在开始的时候不像policy iteration那样有个明确的policy，而且在迭代过程中state value并不对应于哪个policy，只有在最终迭代完成后（在例子中是V7）才对应于一个policy，而且还是optimal policy</li><li>如果以右下角那个state为开始的话，那路径就是-6 -5 -4 -3 -2 -1 0，而计算过程是-1 -2 -3 -4 -5 -6，所以符合上面说的Principle of Optimality的倒推性</li></ul><p>所以对于value iteration来说</p><p>需要解决的问题：找到optimal policy pi*</p><p>解决方案：不断去迭代Bellman optimality Equation</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/7FbcF26E5K.png" alt=""></p><p>可以证明， <strong>最终value function会收敛到v*</strong></p><p>采用同步更新的方式，在新的时刻所有的state同时更新</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/0cL2DLfmib.png" alt=""></p><p>计算过程：</p><table><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/EeFJ4dfg6c.png" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/FjBAl9jHbk.png" alt=""></td></tr></table><p>总结：value iteration的本质就是在每次迭代中每个state都选择optimal value，那么当所有的state都选择出了optimal value后，所对应的policy就是optimal policy。</p><p>对比policy iteration，是迭代到收敛后每个state才选择optimal value，然后这时候对应的policy就是improve后的policy pi’</p><p>有个value iteration的例子：需要安装java环境</p><p><a href="http://www.cs.ubc.ca/~poole/demos/mdp/vi.html" target="_blank" rel="noopener">http://www.cs.ubc.ca/~poole/demos/mdp/vi.html</a></p><p>下表是对DP的一个总结</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/I7B4KjDCFG.png" alt=""></p><p>5、</p><ul><li>Extensions to Dynamic Programming</li><li>Asynchronous Dynamic Programming</li><li>In-Place Dynamic Programming</li><li>Prioritised Sweeping</li><li>Real-Time Dynamic Programming</li><li>Full-Width Backups</li><li>Approximate Dynamic Programming</li></ul><p>6、</p><ul><li>Contraction Mapping</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是在学习David Silver所教授的Reinforcement learning课程过程中所记录的笔记。因为个人知识的不足以及全程啃生肉，难免会有理解偏差的地方，欢迎一起交流。&lt;/p&gt;
&lt;p&gt;课程资料：&lt;a href=&quot;http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;1、Introduction&quot;&gt;&lt;a href=&quot;#1、Introduction&quot; class=&quot;headerlink&quot; title=&quot;1、Introduction&quot;&gt;&lt;/a&gt;1、Introduction&lt;/h2&gt;&lt;p&gt;这一部分是用动态规划来解决MDP问题，所以先介绍下什么是动态规划&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;Dynamic Programming&lt;/mark&gt;（动态规划），是用来解决一些复杂的问题，将复杂的问题分解为一些子问题，然后分别解决这些子问题，最后再把解决方案合并得到复杂问题的解&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dynamic：代表这个问题是连续的或者是跟时间相关的&lt;/li&gt;
&lt;li&gt;Programming：不是编程的意思，而指的是优化一个程序&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="笔记" scheme="https://bluesmilery.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement Learning" scheme="https://bluesmilery.github.io/tags/Reinforcement-Learning/"/>
    
  </entry>
  
  <entry>
    <title>增强学习 Reinforcement learning part 2 - Markov Decision Process</title>
    <link href="https://bluesmilery.github.io/blogs/e4dc3fbf/"/>
    <id>https://bluesmilery.github.io/blogs/e4dc3fbf/</id>
    <published>2017-03-21T19:11:51.000Z</published>
    <updated>2018-01-31T03:44:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文是在学习David Silver所教授的Reinforcement learning课程过程中所记录的笔记。因为个人知识的不足以及全程啃生肉，难免会有理解偏差的地方，欢迎一起交流。</p><p>课程资料：<a href="http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html" target="_blank" rel="noopener">http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html</a></p><h2 id="1、Markov-Processes"><a href="#1、Markov-Processes" class="headerlink" title="1、Markov Processes"></a>1、Markov Processes</h2><p>在RL中，<strong>MDP是用来描述environment的，并且假设environment是full observable的</strong></p><p>i.e. The current state completely characterizes the process</p><p>许多RL问题可以用MDP来表示：</p><ul><li>Optimal control primarily deals with continuous MDPs</li><li>Partially observable problems can be converted into MDPs</li><li>Bandits are MDPs with one state</li></ul><a id="more"></a><p>先介绍两个基本知识</p><p><em>Markov Property</em>：The future is independent of the past given the present</p><p>未来只与现在有关，与过去无关。这一点性质相当于是简化了模型。</p><p><em>State Transition Matrix</em>：从状态s转换到状态s’的概率</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/5IDhf7mckm.png" alt=""></p><p>把所有的状态转换概率写成矩阵P     PS：矩阵每一行和为1</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/3fDk2ChCAi.png" alt=""></p><p><strong>Markov Process</strong>：具有马尔可夫性的随机过程，用&lt;S, P&gt;来表示</p><ul><li>S is a (finite) set of states</li><li>P is a state transition probability matrix</li></ul><p>example：</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/AAJkdFdCBI.png" alt=""></p><h2 id="2、Markov-Reward-Processes"><a href="#2、Markov-Reward-Processes" class="headerlink" title="2、Markov Reward Processes"></a>2、Markov Reward Processes</h2><p>MRP是在MP的基础上增加了一个值——reward。MRP用&lt;S, P, R, gamma&gt;来表示</p><p>其中S和P的定义与MP中的一样，R代表的是到达状态s后（或者说在状态s时）会得到的奖励（或者说是收益），gamma是折现因子，用于表示未来的收益折算到现在的比例，范围是0~1</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/ebHbbF59ha.png" alt=""></p><p>example：</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/hH93g4EKfc.png" alt=""></p><p><strong>Return</strong>：从第t步开始，未来N步的总收益，未来的部分是有折扣的。return只是针对某个sequence而言</p><p>The return Gt is the total discounted reward from time-step t.</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/EabBfi6BEa.png" alt=""></p><p>gamma越接近于0表示越看重眼前的收益，越接近于1表示越看重未来的收益。</p><p>为什么要用gamma这个discounted factor？</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/FCHihJcBkk.png" alt=""></p><p>example：</p><p>假如MRP中某个sequence是C1 C2 C3 Pass Sleep，那么在C1时的return G1 = - 2 - 2 <em> 0.5 - 2 </em> 0.25 + 10 * 0.125 = -2.25</p><p>注意这里R2就表示在C1（S1）处的reward，之所以下标不一样，是因为在建模时，对于t的界定是新的t+1是从agent的action结束后，environment反馈reward和observation时开始算，所以状态St = s的reward用Rt+1来表示</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/bGGk6Kc4m5.png" alt="">&gt;</p><p><strong>Value Function</strong>：MRP中只有state value function——v(s)，状态s的value function。表示的是从状态s开始对未来的期望收益。（从状态s开始往后所有的sequence的收益的期望）</p><p>The state value function v(s) of an MRP is the expected return starting from state s</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/h6I1L65Jde.png" alt=""></p><p>example：</p><p>计算方法见Bellman Equation部分</p><table><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/LggGfhFbBB.png" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/bcCcb8jEhf.png" alt=""></td></tr></table><p><strong>Bellman Equation</strong>：我们将v(s)进行一些数学变换，会发现v(s)可以分解成两部分，一部分是状态s的immediate reward Rt+1，另一部分是折扣过的下一state的v(St+1)</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/4hE85lCam4.png" alt=""></p><p>假设下一个state有两个，那么结构类似与下面这种树状图，将下一个的所有state都average起来求期望。就相当于树下面的加起来给上面的根节点</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/A0HmB32GmB.png" alt=""></p><p>所以最终v(s)可以表示为</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/db2413DJ2A.png" alt=""></p><p>所以上面那个例子中每个state的v(s)计算过程如下：</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/HdEeDH6L9m.png" alt=""></p><p>可能会注意到计算class3的时候用到了pub，而如果计算pub的时候又会用到class3，有种循环递归的问题，但是实际上v(s)的计算表达式可以用矩阵的形式来表示</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/55F9Cg7dHd.png" alt=""></p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/FHHiIGIEgD.png" alt=""></p><p>由于这是个线性方程，所以可以直接解出。那么上面说的循环递归的问题就不存在了，因为所有state的value function是同时求出来的</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/chab04bLEj.png" alt=""></p><p>但是直接解的复杂度为O(n3)，n为state的个数，所以如果要用直接解法那么只能用于很小的MRP，对于很大的MRP，有很多迭代的方法可以用：</p><ul><li>Dynamic programming（动态规划）</li><li>Monte-Carlo evaluation（蒙特卡罗强化学习）</li><li>Temporal-Difference learning（时序差分学习）</li></ul><h2 id="3、Markov-Decision-Processes"><a href="#3、Markov-Decision-Processes" class="headerlink" title="3、Markov Decision Processes"></a>3、Markov Decision Processes</h2><p>MDP是在MRP的基础上再加一个元素——action。MDP用&lt;S, A, P, R, gamma&gt;来表示</p><p>MDP是agent在自己的大脑中用来描述environment模型的，MDP中每个state都具有马尔可夫性</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/HC6IEIa7ma.png" alt=""></p><p>example：与MRP的example相比，概率变成了action，pub的state消失了，在这里如果在class3选择了pub这个action，直接会有0.2概率去class1，而不会有个pub这种state来停留</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/F604DLhE9d.png" alt=""></p><p><strong>Policy</strong>：在一个state处，agent接下来会采取各种action的概率分布。实质上policy就是定义了agent的行为，在这个state会怎么选，在下一个state会怎么选等等</p><p>MDP的policy具有马尔可夫性，只与当前state有关。所以policy也是time-independent的</p><p>加入了policy后，P和R的计算方式</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/3eB784FE2d.jpg" alt=""></p><p>Value Function：在MDP中有两种value function</p><p><strong>state-value function</strong>——vπ(s)，状态s的value function。在遵循policy的前提下，从状态s算起的未来期望收益</p><p>The state-value function vπ(s) of an MDP is the expected return starting from state s, and then following policy pi</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/ce9ad85gLD.jpg" alt=""></p><p><strong>action-value function</strong>——qπ(s, a)，在状态s处执行action a的value function。在遵循policy的前提下，在状态s处执行action a后算起的未来期望收益</p><p>The action-value function qπ(s, a) is the expected return starting from state s, taking action a, and then following policy pi</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/F5iH7582Ag.jpg" alt=""></p><p>v是针对state而言，q是针对action而言</p><p>example：</p><p>计算方法见Bellman Expectation Equation部分</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/1DKibehlEc.jpg" alt=""></p><p><strong>Bellman Expectation Equation</strong>：类似于MRP中的Bellman Equation，vpi(s)和qpi(s, a)也可以写成类似的表达式</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/mI4ECD0kJe.jpg" alt=""></p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/maf91FD5iL.jpg" alt=""></p><p>类似的，也可以用树状图来表示其计算过程</p><p>在state s处，有两种action的选择，每种action有自己的value function，所以state s的value function就等于两种action的value function的加权和。</p><p>而选好一种action后，可能会跳转到的下一state有两种，所以action a的value function就等于两个下一state的value function的加权和。</p><table><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/haddhfgmLh.jpg" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/m6eEiJ4c4I.jpg" alt=""></td></tr></table><p>把上面的树形结构链接起来后就是下面这个样子，如果要求state s的value function（也就是根结点是白点），那么如图左；如果要求action a的value function（也就是根结点是黑点），那么如图右。</p><table><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/kaglF63HfB.jpg" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/fJmf4l7Elc.jpg" alt=""></td></tr></table><p>example：</p><p>在MDP中vpi(s)的计算过程如下，假设选取每种action的概率相同</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/LhHCEmEEIK.jpg" alt=""></p><p>当然vpi(s)的计算还是可以用矩阵来表示的</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/L8f4KI62DE.jpg" alt=""></p><p>direct solution：</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/7i5c464i1A.jpg" alt=""></p><p>当然，在实际中我们的目标是寻求一个最优解，在state处选择哪个action收益最好，所以就有了以下的内容</p><p>Optimal Value Function：</p><p><strong>optimal state-value function</strong>——v* (s)，在所有的policy中，选出最大的vpi(s)<br>The optimal state-value function v*(s) is the maximum value function over all policies</p><p><strong>optimal action-value function</strong>——q* (s, a)，在所有的policy中，选出最大的qpi(s, a)<br>The optimal action-value function q*(s, a) is the maximum action-value function over all policies</p><p>OVF代表着MDP中最优的表现（收益）。如果我们知道了OVF，也就意味着这个MDP是solved</p><p>example：</p><p>右边那个图pub的q* 应该是9.4 = 1 + (0.2 x 6 + 0.4 x 8 + 0.4 x 10)</p><table><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/BE6Ljme1Ck.jpg" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/Dmc4ibc94a.jpg" alt=""></td></tr></table><p><strong>Optimal Policy</strong>：</p><p>先定义一下policy的好坏，如下，pi好于pi’</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/Ge7lehFkCd.jpg" alt=""></p><p>比较重要的是在任意一个MDP中，肯定会有一个optimal policy pi*</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/G82cILLLE5.jpg" alt=""></p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/AhbDKjh5Jf.jpg" alt=""></p><p>example：</p><p>红色的箭头就代表了optimal policy</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/E19k621JI0.jpg" alt=""></p><p><strong>Bellman Optimality Equation</strong>：</p><p>依旧用树状图来表示计算过程</p><table><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/lD95b50FCc.jpg" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/gHaD9gf6e1.jpg" alt=""></td></tr><tr><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/1E5eA59KiC.jpg" alt=""></td><td><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/b757mj0CDh.jpg" alt=""></td></tr></table><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/IHe7BDjEFD.jpg" alt=""></p><p>既然要找到MDP的最优解，那么就要解Bellman Optimality Equation</p><p>因为存在max的计算过程，所以BOE是一个非线性方程，通常没有什么直接解法，但是可以用一些迭代方法来解</p><ul><li>Value Iteration</li><li>Policy Iteration</li><li>Q-learning</li><li>Sarsa</li></ul><h2 id="4、Extensions-to-MDPs"><a href="#4、Extensions-to-MDPs" class="headerlink" title="4、Extensions to MDPs"></a>4、Extensions to MDPs</h2><ul><li>Infinite and continuous MDPs</li><li>Partially observable MDPs</li><li>Undiscounted, average reward MDPs</li></ul><p>思考：<br>episode是sequence</p><p>为什么用discount，因为我们没有一个perfect model，我们对未来做的决定不是完全相信，不能确定是百分百正确的，所以因为这种不完美不确定性，用discount来减少对现在的影响</p><p>alphaGo 考虑几步之后，这不就是考虑未来的reward，用gamma来控制考虑几步之后</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是在学习David Silver所教授的Reinforcement learning课程过程中所记录的笔记。因为个人知识的不足以及全程啃生肉，难免会有理解偏差的地方，欢迎一起交流。&lt;/p&gt;
&lt;p&gt;课程资料：&lt;a href=&quot;http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;1、Markov-Processes&quot;&gt;&lt;a href=&quot;#1、Markov-Processes&quot; class=&quot;headerlink&quot; title=&quot;1、Markov Processes&quot;&gt;&lt;/a&gt;1、Markov Processes&lt;/h2&gt;&lt;p&gt;在RL中，&lt;strong&gt;MDP是用来描述environment的，并且假设environment是full observable的&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;i.e. The current state completely characterizes the process&lt;/p&gt;
&lt;p&gt;许多RL问题可以用MDP来表示：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Optimal control primarily deals with continuous MDPs&lt;/li&gt;
&lt;li&gt;Partially observable problems can be converted into MDPs&lt;/li&gt;
&lt;li&gt;Bandits are MDPs with one state&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="笔记" scheme="https://bluesmilery.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement Learning" scheme="https://bluesmilery.github.io/tags/Reinforcement-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习 Machine learning part 1 - Linear Regression</title>
    <link href="https://bluesmilery.github.io/blogs/18a3f212/"/>
    <id>https://bluesmilery.github.io/blogs/18a3f212/</id>
    <published>2017-03-20T11:36:02.000Z</published>
    <updated>2018-12-01T09:01:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文是在学习Andrew Ng所教授的Machine learning课程过程中所记录的笔记。因为个人知识的不足以及英文教学，难免会有理解偏差的地方，欢迎一起交流。</p><p>课程资料：<a href="https://www.coursera.org/learn/machine-learning" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning</a></p><p>Machine learning 主要分为两类：</p><p><strong>Supervised Learning</strong>：regression problem、classification problem</p><p>例子：房价估计，良性恶性肿瘤判断</p><p>supervised learning:”right answers” given</p><ul><li>regression: predict continuous valued output(price)</li><li>classification: discrete valued output(0 or 1)</li></ul><p><strong>Unsupervised Learning</strong>：clustering algorithm</p><p>例子：谷歌新闻，基因，organize computing clusters，social network analysis，market segmentation，astronomical data analysis，cocktail party problem（录音分辨）</p><a id="more"></a><p>编程作业工具：octave\matlab</p><hr><p>第一周测验，这道题总是错</p><p>A computer program is said to learn from experience E with respect to some task T and some performance measure P if its performance on T, as measured by P, improves with experience E. Suppose we feed a learning algorithm a lot of historical weather data, and have it learn to predict weather. What would be a reasonable choice for P?</p><p>这是这节课wiki的地址： <a href="https://share.coursera.org/wiki/index.php/ML:Main" target="_blank" rel="noopener">https://share.coursera.org/wiki/index.php/ML:Main</a>.</p><p>我在这上面找到了答案</p><p>Two definitions of Machine Learning are offered. Arthur Samuel described it as: “the field of study that gives computers the ability to learn without being explicitly programmed.” This is an older, informal definition.</p><p>Tom Mitchell provides a more modern definition: “A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.”</p><p>Example: playing checkers.</p><ul><li>E = the experience of playing many games of checkers</li><li>T = the task of playing checkers.</li><li>P = the probability that the program will win the next game.</li></ul><hr><h2 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h2><p>regression problem：<strong>用training set来进行训练</strong>。</p><p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170320/ml1-1.png" alt=""></p><p><strong>Linear regression model：</strong></p><p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170320/ml1-2.png" alt=""></p><p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170320/ml1-3.png" alt=""></p><p>求h的思路：idea</p><p><strong>J函数被称为cost function，用于确定h中的θ</strong></p><p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170320/ml1-4.png" alt=""></p><p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170320/ml1-5.png" alt=""></p><p><strong>h与J的关系，只有θ1</strong></p><p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170320/ml1-6.png" alt=""></p><p><strong>h与J的关系，有θ0和θ1</strong>，右边的叫做contour plot等高线图</p><p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170320/ml1-7.png" alt=""></p><h2 id="An-algorithm-gradient-descent-梯度下降"><a href="#An-algorithm-gradient-descent-梯度下降" class="headerlink" title="An algorithm: gradient descent 梯度下降"></a>An algorithm: gradient descent 梯度下降</h2><p>For minimizing the cost function J.</p><p>gradient descent不止用于最小化J函数，它是一个非常通用的算法。</p><p>可以把这个算法想象成你在一个山上，要下山，要走下山最快的路</p><p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170320/ml1-8.png" alt=""></p><h2 id="Multivariate-linear-regression多元线性回归-linear-regression-with-multiple-variables"><a href="#Multivariate-linear-regression多元线性回归-linear-regression-with-multiple-variables" class="headerlink" title="Multivariate linear regression多元线性回归(linear regression with multiple variables)"></a>Multivariate linear regression多元线性回归(linear regression with multiple variables)</h2><p>此时有多个变量，而不只有面积着一个变量。</p><p>注意x上标和下标的意思。上标：第几个training example；下标：第几个变量</p><p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170320/ml1-9.png" alt=""></p><p><strong>此时的hypothesis</strong>：<img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170320/ml1-10.png" alt=""></p><p>对其进行线性代数化简</p><p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170320/ml1-11.png" alt=""></p><p><strong>多元变量下的gradient descent</strong></p><p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170320/ml1-12.png" alt=""></p><p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170320/ml1-13.png" alt=""></p><h2 id="梯度下降运算中的使用技巧"><a href="#梯度下降运算中的使用技巧" class="headerlink" title="梯度下降运算中的使用技巧"></a>梯度下降运算中的使用技巧</h2><h5 id="1）feature-scaling-特征缩放"><a href="#1）feature-scaling-特征缩放" class="headerlink" title="1）feature scaling 特征缩放"></a>1）feature scaling 特征缩放</h5><p>如果不同变量（feature，就是那些参数）的取值范围差的很多，（假如只有两个feature）那么画出来的等高线图会特别椭圆，这个时候gradient descent下降的会很慢，θ收敛的很慢。下面为比较</p><p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170320/ml1-14.png" alt=""></p><p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170320/ml1-15.png" alt=""></p><p><strong>特征缩放一般有两种方式</strong></p><ul><li>一种是除以范围绝对值的最大值，使其范围在－1到＋1之间。（不用严格满足，比如－3 to 3或者－0.3 to 0.3都可以，只要差的不是太多，并且所有feature的范围接近即可）</li></ul><p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170320/ml1-16.png" alt=""></p><ul><li>另一种方法叫做mean normalization均值归一化</li></ul><p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170320/ml1-17.png" alt=""></p><p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170320/ml1-18.png" alt=""></p><h5 id="2）learning-rate－α的选择"><a href="#2）learning-rate－α的选择" class="headerlink" title="2）learning rate－α的选择"></a>2）learning rate－α的选择</h5><p>对于gradient descent：</p><ul><li>“debugging”:how to make sure gradient descent is working correctly.</li><li>how to choose learning rate α</li></ul><p>对于每一次迭代，J函数都应该下降</p><p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170320/ml1-19.png" alt=""></p><p>可以用自动收敛测试，比如每次J函数下降小于1e-3，但是不推荐，因为选阀值很难。</p><p>以下这些情况都要选择更小的α</p><p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170320/ml1-20.png" alt=""></p><p><strong>总结：α太小，收敛慢；α太大，J不收敛</strong></p><p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170320/ml1-21.png" alt=""></p><p>吴恩达选择α的方式：三倍一选</p><p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170320/ml1-22.png" alt=""></p><h2 id="features-and-polynomial-regression"><a href="#features-and-polynomial-regression" class="headerlink" title="features and polynomial regression"></a>features and polynomial regression</h2><p>合理选择feature（变量）能有效降低假设的复杂度。比如有两个feature房屋的长度和宽度，可以组合成一个feature面积。</p><p>有时候也可以选择多项式回归，使得model更适合data。</p><p>至于怎么选择后面的课会讲。</p><h2 id="normal-equation："><a href="#normal-equation：" class="headerlink" title="normal equation："></a>normal equation：</h2><p>目前线性回归的算法有</p><ol><li><strong>梯度下降法</strong>，多次迭代逐渐收敛到J函数的全局最小值。</li><li><strong>normal equation</strong>，method to solve for θ analytically。解析解法，一次求出θ最优值</li></ol><h2 id="向量化："><a href="#向量化：" class="headerlink" title="向量化："></a>向量化：</h2><p>编程的时候向量化，可以让运算速度更快，效率更高</p><p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170320/ml1-23.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是在学习Andrew Ng所教授的Machine learning课程过程中所记录的笔记。因为个人知识的不足以及英文教学，难免会有理解偏差的地方，欢迎一起交流。&lt;/p&gt;
&lt;p&gt;课程资料：&lt;a href=&quot;https://www.coursera.org/learn/machine-learning&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/machine-learning&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Machine learning 主要分为两类：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Supervised Learning&lt;/strong&gt;：regression problem、classification problem&lt;/p&gt;
&lt;p&gt;例子：房价估计，良性恶性肿瘤判断&lt;/p&gt;
&lt;p&gt;supervised learning:”right answers” given&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;regression: predict continuous valued output(price)&lt;/li&gt;
&lt;li&gt;classification: discrete valued output(0 or 1)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Unsupervised Learning&lt;/strong&gt;：clustering algorithm&lt;/p&gt;
&lt;p&gt;例子：谷歌新闻，基因，organize computing clusters，social network analysis，market segmentation，astronomical data analysis，cocktail party problem（录音分辨）&lt;/p&gt;
    
    </summary>
    
      <category term="笔记" scheme="https://bluesmilery.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Machine Learning" scheme="https://bluesmilery.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>增强学习 Reinforcement learning part 1 - Introduction</title>
    <link href="https://bluesmilery.github.io/blogs/481fe3af/"/>
    <id>https://bluesmilery.github.io/blogs/481fe3af/</id>
    <published>2017-03-20T09:24:02.000Z</published>
    <updated>2018-01-31T03:44:45.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文是在学习David Silver所教授的Reinforcement learning课程过程中所记录的笔记。因为个人知识的不足以及全程啃生肉，难免会有理解偏差的地方，欢迎一起交流。</p><p>课程资料：<a href="http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html" target="_blank" rel="noopener">http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html</a></p><p>关于 Reinforcement learning的两本参考：</p><p>An Introduction to Reinforcement Learning</p><p><a href="https://webdocs.cs.ualberta.ca/~sutton/book/the-book-1st.html" target="_blank" rel="noopener">https://webdocs.cs.ualberta.ca/~sutton/book/the-book-1st.html</a></p><p>Algorithms for Reinforcement Learning</p><p><a href="https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf" target="_blank" rel="noopener">https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf</a></p><a id="more"></a><h2 id="1、About-Reinforcement-Learning"><a href="#1、About-Reinforcement-Learning" class="headerlink" title="1、About Reinforcement Learning"></a>1、About Reinforcement Learning</h2><p>Many Faces of Reinforcement Learning</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/Hcm8Hgd1m0.jpg" alt=""></p><p>Machine Learningd的三个分支：Supervised Learning、Unsupervised Learning、Reinforcement Learning</p><p>RL与其他两种的区别：</p><ul><li>There is no supervisor, only <strong>a reward signal</strong></li><li><strong>Feedback is delayed</strong>, not instantaneous</li><li><strong>Time</strong> really matters (sequential, non i.i.d data)</li><li>Agent’s <strong>actions affect the subsequent data</strong> it receives</li></ul><h2 id="2、The-Reinforcement-Learning-Problem"><a href="#2、The-Reinforcement-Learning-Problem" class="headerlink" title="2、The Reinforcement Learning Problem"></a>2、The Reinforcement Learning Problem</h2><p>介绍三个概念：reward、environment、state</p><p>==<strong>reward</strong>==</p><p>用Rt来表示reward（标量，就是个’数’），衡量在第t步agent表现的好坏（收益），agent的目标就是最大化累计reward</p><p>Reinforcement learning is based on the <strong>reward hypothesis</strong>（All goals can be described by the maximisation of expected cumulative reward）</p><p>简而言之就是假设所有的目标都可以用最大化累计收益来表示</p><p>example：</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/B0amGIICH0.png" alt=""></p><p>Sequential Decision Making</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/8dk3DCkdf9.png" alt=""></p><p>==<strong>environment</strong>==</p><p>agent与environment的关系，agent执行action影响environment，environment给agent关于observation和reward的反馈</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/7aI7kG3Did.jpg" alt=""></p><p>==<strong>state</strong>==</p><p><em>history</em>：在第t步之前的observation、reward、action。注意没有At，因为agent是基于observation和reward来选择action，在选择action之前的这个时间点，在此之前的都算是过去</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/336jfc5JGh.png" alt=""></p><p><em>state</em>：is the information used to determine what happens next。就是说我利用了history中某些信息来判断接下来会发生什么，所利用的这些信息就被称为state</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/bJA5FC9L2h.png" alt=""></p><p>这里what happens next分为两部分</p><ul><li>The agent selects actions。agent会选择什么action</li><li>The environment selects observations/rewards。environment会给出什么observation/reward</li></ul><p>state又分为environment state、agent state、information state</p><p><em>environment state</em>：Ste is the environment’s private representation。对于agent而言一般是invisible的，就算是visible，那也包含不相关的信息</p><p><em>agent state</em>：Sta is the agent’s internal representation。It can be any function of history</p><p><em>information state</em>：又被称为markov state。所以具有’The future is independent of the past given the present’</p><p>The environment state and the history are Markov</p><p>例子：每一行为一次过程，第一次灯亮灯亮，老鼠按下开关，然后铃响，结果老鼠遭到电击。第二次先铃响灯亮，然后老鼠两次按下开关，结果老鼠得到奶酪。第三次老鼠先按下开关，然后灯亮，然后老鼠又按下开关，然后铃响，猜测老鼠会得到什么？</p><p>分析：如果agent state是利用最后三个动作的顺序，那么老鼠会遭到电击。如果agent state是利用灯亮铃响按下开关的次数，那么老鼠会得到奶酪。如果agent state是利用整个序列，那我们也不知道会发生什么</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/mfafgjBcBd.jpg" alt=""></p><p><em>Fully Observable Environments</em>：agent <strong>directly</strong> observes environment state。这种被称为<strong>Markov decision process</strong> (MDP)</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/kDKG917gjc.png" alt=""></p><p>agent state = environment state = information state</p><p><em>Partially Observable Environments</em>：agent <strong>indirectly</strong> observes environment。这种被称为<strong>partially observable Markov decision process</strong>（POMDP）</p><ul><li>A robot with camera vision isn’t told its absolute location</li><li>A trading agent only observes current prices</li><li>A poker playing agent only observes public cards</li></ul><p>agent state 不等于 environment state</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/ejAcm539ga.png" alt=""></p><h2 id="3、Inside-An-Reinforcement-Learning-Agent"><a href="#3、Inside-An-Reinforcement-Learning-Agent" class="headerlink" title="3、Inside An Reinforcement Learning Agent"></a>3、Inside An Reinforcement Learning Agent</h2><p>agent的三要素</p><ul><li>Policy：agent采取的行为策略（behaviour function）</li><li>Value Function：评估state/action的好坏</li><li>Model：agent对environment所构建的模型（在agent眼中environment的样子）</li></ul><p>==<strong>Policy</strong>==：agent的策略，也就是agent在某个状态会采取什么样的行动，所以policy is a map from state to action</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/3bcFHGCgCL.png" alt=""></p><p>==<strong>Value Function</strong>==：是对未来收益的一个预测，用来评估状态的好坏程度</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/Cj20H41d1d.png" alt=""></p><p>其中gamma是discounted系数，表示了未来的收益对现在的影响，越远的影响越小。比如gamma是0.9，那么这个预测的时间跨度大约是未来三四十步</p><p>example：左上角那个是state value function，游戏画面上有一个紫色的，那个是mothership，击落的分数奖励更高，所以当mothership从右边出现后，对未来收益的预测增加，从而value function的值开始上升。当mothership从眼前过去后，不管打没打中，value function都会陡然下降，因为后面都是小兵，所以对未来收益的预期也就回到了一般水平。</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/3F4CBI33g8.jpg" alt=""></p><p>还有个打砖块的例子，越靠上面的砖块分数越高，所以在游戏刚开始的时候value function比较平滑，当下面的打了好多以后，打到更深的砖块的概率上升，所以value function的波动增加了。</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/hgbl3iHKeh.jpg" alt=""></p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/aLcfGI54cc.jpg" alt=""></p><p>==<strong>Model</strong>==：agent对environment构建的模型，用来预测environment下一步会干什么（会跳转到哪个state，会给出什么reward）</p><p>P predicts the next state</p><p>R predicts the next (immediate) reward</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/DkLL2HD7Ed.png" alt=""></p><p>基于上面三要素，RL agent有以下几种分类</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/kcife76K2j.jpg" alt=""></p><h2 id="4、Problems-within-Reinforcement-Learning"><a href="#4、Problems-within-Reinforcement-Learning" class="headerlink" title="4、Problems within Reinforcement Learning"></a>4、Problems within Reinforcement Learning</h2><p>==<strong>Learning and Planning</strong>==</p><p>Two fundamental problems in sequential decision making</p><ul><li>Reinforcement Learning:<ul><li><strong>The environment is initially unknown</strong></li><li>The agent interacts with the environment</li><li>The agent improves its policy</li></ul></li><li>Planning:<ul><li><strong>A model of the environment is known</strong></li><li>The agent performs computations with its model (without any external interaction)</li><li>The agent improves its policy</li><li>a.k.a. deliberation, reasoning, introspection, pondering, thought, search</li></ul></li></ul><p>Reinforcement Learning的例子：游戏的机制不清楚，只能通过玩来学习，通过观察得分与游戏画面来选择下一步行动</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/9e9a5184Em.jpg" alt=""></p><p>Planning的例子：游戏机制很清楚，下一步是什么样子的都知道，有完整的策略（就像是玩游戏有攻略一样）</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/KK1aE0KC7f.png" alt=""></p><p>==<strong>Exploration and Exploitation</strong>==</p><p>Reinforcement learning is like trial-and-error learning</p><ul><li>Exploration：探索，更多的去探索environment的信息</li><li>Exploitation：利用，更多的利用已知的environment信息来最大化reward</li></ul><p>举个例子，吃饭选择餐厅，exploration是选择一个新餐厅，exploitation是选择自己平时最喜欢吃的餐厅</p><p>==<strong>Prediction and Control</strong>==</p><ul><li>Prediction：估计未来的收益，given a policy</li><li>Control：最优化未来的收益，find the best policy</li></ul><p>Gridworld Example，没看懂</p><p>后记：有了一些理解，如果移动到A的话那么就会跳转到A’，并且reward +10，如果移动到B的话那么就会跳转到B’，并且reward + 5</p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/cBJiHjmgfg.png" alt=""></p><p><img src="http://p3awn0zgi.bkt.clouddn.com/blog/180130/HJ2bcbljlB.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是在学习David Silver所教授的Reinforcement learning课程过程中所记录的笔记。因为个人知识的不足以及全程啃生肉，难免会有理解偏差的地方，欢迎一起交流。&lt;/p&gt;
&lt;p&gt;课程资料：&lt;a href=&quot;http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;关于 Reinforcement learning的两本参考：&lt;/p&gt;
&lt;p&gt;An Introduction to Reinforcement Learning&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://webdocs.cs.ualberta.ca/~sutton/book/the-book-1st.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://webdocs.cs.ualberta.ca/~sutton/book/the-book-1st.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Algorithms for Reinforcement Learning&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="笔记" scheme="https://bluesmilery.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement Learning" scheme="https://bluesmilery.github.io/tags/Reinforcement-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习系统环境配置指南 —— GTX 1080 + Ubuntu16.04 + CUDA8 + cuDNN5.1 + TensorFlow</title>
    <link href="https://bluesmilery.github.io/blogs/9a018dfc/"/>
    <id>https://bluesmilery.github.io/blogs/9a018dfc/</id>
    <published>2017-03-15T20:51:45.000Z</published>
    <updated>2018-12-01T07:35:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近开始学习机器学习，所以需要配一台电脑。本文主要写的是系统环境配置的内容，依据前人经验总结自己的安装过程，希望可以给大家一个参考。</p><p>主机配置：i7-6700 + 24G内存 + GTX 1080</p><p>系统环境配置：</p><ul><li>Ubuntu 16.04 LTS 64位</li><li>CUDA 8.0</li><li>cuDNN v5.1</li><li>TensorFlow v0.12.0 RC1</li><li>Python 2.7</li><li>Bazel 0.4.2</li></ul><p>在整个环境配置过程中，有许多东西可以提前下载好，在配置时便可以节省时间了。</p><ul><li><a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="noopener">CUDA 8.0</a> (1.4GB)：Linux &gt; x86_64 &gt; Ubuntu &gt; 16.04 &gt; runfile(local)</li><li><a href="https://developer.nvidia.com/rdp/cudnn-download" target="_blank" rel="noopener">cuDNN v5.1</a> (100MB)： 需要注册Nvidia开发者账号，Download cuDNN v5.1 (August 10, 2016), for CUDA 8.0 &gt; cuDNN v5.1 Library for Linux。最好在Linux系统下下载，格式为.tgz。在Windows下下载的格式会识别成.solitairetheme8格式。</li><li><a href="https://github.com/tensorflow/tensorflow/releases" target="_blank" rel="noopener">TensorFlow 源码release版</a> (10MB+)：下载v0.12.0 RC1，zip或者tar.gz均可</li><li><a href="https://github.com/tensorflow/tensorflow" target="_blank" rel="noopener">TensorFlow pip安装包</a> (CPU版40MB+，GPU版80MB+)：选择Linux和Python2的版本，CPU和GPU的都下。pip安装包只会下载最新版本</li><li><a href="https://github.com/bazelbuild/bazel/releases" target="_blank" rel="noopener">Bazel 源码</a> (100MB+)：下载0.4.2版本，选择bazel-0.4.2-installer-linux-x86_64.sh</li></ul><a id="more"></a><h2 id="一、安装Nvidia显卡驱动（GTX-1080）"><a href="#一、安装Nvidia显卡驱动（GTX-1080）" class="headerlink" title="一、安装Nvidia显卡驱动（GTX 1080）"></a>一、安装Nvidia显卡驱动（GTX 1080）</h2><p>1、安装完Ubuntu16.04系统后，第一次进入系统分辨率很低，所以先简单修改一下分辨率。</p><p>打开terminal，执行：<code>sudo gedit /etc/default/grub</code></p><p>有一行内容是 #GRUB_GFXMODE=640x480，然后把#号去掉，后面的640x480改为1024x768。</p><p>然后执行：<code>sudo update-grub</code></p><p>重启电脑后，看起来比刚才舒服一些了</p><p>2、更新软件源，这里用的是中科大的源</p><p><code>cd /etc/apt/</code></p><p><code>sudo cp sources.list sources.list.bak</code></p><p><code>sudo gedit sources.list</code></p><p>把下面的内容添加到sources.list文件头部：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">deb http://mirrors.ustc.edu.cn/ubuntu/ xenial main restricted universe multiverse</span><br><span class="line">deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-security main restricted universe multiverse</span><br><span class="line">deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse</span><br><span class="line">deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse</span><br><span class="line">deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial-security main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse</span><br></pre></td></tr></table></figure><p>然后更新源和更新已安装的包</p><p><code>sudo apt-get update</code></p><p><code>sudo apt-get upgrade</code></p><p>3、安装Nvidia显卡驱动</p><p>先添加Ubuntu社区建立的Graphics Drivers PPA</p><p><code>sudo add-apt-repository ppa:graphics-drivers/ppa</code></p><p>出现一系列内容后，回车后继续</p><p><code>sudo apt-get update</code></p><p>此时，有两种方式可以安装驱动</p><p>1）第一种是进入System Settings &gt; Software&amp;Updates &gt; Additional Drivers，然后选择想安装的驱动，然后点apply，等待系统提醒重启系统即可</p><p>2）第二种是看一下软件源中有哪些Nvidia驱动</p><p><code>sudo apt-cache search nvidia</code></p><p>我选择安装最新的驱动版本375</p><p><code>sudo apt-get install nvidia-375</code></p><p>重启电脑后驱动生效</p><p>可以执行：<code>nvidia-smi</code>来查看信息，或者执行：<code>nvidia-settings</code>查看更详细的信息</p><h2 id="二、安装CUDA8-0"><a href="#二、安装CUDA8-0" class="headerlink" title="二、安装CUDA8.0"></a>二、安装CUDA8.0</h2><p>1、先去Nvidia开发者网站下载<a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="noopener">CUDA8.0</a><br><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170315/cuda.png" alt=""></p><p>下载好后，执行：<code>sudo sh cuda_8.0.44_linux.run</code>进行安装。如果出现了提示空间不足，那么执行：<code>sudo sh cuda_8.0.27_linux.run --tmpdir=/opt/temp/</code></p><p>执行后会有一系列提示需要确认，其中，询问是否安装Nvidia显卡驱动的时候选n，因为我们之前已经装过了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">Do you accept the previously read EULA?</span><br><span class="line">accept/decline/quit:               accept</span><br><span class="line"></span><br><span class="line">Install NVIDIA Accelerated Graphics Driver for Linux-x86_64 367.48?</span><br><span class="line">(y)es/(n)o/(q)uit: n</span><br><span class="line"></span><br><span class="line">Install the CUDA 8.0 Toolkit?</span><br><span class="line">(y)es/(n)o/(q)uit: y</span><br><span class="line"></span><br><span class="line">Enter Toolkit Location</span><br><span class="line"> [ default is /usr/local/cuda-8.0 ]:</span><br><span class="line"></span><br><span class="line">Do you want to install a symbolic link at /usr/local/cuda?</span><br><span class="line">(y)es/(n)o/(q)uit: y</span><br><span class="line"></span><br><span class="line">Install the CUDA 8.0 Samples?</span><br><span class="line">(y)es/(n)o/(q)uit: y</span><br><span class="line"></span><br><span class="line">Enter CUDA Samples Location</span><br><span class="line"> [ default is /home/gai ]:</span><br><span class="line"></span><br><span class="line">Installing the CUDA Toolkit in /usr/local/cuda-8.0 ...</span><br><span class="line">Missing recommended library: libGLU.so</span><br><span class="line">Missing recommended library: libX11.so</span><br><span class="line">Missing recommended library: libXi.so</span><br><span class="line">Missing recommended library: libXmu.so</span><br><span class="line"></span><br><span class="line">Installing the CUDA Samples in /home/gai ...</span><br><span class="line">Copying samples to /home/gai/NVIDIA_CUDA-8.0_Samples now...</span><br><span class="line">Finished copying samples.</span><br><span class="line"></span><br><span class="line">===========</span><br><span class="line">= Summary =</span><br><span class="line">===========</span><br><span class="line"></span><br><span class="line">Driver:   Not Selected</span><br><span class="line">Toolkit:  Installed in /usr/local/cuda-8.0</span><br><span class="line">Samples:  Installed in /home/gai, but missing recommended libraries</span><br><span class="line"></span><br><span class="line">Please make sure that</span><br><span class="line"> -   PATH includes /usr/local/cuda-8.0/bin</span><br><span class="line"> -   LD_LIBRARY_PATH includes /usr/local/cuda-8.0/lib64, or, add /usr/local/cuda-8.0/lib64 to /etc/ld.so.conf and run ldconfig as root</span><br><span class="line"></span><br><span class="line">To uninstall the CUDA Toolkit, run the uninstall script in /usr/local/cuda-8.0/bin</span><br><span class="line"></span><br><span class="line">Please see CUDA_Installation_Guide_Linux.pdf in /usr/local/cuda-8.0/doc/pdf for detailed information on setting up CUDA.</span><br><span class="line"></span><br><span class="line">***WARNING: Incomplete installation! This installation did not install the CUDA Driver. A driver of version at least 361.00 is required for CUDA 8.0 functionality to work.</span><br><span class="line">To install the driver using this installer, run the following command, replacing &lt;CudaInstaller&gt; with the name of this run file:</span><br><span class="line">    sudo &lt;CudaInstaller&gt;.run -silent -driver</span><br><span class="line"></span><br><span class="line">Logfile is /tmp/cuda_install_6100.log</span><br></pre></td></tr></table></figure><p>可以发现系统提示缺少一些推荐安装的库：libGLU.so、libX11.so、libXi.so、libXmu.so</p><p>使用<a href="http://packages.ubuntu.com/" target="_blank" rel="noopener">Ubuntu Packages Search</a>进行搜索，在Search the contents of packages处分别键入以上四个库，可以发现需要安装以下软件包：libglu1-mesa-dev、libx11-dev、libxi-dev、libxmu-dev</p><p>所以执行：<code>sudo apt-get install libglu1-mesa-dev libx11-dev libxi-dev libxmu-dev</code></p><p>再参考下<a href="http://docs.nvidia.com/cuda/cuda-installation-guide-linux/#mandatory-post" target="_blank" rel="noopener">官方的安装指南</a>，发现还要配置环境变量，在home目录下执行：<code>sudo gedit .bashrc</code>，然后在文件末尾添加上下面两行内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export PATH=/usr/local/cuda-8.0/bin$&#123;PATH:+:$&#123;PATH&#125;&#125;</span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;</span><br></pre></td></tr></table></figure><p>然后执行：<code>source ~/.bashrc</code>更新一下</p><p>2、测试CUDA</p><p>测试几个官方CUDA的例子</p><p>在CUDA例子的1_Utilities/deviceQuery目录下执行：<code>make</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&quot;/usr/local/cuda-8.0&quot;/bin/nvcc -ccbin g++ -I../../common/inc  -m64    -gencode arch=compute_20,code=sm_20 -gencode arch=compute_30,code=sm_30 -gencode arch=compute_35,code=sm_35 -gencode arch=compute_37,code=sm_37 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_60,code=compute_60 -o deviceQuery.o -c deviceQuery.cpp</span><br><span class="line">nvcc warning : The &apos;compute_20&apos;, &apos;sm_20&apos;, and &apos;sm_21&apos; architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).</span><br><span class="line">&quot;/usr/local/cuda-8.0&quot;/bin/nvcc -ccbin g++   -m64      -gencode arch=compute_20,code=sm_20 -gencode arch=compute_30,code=sm_30 -gencode arch=compute_35,code=sm_35 -gencode arch=compute_37,code=sm_37 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_60,code=compute_60 -o deviceQuery deviceQuery.o</span><br><span class="line">nvcc warning : The &apos;compute_20&apos;, &apos;sm_20&apos;, and &apos;sm_21&apos; architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).</span><br><span class="line">mkdir -p ../../bin/x86_64/linux/release</span><br><span class="line">cp deviceQuery ../../bin/x86_64/linux/release</span><br></pre></td></tr></table></figure><p>编译完成，然后执行：<code>./deviceQuery</code>，会得到如下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">./deviceQuery Starting...</span><br><span class="line"></span><br><span class="line"> CUDA Device Query (Runtime API) version (CUDART static linking)</span><br><span class="line"></span><br><span class="line">Detected 1 CUDA Capable device(s)</span><br><span class="line"></span><br><span class="line">Device 0: &quot;GeForce GTX 1080&quot;</span><br><span class="line">  CUDA Driver Version / Runtime Version          8.0 / 8.0</span><br><span class="line">  CUDA Capability Major/Minor version number:    6.1</span><br><span class="line">  Total amount of global memory:                 8110 MBytes (8504279040 bytes)</span><br><span class="line">  (20) Multiprocessors, (128) CUDA Cores/MP:     2560 CUDA Cores</span><br><span class="line">  GPU Max Clock rate:                            1810 MHz (1.81 GHz)</span><br><span class="line">  Memory Clock rate:                             5005 Mhz</span><br><span class="line">  Memory Bus Width:                              256-bit</span><br><span class="line">  L2 Cache Size:                                 2097152 bytes</span><br><span class="line">  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)</span><br><span class="line">  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers</span><br><span class="line">  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers</span><br><span class="line">  Total amount of constant memory:               65536 bytes</span><br><span class="line">  Total amount of shared memory per block:       49152 bytes</span><br><span class="line">  Total number of registers available per block: 65536</span><br><span class="line">  Warp size:                                     32</span><br><span class="line">  Maximum number of threads per multiprocessor:  2048</span><br><span class="line">  Maximum number of threads per block:           1024</span><br><span class="line">  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)</span><br><span class="line">  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)</span><br><span class="line">  Maximum memory pitch:                          2147483647 bytes</span><br><span class="line">  Texture alignment:                             512 bytes</span><br><span class="line">  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)</span><br><span class="line">  Run time limit on kernels:                     Yes</span><br><span class="line">  Integrated GPU sharing Host Memory:            No</span><br><span class="line">  Support host page-locked memory mapping:       Yes</span><br><span class="line">  Alignment requirement for Surfaces:            Yes</span><br><span class="line">  Device has ECC support:                        Disabled</span><br><span class="line">  Device supports Unified Addressing (UVA):      Yes</span><br><span class="line">  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0</span><br><span class="line">  Compute Mode:</span><br><span class="line">     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;</span><br><span class="line"></span><br><span class="line">deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 8.0, CUDA Runtime Version = 8.0, NumDevs = 1, Device0 = GeForce GTX 1080</span><br><span class="line">Result = PASS</span><br></pre></td></tr></table></figure><p>再测试另外一个例子，在5_Simulations/nbody目录下执行<code>make</code>，如果提示cannot find -lglut，那么需要执行下：<code>sudo apt-get install freeglut3-dev</code></p><p>编译完成后执行：<code>./nbody -benchmark -numbodies=256000 -device=0</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">Run &quot;nbody -benchmark [-numbodies=&lt;numBodies&gt;]&quot; to measure performance.</span><br><span class="line">    -fullscreen       (run n-body simulation in fullscreen mode)</span><br><span class="line">    -fp64             (use double precision floating point values for simulation)</span><br><span class="line">    -hostmem          (stores simulation data in host memory)</span><br><span class="line">    -benchmark        (run benchmark to measure performance)</span><br><span class="line">    -numbodies=&lt;N&gt;    (number of bodies (&gt;= 1) to run in simulation)</span><br><span class="line">    -device=&lt;d&gt;       (where d=0,1,2.... for the CUDA device to use)</span><br><span class="line">    -numdevices=&lt;i&gt;   (where i=(number of CUDA devices &gt; 0) to use for simulation)</span><br><span class="line">    -compare          (compares simulation results running once on the default GPU and once on the CPU)</span><br><span class="line">    -cpu              (run n-body simulation on the CPU)</span><br><span class="line">    -tipsy=&lt;file.bin&gt; (load a tipsy model file for simulation)</span><br><span class="line"></span><br><span class="line">NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.</span><br><span class="line"></span><br><span class="line">&gt; Windowed mode</span><br><span class="line">&gt; Simulation data stored in video memory</span><br><span class="line">&gt; Single precision floating point simulation</span><br><span class="line">&gt; 1 Devices used for simulation</span><br><span class="line">gpuDeviceInit() CUDA Device [0]: &quot;GeForce GTX 1080</span><br><span class="line">&gt; Compute 6.1 CUDA device: [GeForce GTX 1080]</span><br><span class="line">number of bodies = 256000</span><br><span class="line">256000 bodies, total time for 10 iterations: 2395.682 ms</span><br><span class="line">= 273.559 billion interactions per second</span><br><span class="line">= 5471.177 single-precision GFLOP/s at 20 flops per interaction</span><br></pre></td></tr></table></figure><p>至此CUDA8.0安装完成。</p><h2 id="三、安装cuDNN-v5-1"><a href="#三、安装cuDNN-v5-1" class="headerlink" title="三、安装cuDNN v5.1"></a>三、安装cuDNN v5.1</h2><p>先去Nvidia开发者网站下载<a href="https://developer.nvidia.com/rdp/cudnn-download" target="_blank" rel="noopener">cuDNN</a>，这个需要注册账号后才能下载。</p><p>选择Download cuDNN v5.1 (August 10, 2016), for CUDA 8.0，然后点cuDNN v5.1 Library for Linux，会下载一个.tgz的文件</p><p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170315/cudnn.png" alt=""></p><p>在文件所在目录下执行：<code>tar -zxvf cudnn-8.0-linux-x64-v5.0-ga.tgz</code></p><p>虽然官方没有提供安装说明，但是google下就能查到。执行下列命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo cp cuda/include/cudnn.h /usr/local/cuda/include/</span><br><span class="line">sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64/</span><br><span class="line">sudo chmod a+r /usr/local/cuda/include/cudnn.h</span><br><span class="line">sudo chmod a+r /usr/local/cuda/lib64/libcudnn*</span><br></pre></td></tr></table></figure><p>这样cuDNN v5.1就安装完成了。</p><h2 id="四、安装TensorFlow"><a href="#四、安装TensorFlow" class="headerlink" title="四、安装TensorFlow"></a>四、安装TensorFlow</h2><p>TensorFlow有自己的<a href="https://www.tensorflow.org/get_started/os_setup#pip-installation" target="_blank" rel="noopener">官方安装文档</a>，提供了多种安装方式，我使用过其中的两种：pip安装和源码编译安装。简单说下两种安装方式的特点，pip安装过程便捷，几句命令就搞定。源码编译安装过程复杂，并且容易遇到各种问题。所以一般我们都会选择pip安装方式。</p><p>安装过程还有些小插曲，第一次安装的时候先尝试了使用pip安装，结果安装完后TensorFlow不识别GPU，捣鼓了很久并且google各种资料安装过程都没有问题，遂放弃pip转为源码编译安装成功了。过了几天重做了系统，又要安装TensorFlow，这次抱着试一试的心态使用pip安装，结果成功了= =然而并不知道为什么，所以第二次也就没用源码安装了。</p><p>本机安装环境为Python2.7，安装的TensorFlow版本为 v0.12.0 RC1</p><h3 id="1、pip安装"><a href="#1、pip安装" class="headerlink" title="1、pip安装"></a>1、pip安装</h3><p>首先安装pip</p><p><code>sudo apt-get install python-pip python-dev</code></p><p>如果使用的是Python3则换成pip3</p><p>安装TensorFlow</p><p><code>sudo pip install tensorflow</code></p><p>安装TensorFlow的GPU版本</p><p><code>sudo pip install tensorflow-gpu</code></p><p>OK，完成！是不是超简单= =哈哈，下面说下注意事项</p><p>如果网速不好的话可以在GitHub上先下载好<a href="https://github.com/tensorflow/tensorflow" target="_blank" rel="noopener">TensorFlow的安装包</a>，然后进行本地安装，当然还是会联网下一些依赖包，但是都比较小<br>在安装包目录下执行</p><p><code>sudo pip install tensorflow-0.12.0rc1-cp27-none-linux_x86_64.whl</code></p><p><code>sudo pip install tensorflow_gpu-0.12.0rc1-cp27-none-linux_x86_64.whl</code></p><p>install后面那部分就是下载的安装包的文件名</p><h3 id="2、源码编译安装"><a href="#2、源码编译安装" class="headerlink" title="2、源码编译安装"></a>2、源码编译安装</h3><p>首先先安装相关依赖包</p><p><code>sudo apt-get install python-pip</code></p><p><code>sudo apt-get install python-numpy swig python-dev python-wheel</code></p><p>在本地编译TensorFlow源码的话需要使用Google开源的一个构建工具——Bazel，也有<a href="http://www.bazel.io/versions/master/docs/install.html" target="_blank" rel="noopener">官方安装文档</a></p><p>首先需要下载Bazel，推荐<a href="https://github.com/bazelbuild/bazel/releases" target="_blank" rel="noopener">去GitHub上下载</a>，下载对应版本的安装包installer-linux-x86_64.sh。之前下载的是0.3.0版本的，结果在配置TensorFlow的时候出了一堆问题，后来换成了0.4.2版本问题解决。所以推荐下载最新版本的</p><p>在Bazel安装包所在目录下执行<br><code>sudo chmod +x bazel-0.4.2-installer-linux-x86_64.sh</code></p><p><code>sudo ./bazel-0.4.2-installer-linux-x86_64.sh --user</code></p><p>需要注意的是Bazel需要Java环境，如果没有的话需要安装，直接使用apt-get安装即可</p><p><code>sudo apt-get update</code></p><p><code>sudo apt-get install default-jre</code></p><p><code>sudo apt-get install default-jdk</code></p><p>安装完成后再执行<code>sudo ./bazel-0.4.2-installer-linux-x86_64.sh --user</code>即可</p><p>然后在 ~/.bashrc中追加：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">source /home/gai/.bazel/bin/bazel-complete.bash</span><br><span class="line">export PATH=$PATH:/home/gai/.bazel/bin</span><br></pre></td></tr></table></figure><p>需要注意的是，把gai换成自己系统的用户名</p><p>至于为什么要追加这个内容，我在Bazel安装文档中找到以下内容</p><blockquote><p>Bazel comes with a bash completion script. To install it:</p><ul><li>Build it with Bazel: bazel build //scripts:bazel-complete.bash.</li><li>Copy the script bazel-bin/scripts/bazel-complete.bash to your completion folder (/etc/bash_completion.d directory under Ubuntu). If you don’t have a completion folder, you can copy it wherever suits you and simply insert source /path/to/bazel-complete.bash in your ~/.bashrc file (under OS X, put it in your ~/.bash_profile file).</li></ul></blockquote><p>我在使用源码编译安装的时候直接在~/.bashrc追加了那两行内容，没有做Bazel安装文档中步骤。等下次使用源码编译安装的时候再尝试下</p><p>在~/.bashrc追加完后执行<code>source ~/.bashrc</code>更新一下</p><p>至此Bazel安装完成，下一步开始编译TensorFlow</p><p>首先先下载TensorFlow源码，可以使用git 命令从GitHub上克隆下来：git clone <a href="https://github.com/tensorflow/tensorflow" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow</a> ，这是最新版，也可以自己<a href="https://github.com/tensorflow/tensorflow" target="_blank" rel="noopener">去GitHub上下载</a>release版，推荐后者</p><p>下载完成后进入TensorFlow主目录，执行：</p><p><code>./configure</code></p><p>开始配置TensorFlow，接下来会有一系列问题需要确认，我印象中会询问是否需要支持Google Cloud、Hadoop、OpenCL、CUDA等，其中CUDA是我们需要的，所以我除了CUDA选了yes以外其他的都选了no</p><p>如果配置成功的话会出现下列信息，可能不完全一样，但至少会有Configuration finished</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">INFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.</span><br><span class="line">.....</span><br><span class="line">____Loading package: tensorflow/contrib/util</span><br><span class="line">____Loading package: tensorflow/tools/test</span><br><span class="line">____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 97,938 bytes</span><br><span class="line">____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 451,148 bytes</span><br><span class="line">____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 802,540 bytes</span><br><span class="line">____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 1,317,340 bytes</span><br><span class="line">____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 2,055,608 bytes</span><br><span class="line">____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 2,247,228 bytes</span><br><span class="line">____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 2,328,350 bytes</span><br><span class="line">____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 2,457,050 bytes</span><br><span class="line">____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 2,585,750 bytes</span><br><span class="line">____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 3,518,110 bytes</span><br><span class="line">____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 3,711,160 bytes</span><br><span class="line">INFO: All external dependencies fetched successfully.</span><br><span class="line">Configuration finished</span><br></pre></td></tr></table></figure><p>然后使用Bazel来编译TensorFlow，在TensorFlow源码目录下执行以下命令</p><p><code>bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer</code></p><p>编译需要一段时间，配置为i7-6700+24G内存大约耗时20分钟</p><p>编译完成后会显示以下信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Target //tensorflow/cc:tutorials_example_trainer up-to-date:</span><br><span class="line">  bazel-bin/tensorflow/cc/tutorials_example_trainer</span><br><span class="line">INFO: Elapsed time: 1196.829s, Critical Path: 986.68s</span><br></pre></td></tr></table></figure><p>执行一下TensorFlow官方提供的例子，看看能否成功调用GPU</p><p><code>bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu</code></p><p>如果看到<strong>successfully opened CUDA library、Creating TensorFlow device (/gpu:0)、显卡信息以及下面的运算过程</strong>，那说明成功调用了GPU</p><p>下面将TensorFlow源码编译成pip安装包供Python使用</p><p>编译CPU版本</p><p><code>bazel build -c opt //tensorflow/tools/pip_package:build_pip_package</code></p><p>如果要编译GPU版本的，不用执行上一句，只需执行以下命令</p><p><code>bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package</code></p><p>然后执行</p><p><code>bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg</code></p><p><code>sudo pip install /tmp/tensorflow_pkg/tensorflow-0.12.0rc1-cp27-cp27mu-linux_x86_64.whl</code></p><p>安装完成~</p><h3 id="3、测试TensorFlow"><a href="#3、测试TensorFlow" class="headerlink" title="3、测试TensorFlow"></a>3、测试TensorFlow</h3><p>下面测试下TensorFlow是否安装成功，并且是否能调用GPU</p><p>首先先配置环境变量，在home目录下执行</p><p><code>sudo gedit .bash_profile</code></p><p>然后在里面添加下面两行内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export LD_LIBRARY_PATH=&quot;$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64&quot;</span><br><span class="line">export CUDA_HOME=/usr/local/cuda</span><br></pre></td></tr></table></figure><p>然后执行：<code>source ~/.bash_profile</code>更新一下</p><p>执行下列代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">$ python</span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">I tensorflow/stream_executor/dso_loader.cc:<span class="number">128</span>] successfully opened CUDA library libcublas.so locally</span><br><span class="line">I tensorflow/stream_executor/dso_loader.cc:<span class="number">128</span>] successfully opened CUDA library libcudnn.so locally</span><br><span class="line">I tensorflow/stream_executor/dso_loader.cc:<span class="number">128</span>] successfully opened CUDA library libcufft.so locally</span><br><span class="line">I tensorflow/stream_executor/dso_loader.cc:<span class="number">128</span>] successfully opened CUDA library libcuda.so<span class="number">.1</span> locally</span><br><span class="line">I tensorflow/stream_executor/dso_loader.cc:<span class="number">128</span>] successfully opened CUDA library libcurand.so locally</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>hello = tf.constant(<span class="string">'Hello, TensorFlow!'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sess = tf.Session()</span><br><span class="line">I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:<span class="number">937</span>] successful NUMA node read <span class="keyword">from</span> SysFS had negative value (<span class="number">-1</span>), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class="line">I tensorflow/core/common_runtime/gpu/gpu_device.cc:<span class="number">885</span>] Found device <span class="number">0</span> <span class="keyword">with</span> properties: </span><br><span class="line">name: GeForce GTX <span class="number">1080</span></span><br><span class="line">major: <span class="number">6</span> minor: <span class="number">1</span> memoryClockRate (GHz) <span class="number">1.8095</span></span><br><span class="line">pciBusID <span class="number">0000</span>:<span class="number">01</span>:<span class="number">00.0</span></span><br><span class="line">Total memory: <span class="number">7.92</span>GiB</span><br><span class="line">Free memory: <span class="number">6.47</span>GiB</span><br><span class="line">I tensorflow/core/common_runtime/gpu/gpu_device.cc:<span class="number">906</span>] DMA: <span class="number">0</span> </span><br><span class="line">I tensorflow/core/common_runtime/gpu/gpu_device.cc:<span class="number">916</span>] <span class="number">0</span>: Y </span><br><span class="line">I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(sess.run(hello))</span><br><span class="line">Hello, TensorFlow!</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.constant(<span class="number">10</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = tf.constant(<span class="number">32</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(sess.run(a + b))</span><br><span class="line"><span class="number">42</span></span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure><p>首先import tensorflow没出错并且能输出<strong>Hello, TensorFlow!</strong>和<strong>42</strong>，这表明TensorFlow是可以使用的</p><p>然后看到<strong>successfully opened CUDA library</strong>和<strong>Creating TensorFlow device (/gpu:0)以及显卡信息</strong>，这表明是能够调用GPU</p><p>此外如何使用GPU以及是否使用了GPU可以<a href="https://www.tensorflow.org/how_tos/using_gpu/" target="_blank" rel="noopener">参考这篇内容</a></p><p>还可以测试一个TensorFlow的neural net model</p><p>执行：<code>python /usr/local/lib/python2.7/dist-packages/tensorflow/models/image/mnist/convolutional.py</code></p><p>第一次执行会下载一些东西然后开始执行。我的电脑执行的时间大约是40s</p><h3 id="4、注意事项"><a href="#4、注意事项" class="headerlink" title="4、注意事项"></a>4、注意事项</h3><ul><li>如果采用pip安装TensorFlow并要启用GPU支持的话，tensorflow和tensorflow-gpu都要安装，并且顺序不可以错。如果卸载了tensorflow，保留了tensorflow-gpu，此时TensorFlow是不好用的，并且在有tensorflow-gpu的情况下再装tensorflow，这时候TensorFlow是无法识别GPU的。所以如果要卸载那么把tensorflow和tensorflow-gpu都卸载，然后再按照先tensorflow后tensorflow-gpu的顺序安装。</li><li>如果采用源码编译安装TensorFlow并要启用GPU支持的话，直接编译GPU版本的pip安装包然后安装即可。</li><li>采用源码编译安装TensorFlow，在进行TensorFlow配置时，如果系统中没有安装OpenCL而在Do you wish to build TensorFlow with OpenCL support? [y/N]时又选择了y，那么会出现”Invalid SYCL 1.2 library path. /usr/local/computecpp/lib/libComputeCpp.so cannot be found “这个错误。</li><li>当初使用Bazel 0.3.0版本，进行TensorFlow配置时出现的错误如下。解决办法是使用了Bazel最新版0.4.2（在Bazel0.3.0时还使用过该命令tensorflow$ git pull –recurse-submodules，然后后来换成的0.4.2，不知是否有影响 ）。这个错误当初参考了以下信息<a href="https://github.com/tensorflow/tensorflow/issues/4365" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/issues/4365</a> 、<a href="https://github.com/tensorflow/tensorflow/issues/5357" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/issues/5357</a> 、<a href="https://github.com/tensorflow/tensorflow/issues/4319" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/issues/4319</a></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">INFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.</span><br><span class="line">.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:17:3: //external:eigen_archive: no such attribute &apos;urls&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:17:3: //external:eigen_archive: missing value for mandatory attribute &apos;url&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:28:3: //external:libxsmm_archive: no such attribute &apos;urls&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:28:3: //external:libxsmm_archive: missing value for mandatory attribute &apos;url&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:44:3: //external:com_googlesource_code_re2: no such attribute &apos;urls&apos; in &apos;http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:44:3: //external:com_googlesource_code_re2: missing value for mandatory attribute &apos;url&apos; in &apos;http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:54:3: //external:gemmlowp: no such attribute &apos;urls&apos; in &apos;http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:54:3: //external:gemmlowp: missing value for mandatory attribute &apos;url&apos; in &apos;http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:64:3: //external:farmhash_archive: no such attribute &apos;urls&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:64:3: //external:farmhash_archive: missing value for mandatory attribute &apos;url&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:80:3: //external:highwayhash: no such attribute &apos;urls&apos; in &apos;http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:80:3: //external:highwayhash: missing value for mandatory attribute &apos;url&apos; in &apos;http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:90:3: //external:nasm: no such attribute &apos;urls&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:90:3: //external:nasm: missing value for mandatory attribute &apos;url&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:101:3: //external:jpeg: no such attribute &apos;urls&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:101:3: //external:jpeg: missing value for mandatory attribute &apos;url&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:112:3: //external:png_archive: no such attribute &apos;urls&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:112:3: //external:png_archive: missing value for mandatory attribute &apos;url&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:123:3: //external:gif_archive: no such attribute &apos;urls&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:123:3: //external:gif_archive: missing value for mandatory attribute &apos;url&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:135:3: //external:six_archive: no such attribute &apos;urls&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:135:3: //external:six_archive: missing value for mandatory attribute &apos;url&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:151:3: //external:protobuf: no such attribute &apos;urls&apos; in &apos;http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:151:3: //external:protobuf: missing value for mandatory attribute &apos;url&apos; in &apos;http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:161:3: //external:gmock_archive: no such attribute &apos;urls&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:161:3: //external:gmock_archive: missing value for mandatory attribute &apos;url&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:187:3: //external:pcre: no such attribute &apos;urls&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:187:3: //external:pcre: missing value for mandatory attribute &apos;url&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:198:3: //external:swig: no such attribute &apos;urls&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:198:3: //external:swig: missing value for mandatory attribute &apos;url&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:222:3: //external:grpc: no such attribute &apos;urls&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:222:3: //external:grpc: missing value for mandatory attribute &apos;url&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:245:3: //external:linenoise: no such attribute &apos;urls&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:245:3: //external:linenoise: missing value for mandatory attribute &apos;url&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:258:3: //external:llvm: no such attribute &apos;urls&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:258:3: //external:llvm: missing value for mandatory attribute &apos;url&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:269:3: //external:jsoncpp_git: no such attribute &apos;urls&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:269:3: //external:jsoncpp_git: missing value for mandatory attribute &apos;url&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:285:3: //external:boringssl: no such attribute &apos;urls&apos; in &apos;http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:285:3: //external:boringssl: missing value for mandatory attribute &apos;url&apos; in &apos;http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:295:3: //external:nanopb_git: no such attribute &apos;urls&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:295:3: //external:nanopb_git: missing value for mandatory attribute &apos;url&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:311:3: //external:zlib_archive: no such attribute &apos;urls&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: /home/gai/tensorflow/tensorflow/workspace.bzl:311:3: //external:zlib_archive: missing value for mandatory attribute &apos;url&apos; in &apos;new_http_archive&apos; rule.</span><br><span class="line">ERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package &apos;&apos;: Encountered error while reading extension file &apos;cuda/build_defs.bzl&apos;: no such package &apos;@local_config_cuda//cuda&apos;: error loading package &apos;external&apos;: Could not load //external package.</span><br><span class="line">ERROR: missing fetch expression. Type &apos;bazel help fetch&apos; for syntax and help.</span><br></pre></td></tr></table></figure><ul><li>使用GPU执行TensorFlow时可能会出现以下内容，对实际运行没看出来有什么影响</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">E tensorflow/core/framework/op_kernel.cc:925] OpKernel (&apos;op: &quot;NegTrain&quot; device_type: &quot;CPU&quot;&apos;) for unknown op: NegTrain</span><br><span class="line">E tensorflow/core/framework/op_kernel.cc:925] OpKernel (&apos;op: &quot;Skipgram&quot; device_type: &quot;CPU&quot;&apos;) for unknown op: Skipgram</span><br></pre></td></tr></table></figure><h2 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h2><p><a href="http://www.52nlp.cn/深度学习主机环境配置-ubuntu-16-04-nvidia-gtx-1080-cuda-8" target="_blank" rel="noopener">深度学习主机环境配置: Ubuntu16.04+Nvidia GTX 1080+CUDA8.0</a></p><p><a href="http://www.52nlp.cn/深度学习主机环境配置-ubuntu16-04-geforce-gtx1080-tensorflow" target="_blank" rel="noopener">深度学习主机环境配置: Ubuntu16.04+GeForce GTX 1080+TensorFlow</a></p><p><a href="http://www.cnblogs.com/yiruparadise/p/5671620.html" target="_blank" rel="noopener">ubuntu14.04 安装 tensorflow</a></p><p><a href="https://alliseesolutions.wordpress.com/2016/09/08/install-gpu-tensorflow-from-sources-w-ubuntu-16-04-and-cuda-8-0-rc/" target="_blank" rel="noopener">Install GPU TensorFlow From Sources w/ Ubuntu 16.04 and Cuda 8.0</a></p><p><a href="http://darren1231.pixnet.net/blog/post/331300298-安裝-tensorflow-教學-gpu%3Anvidia1070(from-source" target="_blank" rel="noopener">安裝 tensorflow 教學 GPU:Nvidia1070(from source)</a>)</p><h4 id="课外小知识"><a href="#课外小知识" class="headerlink" title="课外小知识"></a>课外小知识</h4><p>为什么使用Bazel来编译TensorFlow，他们是什么关系？</p><p>Bazel是一个构建工具（构建工具：依据文件之间的依赖关系来决定文件编译的顺序），类似于Linux下的make命令。Bazel的出现是Google为了解决自己的问题：Google所有的源代码都在一个源代码仓库，而Google是一个跨国公司，世界各地的程序员都需要下载代码然后编译，所以在项目的构建过程中，性能问题是最关键的需求。</p><p>Bazel相对于其他的构建工具相比，更加强调结构化和速度。</p><p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170315/bazel.png" alt=""></p><p>在Bazel中文件编译顺序的自由度很高。构建过程可以用这样的二分图来表示。在一次构建过程中并发度越高（即二分图宽度越宽），执行时间就越短，如果机器足够多，那么一次构建的时间就主要由二分图的高度来决定。所以便可以在许多机器上执行分布式并发构建。</p><p>通过这个二分图可以看出，Bazel还具有增量构建的特点。当只有小部分源代码更改的时候，只需构建相应的部分即可。</p><p>构建行为具有函数式特点，即输入是相同的则输出也是相同的。依据这个特点，来缓存和复用构建结果。</p><p>Bazel和TensorFlow师出同门都来自于Google，在Google内部编译都使用Bazel，所以TensorFlow的编译过程自然也使用Bazel啦。</p><p>想进一步了解Bazel的同学可以看看以下内容：</p><p>Google开发Bazel的背景知识</p><p><a href="http://google-engtools.blogspot.co.uk/2011/06/build-in-cloud-accessing-source-code.html" target="_blank" rel="noopener">http://google-engtools.blogspot.co.uk/2011/06/build-in-cloud-accessing-source-code.html</a></p><p><a href="http://google-engtools.blogspot.tw/2011/08/build-in-cloud-how-build-system-works.html" target="_blank" rel="noopener">http://google-engtools.blogspot.tw/2011/08/build-in-cloud-how-build-system-works.html</a></p><p><a href="http://google-engtools.blogspot.jp/2011/09/build-in-cloud-distributing-build-steps.html" target="_blank" rel="noopener">http://google-engtools.blogspot.jp/2011/09/build-in-cloud-distributing-build-steps.html</a></p><p><a href="http://google-engtools.blogspot.tw/2011/10/build-in-cloud-distributing-build.html" target="_blank" rel="noopener">http://google-engtools.blogspot.tw/2011/10/build-in-cloud-distributing-build.html</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近开始学习机器学习，所以需要配一台电脑。本文主要写的是系统环境配置的内容，依据前人经验总结自己的安装过程，希望可以给大家一个参考。&lt;/p&gt;
&lt;p&gt;主机配置：i7-6700 + 24G内存 + GTX 1080&lt;/p&gt;
&lt;p&gt;系统环境配置：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ubuntu 16.04 LTS 64位&lt;/li&gt;
&lt;li&gt;CUDA 8.0&lt;/li&gt;
&lt;li&gt;cuDNN v5.1&lt;/li&gt;
&lt;li&gt;TensorFlow v0.12.0 RC1&lt;/li&gt;
&lt;li&gt;Python 2.7&lt;/li&gt;
&lt;li&gt;Bazel 0.4.2&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在整个环境配置过程中，有许多东西可以提前下载好，在配置时便可以节省时间了。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://developer.nvidia.com/cuda-downloads&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CUDA 8.0&lt;/a&gt; (1.4GB)：Linux &amp;gt; x86_64 &amp;gt; Ubuntu &amp;gt; 16.04 &amp;gt; runfile(local)&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://developer.nvidia.com/rdp/cudnn-download&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;cuDNN v5.1&lt;/a&gt; (100MB)： 需要注册Nvidia开发者账号，Download cuDNN v5.1 (August 10, 2016), for CUDA 8.0 &amp;gt; cuDNN v5.1 Library for Linux。最好在Linux系统下下载，格式为.tgz。在Windows下下载的格式会识别成.solitairetheme8格式。&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/tensorflow/tensorflow/releases&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;TensorFlow 源码release版&lt;/a&gt; (10MB+)：下载v0.12.0 RC1，zip或者tar.gz均可&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/tensorflow/tensorflow&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;TensorFlow pip安装包&lt;/a&gt; (CPU版40MB+，GPU版80MB+)：选择Linux和Python2的版本，CPU和GPU的都下。pip安装包只会下载最新版本&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/bazelbuild/bazel/releases&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Bazel 源码&lt;/a&gt; (100MB+)：下载0.4.2版本，选择bazel-0.4.2-installer-linux-x86_64.sh&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="技术" scheme="https://bluesmilery.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="Linux" scheme="https://bluesmilery.github.io/tags/Linux/"/>
    
      <category term="Machine Learning" scheme="https://bluesmilery.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Linux系统安装——Ubuntu16.04+Windows7双系统</title>
    <link href="https://bluesmilery.github.io/blogs/1509af3a/"/>
    <id>https://bluesmilery.github.io/blogs/1509af3a/</id>
    <published>2017-03-11T20:33:07.000Z</published>
    <updated>2018-12-01T05:47:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文安装的是 Ubuntu 16.04 LTS 64位版本，与Windows7构成双系统</p><p>1、先从<a href="https://www.ubuntu.com/download/alternative-downloads" target="_blank" rel="noopener">Ubunbu官方</a>下载系统镜像，选择64位的16.04 LTS版本。</p><p>2、制作USB安装盘。在Windows系统下， 选择<a href="https://www.ezbsystems.com/ultraiso/download.htm" target="_blank" rel="noopener">UltraISO</a>来制作USB安装盘。注意，制作过程会将U盘格式化，请提前备份好资料。<br>进入UltralISO后，选择 文件&gt;打开，选择下载好的Ubuntu镜像。<br><a id="more"></a><br><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170311/ultraliso1.png" alt=""></p><p>然后选择 启动&gt;写入硬盘映像</p><p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170311/ultraliso2.png" alt=""></p><p>然后按默认值写入即可。写入方式处可以选择默认的USB-HDD+，也可以选择USB-ZIP+。我选择的是后者。</p><p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170311/ultraliso3.png" alt=""></p><p>至此，USB启动盘就做好了。</p><p>3、重启电脑，在BIOS中设置USB启动优先。插上U盘，再开机就可以进入Ubuntu的安装界面了。<br>点击install Ubuntu后会进入该界面。这里可能会有个问题，就是点击安装后屏幕会黑屏，我的电脑显卡是GTX 1080，在一个旧显示器（1440x900）上会出现这种现象，但是换成DELL P2415Q（4K）就没有这个黑屏的问题。如果你有黑屏的问题，可以参考这两篇文章：<a href="http://blog.sciencenet.cn/blog-655584-877622.html" target="_blank" rel="noopener">安装ubuntu黑屏问题的解决</a>、<a href="http://askubuntu.com/questions/38780/how-do-i-set-nomodeset-after-ive-already-installed-ubuntu" target="_blank" rel="noopener">How do I set ‘nomodeset’ after I’ve already installed Ubuntu?</a></p><p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170311/ubuntu1.png" alt=""></p><p>这个界面上的两个选项分别是是否现在下载更新以及一些第三方驱动和软件，这里为了不耽误安装进度，可以都不勾选，等进入系统后再更新。</p><p>点击Continue后进入下一界面。</p><ul><li>如果你的电脑之前安装有其他系统，就会有第一个选项，它会在第一块硬盘上的一个空白分区里安装Ubuntu，需要注意的是它只会在第一块硬盘上选，如果你后来加了块新硬盘并且想在新硬盘上安装Ubuntu，需要选第三个选项something else。</li><li>第二个选项是会把整块硬盘格式化，如果你不需要双系统（以及不需要硬盘上原来的资料），那么选这个就可以。</li><li>第三个选项是全手动。如果想要安装双系统，或者Ubuntu的分区想自己控制的话，那么选这个。</li></ul><p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170311/ubuntu2.png" alt=""></p><p>选something else后进入下面这个页面。先简单介绍下界面内容。sd代表硬盘，第一块是sda，第二块sdb，依次类推。以第一块硬盘为例，sda1后面的数字表示分区，1234代表主分区，从5往后代表逻辑分区。可以看出我的第一块硬盘现在有四个分区，sda1是Windows自带的隐藏分区，sda2是windows系统盘，sda3是用做了Windows的数据盘，sda5是打算给Ubuntu做数据盘。</p><p>选择你想要安装Ubuntu的分区，点下面的加号开始进行Ubuntu分区。</p><p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170311/ubuntu3.png" alt=""></p><p>下图是分区选项。第一个是分区大小。第二个是选择分区类型，主分区还是逻辑分区（关于主分区和逻辑分区的知识可以自行百度了解，在Windows和Linux下的概念一样），对于Ubuntu系统本身而言，所有分区都是逻辑分区也没事。第三个是分区的起始位置，正着分还是倒着分，一般都选第一个。第四个是分区的文件系统。第五个是文件挂载点。</p><p>如果你对Linux系统完全不了解，并且不想了解分区的各种学问，那么按照我这种最简单的分区方法来做就可以：<strong>分两个区，第一个分区的大小和内存大小挂钩，如果你内存比较小（4G、8G）那么分区大小是内存大小的两倍，如果内存比较大，那么分区大小是内存大小的1.5倍，文件系统选Swap，这个分区是作为虚拟内存使用的；第二个分区的大小就是剩余空间全用上就可以，文件类型选Ext4，挂载点就选”/“。</strong></p><p>网上有各种分区的说法，当然各有各的好处或者是目的，如果你是小白并且只是想简单使用Linux系统，照我的分法肯定没问题。还有关于双系统，网上有说想要装双系统需要把/BOOT分区单独分区来，其实完全没这个必要，按照我的两个分区分法也可以成功安装双系统。</p><p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170311/ubuntu4.png" alt=""></p><p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170311/ubuntu5.png" alt=""></p><p>分完区后界面如下。我在新硬盘上分了一个主分区ext4，一个逻辑分区swap，还有一些空间没有使用。</p><p>在下面的device for boot那里选择”/“分区（如果你分出来了/BOOT分区，那么就选/BOOT）。然后在上面的区域内点击选择”/“分区，然后点Install Now。</p><p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170311/ubuntu6.png" alt=""></p><p>会出现一个确认对话框，点Continue。</p><p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170311/ubuntu7.png" alt=""></p><p>然后下面就是选地区选语言设置用户等等，最后出现下图后点Restart Now就安装完成啦。</p><p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170311/ubuntu8.png" alt=""></p><p>如果你点完以后出现下面的鬼提示，不用管，直接电源键强制重启即可。记得拔U盘哦。</p><p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170311/ubuntu9.png" alt=""></p><p>4、如果你只安装了Ubuntu，那么重启后就会进入Ubuntu系统了。如果你是要安装双系统，那么重启之后你会进入到Windows系统，那Ubuntu呢，别着急，接下来我们就添加Ubuntu的启动项。</p><p>需要下载一个软件<a href="http://neosmart.net/EasyBCD/" target="_blank" rel="noopener">EasyBCD</a>。进入页面后往下拉选择REGISTER，进入下一页面后不用填信息，直接点Download即可。</p><p>软件的界面大致如下。在工具菜单里可以设置语言。点左边的添加新条目，操作系统选Linux，类型是GRUB 2，名称可以自己随便设，驱动器的话选当初分的”/“分区，根据分区大小来判断（如果当初分了/BOOT分区，那么就选/BOOT分区），点击添加条目。然后点左边的编辑引导菜单，可以看到这时候条目里有两个系统了，下面有一些选项，比如倒计时或者是等待用户选择，依据情况自己设置。</p><p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170311/easybcd1.png" alt=""></p><p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170311/easybcd2.png" alt=""></p><p>弄好后重启电脑，看看是不是可以选择两个系统啦，大功告成~</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文安装的是 Ubuntu 16.04 LTS 64位版本，与Windows7构成双系统&lt;/p&gt;
&lt;p&gt;1、先从&lt;a href=&quot;https://www.ubuntu.com/download/alternative-downloads&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Ubunbu官方&lt;/a&gt;下载系统镜像，选择64位的16.04 LTS版本。&lt;/p&gt;
&lt;p&gt;2、制作USB安装盘。在Windows系统下， 选择&lt;a href=&quot;https://www.ezbsystems.com/ultraiso/download.htm&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;UltraISO&lt;/a&gt;来制作USB安装盘。注意，制作过程会将U盘格式化，请提前备份好资料。&lt;br&gt;进入UltralISO后，选择 文件&amp;gt;打开，选择下载好的Ubuntu镜像。&lt;br&gt;
    
    </summary>
    
      <category term="技术" scheme="https://bluesmilery.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="Linux" scheme="https://bluesmilery.github.io/tags/Linux/"/>
    
  </entry>
  
</feed>
