<!DOCTYPE html>
<html lang="">
  <head><meta name="generator" content="Hexo 3.8.0">
    
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">


<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">

<meta name="theme-color" content="#f8f5ec">
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">



  <meta name="description" content="增强学习 Reinforcement learning part 3 - Planning by Dynamic Programming">




  <meta name="keywords" content="Reinforcement Learning, Gai's Blog">





  <meta name="google-site-verification" content="SrSETrQ8JjhGYLB-qHgsT23NdViq-VEpvghPoNFdn2g">






  <link rel="alternate" href="/atom.xml" title="Gai's Blog">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=2.9.0">



<link rel="canonical" href="https://bluesmilery.github.io/blogs/b96003ba/">



  <link rel="stylesheet" type="text/css" href="/lib/fancybox/jquery.fancybox.css">




  <link rel="stylesheet" type="text/css" href="/lib/nprogress/nprogress.min.css">



<link rel="stylesheet" type="text/css" href="/css/style.css?v=2.9.0">



  
  <script id="baidu_analytics">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?a3f2fd48827f9ed28f8d9432317653de";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-121307266-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-121307266-1');
</script>


  <script id="baidu_push">
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>





  <script src="//cdn1.lncld.net/static/js/3.1.1/av-min.js"></script>
  <script id="leancloud">
    AV.init({
      appId: "BmUgD1geFKBFQpURVdGNzgl1-gzGzoHsz",
      appKey: "mOzxpdEEenjouuAKMFPazvnd"
    });
  </script>




<script>
  window.config = {"title":"Gai's Blog","subtitle":"A Zone for Knowledge","description":null,"author":"Gai","language":null,"timezone":null,"url":"https://bluesmilery.github.io","root":"/","permalink":"blogs/:abbrlink/","permalink_defaults":null,"source_dir":"source","public_dir":"public","tag_dir":"tags","archive_dir":"archives","category_dir":"categories","code_dir":"downloads/code","i18n_dir":":lang","skip_render":null,"new_post_name":":year-:month-:day-:title.md","default_layout":"post","titlecase":false,"external_link":true,"filename_case":0,"render_drafts":false,"post_asset_folder":false,"relative_link":false,"future":true,"highlight":{"enable":true,"auto_detect":false,"line_number":true,"tab_replace":null},"default_category":"uncategorized","category_map":null,"tag_map":null,"date_format":"YYYY-MM-DD","time_format":"HH:mm:ss","per_page":10,"pagination_dir":"page","theme":"even","deploy":[{"type":"git","repo":"git@github.com:bluesmilery/bluesmilery.github.io.git","branch":"master"},{"type":"baidu_url_submitter"}],"ignore":[],"keywords":null,"index_generator":{"per_page":10,"order_by":"-date","path":""},"sitemap":{"path":"sitemap.xml"},"baidusitemap":{"path":"baidusitemap.xml"},"abbrlink":{"alg":"crc32","rep":"hex"},"baidu_url_submit":{"count":1,"host":"bluesmilery.github.io","token":"cktimjuXHORRMXsM","path":"baidu_urls.txt"},"archive_generator":{"per_page":10,"yearly":true,"monthly":true,"daily":false},"category_generator":{"per_page":10},"tag_generator":{"per_page":10},"feed":{"type":"atom","limit":20,"hub":"","content":true,"content_limit":140,"content_limit_delim":"","path":"atom.xml"},"marked":{"gfm":true,"pedantic":false,"sanitize":false,"tables":true,"breaks":true,"smartLists":true,"smartypants":true,"modifyAnchors":"","autolink":true},"server":{"port":4000,"log":false,"compress":false,"header":true},"since":2017,"favicon":"/favicon.ico","rss":"default","menu":{"Home":"/","Archives":"/archives/","Tags":"/tags","Categories":"/categories"},"color":"Default","toc":true,"fancybox":true,"pjax":true,"copyright":{"enable":true,"license":"<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\" target=\"_blank\">知识共享署名-非商业性使用 4.0 国际许可协议</a>"},"reward":{"enable":false,"qrCode":{"wechat":null,"alipay":null}},"social":{"email":"plgaixd92498@gmail.com","stack-overflow":null,"twitter":null,"facebook":null,"linkedin":null,"google":null,"github":"https://github.com/bluesmilery","weibo":null,"zhihu":null,"douban":null,"pocket":null,"tumblr":null,"instagram":null},"leancloud":{"app_id":"BmUgD1geFKBFQpURVdGNzgl1-gzGzoHsz","app_key":"mOzxpdEEenjouuAKMFPazvnd"},"baidu_analytics":"a3f2fd48827f9ed28f8d9432317653de","baidu_verification":null,"google_analytics":"UA-121307266-1","google_verification":"SrSETrQ8JjhGYLB-qHgsT23NdViq-VEpvghPoNFdn2g","disqus_shortname":null,"changyan":{"appid":null,"appkey":null},"livere_datauid":"MTAyMC8zMzY4MC8xMDIzNQ","version":"2.9.0","word_count":true};
</script>

    <title> 增强学习 Reinforcement learning part 3 - Planning by Dynamic Programming - Gai's Blog </title>
  </head>

  <body><div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/." class="logo">Gai's Blog</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>

<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    
      <a href="/">
        <li class="mobile-menu-item">
          
          
            首页
          
        </li>
      </a>
    
      <a href="/archives/">
        <li class="mobile-menu-item">
          
          
            归档
          
        </li>
      </a>
    
      <a href="/tags">
        <li class="mobile-menu-item">
          
          
            标签
          
        </li>
      </a>
    
      <a href="/categories">
        <li class="mobile-menu-item">
          
          
            分类
          
        </li>
      </a>
    
  </ul>
</nav>

    <div class="container" id="mobile-panel">
      <header id="header" class="header"><div class="logo-wrapper">
  <a href="/." class="logo">Gai's Blog</a>
</div>

<nav class="site-navbar">
  
    <ul id="menu" class="menu">
      
        <li class="menu-item">
          <a class="menu-item-link" href="/">
            
            
              首页
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/archives/">
            
            
              归档
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/tags">
            
            
              标签
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/categories">
            
            
              分类
            
          </a>
        </li>
      
    </ul>
  
</nav>

      </header>

      <main id="main" class="main">
        <div class="content-wrapper">
          <div id="content" class="content">
            
  
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          增强学习 Reinforcement learning part 3 - Planning by Dynamic Programming
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2017-04-11
        </span>
        
          <div class="post-category">
            
              <a href="/categories/笔记/">笔记</a>
            
          </div>
        
        
        <div class="post-visits" data-url="/blogs/b96003ba/" data-title="增强学习 Reinforcement learning part 3 - Planning by Dynamic Programming">
            阅读次数 0
          </div>
        
        
        
        
          <span class="post-count">本文共2.6k字</span>
          <span class="post-count">阅读约10分钟</span>
        
      </div>
    </header>

    
    
  <div class="post-toc" id="post-toc">
    <h2 class="post-toc-title">文章目录</h2>
    <div class="post-toc-content">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1、Introduction"><span class="toc-text">1、Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2、Policy-Evaluation"><span class="toc-text">2、Policy Evaluation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3、Policy-Iteration"><span class="toc-text">3、Policy Iteration</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4、Value-Iteration"><span class="toc-text">4、Value Iteration</span></a></li></ol>
    </div>
  </div>



    <div class="post-content">
      
        <p>本文是在学习David Silver所教授的Reinforcement learning课程过程中所记录的笔记。因为个人知识的不足以及全程啃生肉，难免会有理解偏差的地方，欢迎一起交流。</p>
<p>课程资料：<a href="http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html" target="_blank" rel="noopener">http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html</a></p>
<h2 id="1、Introduction"><a href="#1、Introduction" class="headerlink" title="1、Introduction"></a>1、Introduction</h2><p>这一部分是用动态规划来解决MDP问题，所以先介绍下什么是动态规划</p>
<p><mark>Dynamic Programming</mark>（动态规划），是用来解决一些复杂的问题，将复杂的问题分解为一些子问题，然后分别解决这些子问题，最后再把解决方案合并得到复杂问题的解</p>
<ul>
<li>Dynamic：代表这个问题是连续的或者是跟时间相关的</li>
<li>Programming：不是编程的意思，而指的是优化一个程序</li>
</ul>
<a id="more"></a>
<p>想要使用DP来解决一个问题，那么这个问题需要具有以下两种性质：</p>
<ul>
<li>Optimal substructure：满足最优化原理，最优解分解后针对子问题也是最优解</li>
<li>Overlapping subproblems：子问题重复出现，对每一种子问题只计算一次，然后将结果存起来，下次直接使用</li>
</ul>
<p>而MDP刚好满足这两条性质，bellman equation具有递归分解性，说明可以分解为子问题并且会重复出现（类似于斐波那契数列）；value function就相当于能够存储并且重复利用的解</p>
<p><strong>要使用DP来解决MDP问题，假设条件是对MDP是充分认知（full knowledge）的</strong></p>
<p>对于MDP来说，还是从两个方面来说，prediction and control</p>
<p>prediction的目的是给你policy，然后去预测使用这个policy的话各个state的value function，也就是预测使用这个policy的话未来的收益</p>
<p>control的目的是不给你policy，自己去寻找最优的policy以及未来最优的收益</p>
<p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170410/rl3-1.png" alt=""></p>
<p>扩展：动态规划还可以解决其他许多问题</p>
<ul>
<li>Scheduling algorithms</li>
<li>String algorithms (e.g. sequence alignment)</li>
<li>Graph algorithms (e.g. shortest path algorithms)</li>
<li>Graphical models (e.g. Viterbi algorithm)</li>
<li>Bioinformatics (e.g. lattice models)</li>
</ul>
<h2 id="2、Policy-Evaluation"><a href="#2、Policy-Evaluation" class="headerlink" title="2、Policy Evaluation"></a>2、Policy Evaluation</h2><p>策略评估</p>
<p>需要解决的问题：评估一个给定的policy</p>
<p>解决方案：以迭代次数为维度（或者理解成时间，一次迭代相当于过去了一个时刻，agent执行了一次action）不断去迭代Bellman Expectation Equation</p>
<p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170410/rl3-2.png" alt=""></p>
<p>可以证明，<strong>最终value function会收敛到vpi</strong></p>
<p>采用同步更新的方式，在新的时刻所有的state同时更新</p>
<p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170410/rl3-3.png" alt=""></p>
<p>计算方法如下：</p>
<p>套用Bellman Expectation Equation，加上迭代次数维度，依据马尔可夫性，未来只与现在有关，所以求state s的k+1时刻的value function，就要使用k时刻state s’的value function，其中s’是s的后续state</p>
<table><tr><td><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170410/rl3-4.png" alt=""></td><td><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170410/rl3-5.png" alt=""></td></tr></table>

<p>上面的是RL part2中的计算方程，相当于是下面的计算方程迭代到最后已经收敛的形式</p>
<table><tr><td><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170410/rl3-6.png" alt=""></td><td><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170410/rl3-7.png" alt=""></td></tr></table>

<p>下面举个例子gridworld</p>
<p>这是个4x4的方格，那两个灰色的方格是特殊的state，可以看作是终点，其他14个白色的方格是普通的state，在白色方格时有四种action选择，向上下左右移动，各自的概率为0.25。每移动一步reward减1，直到灰色的方格</p>
<p>所以这个例子可以看作你随机出现在某个方格上，然后用最少的步数走到灰色方格。</p>
<p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170410/rl3-8.png" alt=""></p>
<p>下面是进行policy evaluation的迭代过程，k代表第几次迭代</p>
<p>先说左边的，选择的policy是random policy，就是在任何一个state，选择四个方向的概率都为0.25。初始v0的时候所有state的value都是0。然后v1时，利用上面vk+1(s)的公式进行计算（0.25x(-1+0)x4）。下面以v2中两个state来说明具体的计算过程</p>
<p><u>先计算state6</u>（就是第二行第三个，这里对应公式的话就是s）在这次迭代中（对应公式的话就是k+1）的state value：如果向上走的话会移动到state2（对应公式的话就是s’），那这个action value就是reward(-1)加上state2上一时刻（对应公式的话就是k）的state value(-1)，所以向上移动是0.25x(-1-1)。向其他三个方向移动的计算方式和向上移动类似，结果也都是0.25x(-1-1)。所以state6在v2时的state value就是0.25x(-1-1)x4=-2</p>
<p><u>再计算state1</u>（第一行第二个）的state value：如果选择向上走的话，因为上面没有格子了，所以会停在原地，那这次action value就是移动的reward(-1)加上state1上一时刻的state value(-1)，所以向上移动是0.25x(-1-1)。向下向右类似。向左移动的话是reward(-1)加上灰色格子上一时刻的state value(0)，所以向左移动就是0.25x(-1+0)。所以state1在v2时的state value就是0.25x(-1-1)x3+0.25x(-1+0)=-1.75。图中显示是-1.7，那是因为只能显示两位数。</p>
<p>如果一直迭代下去，当k为无穷的时候，各个state的value就已经收敛不会变了。</p>
<table><tr><td><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170410/rl3-9.png" alt=""></td><td><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170410/rl3-10.png" alt=""></td></tr></table>

<p>再说下右边的，显示的是每次迭代根据左边计算出来的state value选择出来的greedy policy。greedy policy指的是选出各个action value中最高的所对应的action。比如在v2时，state1的四个action value中，向左移动的action value是最高的，所以greedy policy在state1处只选择向左移动</p>
<p>当random policy经过许多步迭代后state value收敛到vpi时，这时候选出的greedy policy就是random policy的optimal policy</p>
<p>注意：evaluate任何一个policy（这里是random policy）都能得到greedy policy</p>
<p>总结：进行策略评估的实质就是计算在使用policy pi时各个state的value function，但是因为reword和action的存在，随着时间的推移（action次数的增加），每个state的value function会变化，但是最终会收敛到一个固定值vpi，然后使用greedy policy选出policy pi的optimal policy</p>
<h2 id="3、Policy-Iteration"><a href="#3、Policy-Iteration" class="headerlink" title="3、Policy Iteration"></a>3、Policy Iteration</h2><p>策略迭代</p>
<p>在策略评估中，讲的是在给定一个policy pi后，先对其进行evaluate得到vpi，然后使用greedy policy来improve policy，得到新的policy pi’</p>
<p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170410/rl3-11.png" alt=""></p>
<p>在上面gridworld的例子中，因为这个例子很简单，所以经过一次improve后得到的pi’就已经是optimal policy pi*</p>
<p>但是通常说来，一般需要经过多次evaluation/improvement的过程才能得到pi*</p>
<p>这种不断evaluation/improvement的过程就称作策略迭代</p>
<p><strong>注意：类似于policy evaluation中state value会收敛到vpi，在policy iteration中policy肯定会收敛到pi*</strong></p>
<table><tr><td><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170410/rl3-12.png" alt=""></td><td><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170410/rl3-13.png" alt=""></td></tr></table>

<p>evaluation和improvement过程中的算法可以是任意的</p>
<p>example：</p>
<table><tr><td><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170410/rl3-14.png" alt=""></td><td><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170410/rl3-15.png" alt=""></td></tr></table>

<p>下面是具体讲一下improve过程，证明了为什么improve后vpi’(s)&gt;=vpi(s)</p>
<table><tr><td><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170410/rl3-16.png" alt=""></td><td><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170410/rl3-17.png" alt=""></td></tr></table>

<p>modified：在evaluation部分，一定要等state value达到vpi吗？</p>
<p>事实上，这并不是必须的，就好比上面的gridworld例子，在第三次迭代的时候其实就已经是optimal policy了，所以没必要继续迭代下去</p>
<p>所以我们可以通过一些方法提前停止evaluation，比如在state value收敛的过程中设置epsilon，小于则停止迭代；或者是设置迭代k次就停止</p>
<p>如果设置evaluation只迭代一次，即k=1，那么这就是下面要讲的value iteration</p>
<p>总结：policy iteration相比于policy evaluation而言，后者是一次evaluation/improvement过程，只能得到比policy pi更好的policy pi’，而前者是多次evaluation/improvement过程，最终能得到该MDP的optimal policy pi*</p>
<h2 id="4、Value-Iteration"><a href="#4、Value-Iteration" class="headerlink" title="4、Value Iteration"></a>4、Value Iteration</h2><p>值迭代</p>
<p>上面说过能用DP解决的问题需要具有一个特点，满足最优化原理（Principle of Optimality），就是最优解分解之后针对每个子问题也是最优解，对于MDP来说，就是optimal policy可以分为两部分，当前state的optimal action A*，后续state的policy也是optimal policy<br>Any optimal policy can be subdivided into two components:</p>
<ul>
<li>An optimal first action A*</li>
<li>Followed by an optimal policy from successor state S0</li>
</ul>
<p>所以，若要满足一个policy pi使的state s的value为optimal value，当且仅当policy pi使得s的后续state s’的value为optimal value</p>
<p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170410/rl3-18.png" alt=""></p>
<p>所以知道了s’的optimal value便可以倒推出来s的optimal value</p>
<p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170410/rl3-19.png" alt=""></p>
<p>所以这便是value iteration的思想所在，迭代重复上面这个公式</p>
<p>直观上看来，是从最后一个state开始，倒推到第一个state，下面以一个例子来说明下</p>
<p>这个例子的目的是每个state找出到达左上角的最短路径（步数），其他方面类似于之前gridworld的例子，action是向四个方向移动，每移动一步reward减1</p>
<p>V1初始时所有的state的value都是0，然后在V2时，每个state从四个action value中选出最高的那个作为自己的state value（在policy iteration中是将四个action value求期望作为自己的state value），以第二行第二个为例，它在V1V2V3的时候四个action value都是一样的，所以没有相对的optimal，但是从V4开始，向上和向左比向下向右好，所以它的state value就一直为-2，拿V6来说，它的state value = max(-2, -2, -4, -4) = -2</p>
<p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170410/rl3-20.png" alt=""></p>
<p>在这个例子中需要注意这么几个问题：</p>
<ul>
<li>每一次迭代中所有的state都要计算value，只不过有些state每次max的时候都是那些值（先达到了optimal value）所以看起来就不变了</li>
<li>在开始的时候不像policy iteration那样有个明确的policy，而且在迭代过程中state value并不对应于哪个policy，只有在最终迭代完成后（在例子中是V7）才对应于一个policy，而且还是optimal policy</li>
<li>如果以右下角那个state为开始的话，那路径就是-6 -5 -4 -3 -2 -1 0，而计算过程是-1 -2 -3 -4 -5 -6，所以符合上面说的Principle of Optimality的倒推性</li>
</ul>
<p>所以对于value iteration来说</p>
<p>需要解决的问题：找到optimal policy pi*</p>
<p>解决方案：不断去迭代Bellman optimality Equation</p>
<p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170410/rl3-21.png" alt=""></p>
<p>可以证明， <strong>最终value function会收敛到v*</strong></p>
<p>采用同步更新的方式，在新的时刻所有的state同时更新</p>
<p><img src="/Users/Gai/Desktop/image/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/压缩/rl3-22.png" alt="rl3-22"></p>
<p>计算过程：</p>
<table><tr><td><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170410/rl3-23.png" alt=""></td><td><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170410/rl3-24.png" alt=""></td></tr></table>

<p>总结：value iteration的本质就是在每次迭代中每个state都选择optimal value，那么当所有的state都选择出了optimal value后，所对应的policy就是optimal policy。</p>
<p>对比policy iteration，是迭代到收敛后每个state才选择optimal value，然后这时候对应的policy就是improve后的policy pi’</p>
<p>有个value iteration的例子：需要安装java环境</p>
<p><a href="http://www.cs.ubc.ca/~poole/demos/mdp/vi.html" target="_blank" rel="noopener">http://www.cs.ubc.ca/~poole/demos/mdp/vi.html</a></p>
<p>下表是对DP的一个总结</p>
<p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170410/rl3-25.png" alt=""></p>
<p>5、</p>
<ul>
<li>Extensions to Dynamic Programming</li>
<li>Asynchronous Dynamic Programming</li>
<li>In-Place Dynamic Programming</li>
<li>Prioritised Sweeping</li>
<li>Real-Time Dynamic Programming</li>
<li>Full-Width Backups</li>
<li>Approximate Dynamic Programming</li>
</ul>
<p>6、</p>
<ul>
<li>Contraction Mapping</li>
</ul>

      
    </div>

    
      
      

  <div class="post-copyright">
    <p class="copyright-item">
      <span>原文作者: </span>
      <a href="https://bluesmilery.github.io">Gai</a>
    </p>
    <p class="copyright-item">
      <span>原文链接: </span>
      <a href="https://bluesmilery.github.io/blogs/b96003ba/">https://bluesmilery.github.io/blogs/b96003ba/</a>
    </p>
    <p class="copyright-item">
      <span>许可协议: </span>
      
      <a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank">知识共享署名-非商业性使用 4.0 国际许可协议</a>
    </p>
  </div>



      
      
    

    
      <footer class="post-footer">
        
          <div class="post-tags">
            
              <a href="/tags/Reinforcement-Learning/">Reinforcement Learning</a>
            
          </div>
        
        
        
  <nav class="post-nav">
    
      <a class="prev" href="/blogs/a6aaca4e/">
        <i class="iconfont icon-left"></i>
        <span class="prev-text nav-default">增强学习 Reinforcement learning part 4 - Model-Free Prediction</span>
        <span class="prev-text nav-mobile">上一篇</span>
      </a>
    
    
      <a class="next" href="/blogs/e4dc3fbf/">
        <span class="next-text nav-default">增强学习 Reinforcement learning part 2 - Markov Decision Process</span>
        <span class="prev-text nav-mobile">下一篇</span>
        <i class="iconfont icon-right"></i>
      </a>
    
  </nav>

      </footer>
    

  </article>


          </div>
          
  <div class="comments" id="comments">
    
      <div id="lv-container" data-id="city" data-uid="MTAyMC8zMzY4MC8xMDIzNQ">
        <noscript>为正常使用来必力评论功能请激活JavaScript</noscript>
      </div>  
    
  </div>


        </div>
      </main>

      <footer id="footer" class="footer"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>


  <div class="social-links">
    
      
        
          <a href="mailto:plgaixd92498@gmail.com" class="iconfont icon-email" title="email"></a>
        
      
    
      
    
      
    
      
    
      
    
      
    
      
        
          <a href="https://github.com/bluesmilery" class="iconfont icon-github" title="github"></a>
        
      
    
      
    
      
    
      
    
      
    
      
    
      
    
    
    
      <a href="/atom.xml" class="iconfont icon-rss" title="rss"></a>
    
  </div>


<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://hexo.io/">Hexo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/ahonn/hexo-theme-even">Even</a>
  </span>

  
  <br>
  <span id="busuanzi_container_site_uv">
    被<span id="busuanzi_value_site_uv"></span>个小伙伴
  </span>
  <span id="busuanzi_container_site_pv">
    查看了<span id="busuanzi_value_site_pv"></span>次~
  </span>

  <span class="copyright-year">
    
    &copy; 
     
      2017 - 
    
    2018

    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">Gai</span>
  </span>
</div>

      </footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div>

    
  
  

  
   <script type="text/javascript">
	(function(d, s) {
       var j, e = d.getElementsByTagName(s)[0];

       if (typeof LivereTower === 'function') { return; }

       j = d.createElement(s);
       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
       j.async = true;

       e.parentNode.insertBefore(j, e);
   })(document, 'script');
  </script>




    
  



  
  





  
    <script type="text/javascript" src="/lib/jquery/jquery.min.js"></script>
  

  
    <script type="text/javascript" src="/lib/slideout/slideout.js"></script>
  

  
    <script type="text/javascript" src="/lib/fancybox/jquery.fancybox.pack.js"></script>
  

  
    <script type="text/javascript" src="/lib/pjax/jquery.pjax.min.js"></script>
  

  
    <script type="text/javascript" src="/lib/nprogress/nprogress.min.js"></script>
  


    <script type="text/javascript" src="/js/src/even.js?v=2.9.0"></script>

  </body>
</html>
