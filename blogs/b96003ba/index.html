<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>增强学习 Reinforcement learning part 3 - Planning by Dynamic Programming | Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="本文是在学习David Silver所教授的Reinforcement learning课程过程中所记录的笔记。因为个人知识的不足以及全程啃生肉，难免会有理解偏差的地方，欢迎一起交流。
课程资料：http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html
1、Introduction这一部分是用动态规划来解决MDP问题，所以先介绍下什么是动态">
<meta property="og:type" content="article">
<meta property="og:title" content="增强学习 Reinforcement learning part 3 - Planning by Dynamic Programming">
<meta property="og:url" content="https://bluesmilery.github.io/blogs/b96003ba/index.html">
<meta property="og:site_name" content="Blog">
<meta property="og:description" content="本文是在学习David Silver所教授的Reinforcement learning课程过程中所记录的笔记。因为个人知识的不足以及全程啃生肉，难免会有理解偏差的地方，欢迎一起交流。
课程资料：http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html
1、Introduction这一部分是用动态规划来解决MDP问题，所以先介绍下什么是动态">
<meta property="og:image" content="https://bluesmilery.github.io/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/737848F0-DA52-41D0-9E1D-0EE4C7F51AE4.png">
<meta property="og:image" content="https://bluesmilery.github.io/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/27F1D6EC-A4DD-447E-824B-19B3A5D88365.png">
<meta property="og:image" content="https://bluesmilery.github.io/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/EEDA561D-0975-437C-9DA5-44FE195B4E5D.png">
<meta property="og:image" content="https://bluesmilery.github.io/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/42BE141A-72F8-4067-A6AC-C1C4074642F2.png">
<meta property="og:image" content="https://bluesmilery.github.io/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/8A1CCBFD-0D78-4BF9-B649-9E8806A3B9CE.png">
<meta property="og:image" content="https://bluesmilery.github.io/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/183C8581-3986-486F-8C46-CB076F5AEF0D.png">
<meta property="og:image" content="https://bluesmilery.github.io/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/887A54FF-532D-4E08-91AD-943439067C96.png">
<meta property="og:image" content="https://bluesmilery.github.io/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/8C060CBB-158A-4DB9-B8A9-AF9C1AEE3819.png">
<meta property="og:image" content="https://bluesmilery.github.io/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/4F2526A0-4B13-4A56-868F-24474E18EFF2.png">
<meta property="og:image" content="https://bluesmilery.github.io/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/3A6FDDB5-A00D-4989-A5D2-9EEDBFC8046A.png">
<meta property="og:image" content="https://bluesmilery.github.io/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/19BC5362-3238-4EBF-81E0-1283C812866C.png">
<meta property="og:image" content="https://bluesmilery.github.io/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/EAE5565A-B89E-463D-B626-45F71EF903DC.jpg">
<meta property="og:image" content="https://bluesmilery.github.io/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/E1B33029-6919-4A03-9185-EAC804BB03A5.png">
<meta property="og:image" content="https://bluesmilery.github.io/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/7D97F226-48B6-420F-AEE8-CF0E568B85E1.jpg">
<meta property="og:image" content="https://bluesmilery.github.io/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/145C478C-215D-4ABD-964B-52222C33D3B1.jpg">
<meta property="og:image" content="https://bluesmilery.github.io/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/9ECAC510-25D1-4E8A-9520-6FC4C9D54ADE.png">
<meta property="og:image" content="https://bluesmilery.github.io/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/BEB4F5C7-E027-4028-8513-1B03EFC444A9.png">
<meta property="og:image" content="https://bluesmilery.github.io/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/FDB650F5-C6EB-4853-B7E8-CBA6B9265F00.png">
<meta property="og:image" content="https://bluesmilery.github.io/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/C57EE2DD-688D-4E34-8C52-267BABA13163.jpg">
<meta property="og:image" content="https://bluesmilery.github.io/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/53F5BD3B-6F51-4940-9D3E-8D244B44DD38.png">
<meta property="og:image" content="https://bluesmilery.github.io/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/16C7EA57-ABD6-4E7F-8533-36BCA94C2A0D.png">
<meta property="og:image" content="https://bluesmilery.github.io/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/D6214822-FC86-4C8C-BF90-429C21F6A836.png">
<meta property="og:image" content="https://bluesmilery.github.io/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/55DFF675-66BA-4CD0-8878-39DBC1809814.png">
<meta property="og:image" content="https://bluesmilery.github.io/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/E887D5E7-5635-4601-A29A-C2D4CEA11F36.png">
<meta property="og:image" content="https://bluesmilery.github.io/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/016A5E4D-2713-4FA7-BC4A-D15A43A0588E.png">
<meta property="og:updated_time" content="2017-04-10T13:54:18.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="增强学习 Reinforcement learning part 3 - Planning by Dynamic Programming">
<meta name="twitter:description" content="本文是在学习David Silver所教授的Reinforcement learning课程过程中所记录的笔记。因为个人知识的不足以及全程啃生肉，难免会有理解偏差的地方，欢迎一起交流。
课程资料：http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html
1、Introduction这一部分是用动态规划来解决MDP问题，所以先介绍下什么是动态">
<meta name="twitter:image" content="https://bluesmilery.github.io/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/737848F0-DA52-41D0-9E1D-0EE4C7F51AE4.png">
  
    <link rel="alternate" href="/atom.xml" title="Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">A Zone for Knowledge</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-github-link" class="nav-icon" href="https://github.com/bluesmilery" title="Github" target="_blank"></a>
        
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://bluesmilery.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Reinforcement learning part 3 - Planning by Dynamic Programming" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/blogs/b96003ba/" class="article-date">
  <time datetime="2017-04-10T12:04:48.000Z" itemprop="datePublished">2017-04-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      增强学习 Reinforcement learning part 3 - Planning by Dynamic Programming
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>本文是在学习David Silver所教授的Reinforcement learning课程过程中所记录的笔记。因为个人知识的不足以及全程啃生肉，难免会有理解偏差的地方，欢迎一起交流。</p>
<p>课程资料：<a href="http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html" target="_blank" rel="external">http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html</a></p>
<h2 id="1、Introduction"><a href="#1、Introduction" class="headerlink" title="1、Introduction"></a>1、Introduction</h2><p>这一部分是用动态规划来解决MDP问题，所以先介绍下什么是动态规划</p>
<p><mark>Dynamic Programming</mark>（动态规划），是用来解决一些复杂的问题，将复杂的问题分解为一些子问题，然后分别解决这些子问题，最后再把解决方案合并得到复杂问题的解</p>
<ul>
<li>Dynamic：代表这个问题是连续的或者是跟时间相关的</li>
<li>Programming：不是编程的意思，而指的是优化一个程序</li>
</ul>
<a id="more"></a>
<p>想要使用DP来解决一个问题，那么这个问题需要具有以下两种性质：</p>
<ul>
<li>Optimal substructure：满足最优化原理，最优解分解后针对子问题也是最优解</li>
<li>Overlapping subproblems：子问题重复出现，对每一种子问题只计算一次，然后将结果存起来，下次直接使用</li>
</ul>
<p>而MDP刚好满足这两条性质，bellman equation具有递归分解性，说明可以分解为子问题并且会重复出现（类似于斐波那契数列）；value function就相当于能够存储并且重复利用的解</p>
<p><strong>要使用DP来解决MDP问题，假设条件是对MDP是充分认知（full knowledge）的</strong><br>对于MDP来说，还是从两个方面来说，prediction and control<br>prediction的目的是给你policy，然后去预测使用这个policy的话各个state的value function，也就是预测使用这个policy的话未来的收益<br>control的目的是不给你policy，自己去寻找最优的policy以及未来最优的收益<br><img src="/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/737848F0-DA52-41D0-9E1D-0EE4C7F51AE4.png" width="70%" height="70%"></p>
<p>扩展：动态规划还可以解决其他许多问题</p>
<ul>
<li>Scheduling algorithms</li>
<li>String algorithms (e.g. sequence alignment)</li>
<li>Graph algorithms (e.g. shortest path algorithms)</li>
<li>Graphical models (e.g. Viterbi algorithm)</li>
<li>Bioinformatics (e.g. lattice models)</li>
</ul>
<h2 id="2、Policy-Evaluation"><a href="#2、Policy-Evaluation" class="headerlink" title="2、Policy Evaluation"></a>2、Policy Evaluation</h2><p>策略评估<br>需要解决的问题：评估一个给定的policy<br>解决方案：以迭代次数为维度（或者理解成时间，一次迭代相当于过去了一个时刻，agent执行了一次action）不断去迭代Bellman Expectation Equation<br><img src="/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/27F1D6EC-A4DD-447E-824B-19B3A5D88365.png" width="50%" height="50%"><br>可以证明，<strong>最终value function会收敛到vpi</strong><br>采用同步更新的方式，在新的时刻所有的state同时更新<br><img src="/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/EEDA561D-0975-437C-9DA5-44FE195B4E5D.png" width="70%" height="70%"><br>计算方法如下：<br>套用Bellman Expectation Equation，加上迭代次数维度，依据马尔可夫性，未来只与现在有关，所以求state s的k+1时刻的value function，就要使用k时刻state s’的value function，其中s’是s的后续state</p>
<table><tr><td><img src="/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/42BE141A-72F8-4067-A6AC-C1C4074642F2.png" width="100%" height="100%"></td><td><img src="/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/8A1CCBFD-0D78-4BF9-B649-9E8806A3B9CE.png" width="100%" height="100%"></td></tr></table><br>上面的是RL part2中的计算方程，相当于是下面的计算方程迭代到最后已经收敛的形式<br><table><tr><td><img src="/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/183C8581-3986-486F-8C46-CB076F5AEF0D.png" width="100%" height="100%"></td><td><img src="/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/887A54FF-532D-4E08-91AD-943439067C96.png" width="100%" height="100%"></td></tr></table>

<p>下面举个例子gridworld<br>这是个4x4的方格，那两个灰色的方格是特殊的state，可以看作是终点，其他14个白色的方格是普通的state，在白色方格时有四种action选择，向上下左右移动，各自的概率为0.25。每移动一步reward减1，直到灰色的方格<br>所以这个例子可以看作你随机出现在某个方格上，然后用最少的步数走到灰色方格。<br><img src="/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/8C060CBB-158A-4DB9-B8A9-AF9C1AEE3819.png" width="50%" height="50%"><br>下面是进行policy evaluation的迭代过程，k代表第几次迭代<br>先说左边的，选择的policy是random policy，就是在任何一个state，选择四个方向的概率都为0.25。初始v0的时候所有state的value都是0。然后v1时，利用上面vk+1(s)的公式进行计算（0.25x(-1+0)x4）。下面以v2中两个state来说明具体的计算过程<br><u>先计算state6</u>（就是第二行第三个，这里对应公式的话就是s）在这次迭代中（对应公式的话就是k+1）的state value：如果向上走的话会移动到state2（对应公式的话就是s’），那这个action value就是reward(-1)加上state2上一时刻（对应公式的话就是k）的state value(-1)，所以向上移动是0.25x(-1-1)。向其他三个方向移动的计算方式和向上移动类似，结果也都是0.25x(-1-1)。所以state6在v2时的state value就是0.25x(-1-1)x4=-2<br><u>再计算state1</u>（第一行第二个）的state value：如果选择向上走的话，因为上面没有格子了，所以会停在原地，那这次action value就是移动的reward(-1)加上state1上一时刻的state value(-1)，所以向上移动是0.25x(-1-1)。向下向右类似。向左移动的话是reward(-1)加上灰色格子上一时刻的state value(0)，所以向左移动就是0.25x(-1+0)。所以state1在v2时的state value就是0.25x(-1-1)x3+0.25x(-1+0)=-1.75。图中显示是-1.7，那是因为只能显示两位数。<br>如果一直迭代下去，当k为无穷的时候，各个state的value就已经收敛不会变了。</p>
<table><tr><td><img src="/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/4F2526A0-4B13-4A56-868F-24474E18EFF2.png" width="100%" height="100%"></td><td><img src="/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/3A6FDDB5-A00D-4989-A5D2-9EEDBFC8046A.png" width="100%" height="100%"></td></tr></table>

<p>再说下右边的，显示的是每次迭代根据左边计算出来的state value选择出来的greedy policy。greedy policy指的是选出各个action value中最高的所对应的action。比如在v2时，state1的四个action value中，向左移动的action value是最高的，所以greedy policy在state1处只选择向左移动<br>当random policy经过许多步迭代后state value收敛到vpi时，这时候选出的greedy policy就是random policy的optimal policy<br>注意：evaluate任何一个policy（这里是random policy）都能得到greedy policy</p>
<p>总结：进行策略评估的实质就是计算在使用policy pi时各个state的value function，但是因为reword和action的存在，随着时间的推移（action次数的增加），每个state的value function会变化，但是最终会收敛到一个固定值vpi，然后使用greedy policy选出policy pi的optimal policy</p>
<h2 id="3、Policy-Iteration"><a href="#3、Policy-Iteration" class="headerlink" title="3、Policy Iteration"></a>3、Policy Iteration</h2><p>策略迭代<br>在策略评估中，讲的是在给定一个policy pi后，先对其进行evaluate得到vpi，然后使用greedy policy来improve policy，得到新的policy pi’<br><img src="/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/19BC5362-3238-4EBF-81E0-1283C812866C.png" width="50%" height="50%"><br>在上面gridworld的例子中，因为这个例子很简单，所以经过一次improve后得到的pi’就已经是optimal policy pi*<br>但是通常说来，一般需要经过多次evaluation/improvement的过程才能得到pi*<br>这种不断evaluation/improvement的过程就称作策略迭代<br><strong>注意：类似于policy evaluation中state value会收敛到vpi，在policy iteration中policy肯定会收敛到pi*</strong></p>
<table><tr><td><img src="/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/EAE5565A-B89E-463D-B626-45F71EF903DC.jpg" width="100%" height="100%"></td><td><img src="/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/E1B33029-6919-4A03-9185-EAC804BB03A5.png" width="100%" height="100%"></td></tr></table>

<p>evaluation和improvement过程中的算法可以是任意的<br>example：</p>
<table><tr><td><img src="/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/7D97F226-48B6-420F-AEE8-CF0E568B85E1.jpg" width="100%" height="100%"></td><td><img src="/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/145C478C-215D-4ABD-964B-52222C33D3B1.jpg" width="100%" height="100%"></td></tr></table>

<p>下面是具体讲一下improve过程，证明了为什么improve后vpi’(s)&gt;=vpi(s)</p>
<table><tr><td><img src="/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/9ECAC510-25D1-4E8A-9520-6FC4C9D54ADE.png" width="100%" height="100%"></td><td><img src="/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/BEB4F5C7-E027-4028-8513-1B03EFC444A9.png" width="100%" height="100%"></td></tr></table>

<p>modified：在evaluation部分，一定要等state value达到vpi吗？<br>事实上，这并不是必须的，就好比上面的gridworld例子，在第三次迭代的时候其实就已经是optimal policy了，所以没必要继续迭代下去<br>所以我们可以通过一些方法提前停止evaluation，比如在state value收敛的过程中设置epsilon，小于则停止迭代；或者是设置迭代k次就停止<br>如果设置evaluation只迭代一次，即k=1，那么这就是下面要讲的value iteration</p>
<p>总结：policy iteration相比于policy evaluation而言，后者是一次evaluation/improvement过程，只能得到比policy pi更好的policy pi’，而前者是多次evaluation/improvement过程，最终能得到该MDP的optimal policy pi*</p>
<h2 id="4、Value-Iteration"><a href="#4、Value-Iteration" class="headerlink" title="4、Value Iteration"></a>4、Value Iteration</h2><p>值迭代<br>上面说过能用DP解决的问题需要具有一个特点，满足最优化原理（Principle of Optimality），就是最优解分解之后针对每个子问题也是最优解，对于MDP来说，就是optimal policy可以分为两部分，当前state的optimal action A*，后续state的policy也是optimal policy<br>Any optimal policy can be subdivided into two components:</p>
<ul>
<li>An optimal first action A*</li>
<li>Followed by an optimal policy from successor state S0<br>所以，若要满足一个policy pi使的state s的value为optimal value，当且仅当policy pi使得s的后续state s’的value为optimal value<br><img src="/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/FDB650F5-C6EB-4853-B7E8-CBA6B9265F00.png" width="80%" height="80%"><br>所以知道了s’的optimal value便可以倒推出来s的optimal value<br><img src="/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/C57EE2DD-688D-4E34-8C52-267BABA13163.jpg" width="80%" height="80%"><br>所以这便是value iteration的思想所在，迭代重复上面这个公式<br>直观上看来，是从最后一个state开始，倒推到第一个state，下面以一个例子来说明下<br>这个例子的目的是每个state找出到达左上角的最短路径（步数），其他方面类似于之前gridworld的例子，action是向四个方向移动，每移动一步reward减1<br>V1初始时所有的state的value都是0，然后在V2时，每个state从四个action value中选出最高的那个作为自己的state value（在policy iteration中是将四个action value求期望作为自己的state value），以第二行第二个为例，它在V1V2V3的时候四个action value都是一样的，所以没有相对的optimal，但是从V4开始，向上和向左比向下向右好，所以它的state value就一直为-2，拿V6来说，它的state value = max(-2, -2, -4, -4) = -2<br><img src="/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/53F5BD3B-6F51-4940-9D3E-8D244B44DD38.png" width="70%" height="70%"><br>在这个例子中需要注意这么几个问题：</li>
<li>每一次迭代中所有的state都要计算value，只不过有些state每次max的时候都是那些值（先达到了optimal value）所以看起来就不变了</li>
<li>在开始的时候不像policy iteration那样有个明确的policy，而且在迭代过程中state value并不对应于哪个policy，只有在最终迭代完成后（在例子中是V7）才对应于一个policy，而且还是optimal policy</li>
<li>如果以右下角那个state为开始的话，那路径就是-6 -5 -4 -3 -2 -1 0，而计算过程是-1 -2 -3 -4 -5 -6，所以符合上面说的Principle of Optimality的倒推性</li>
</ul>
<p>所以对于value iteration来说<br>需要解决的问题：找到optimal policy pi*<br>解决方案：不断去迭代Bellman optimality Equation<br><img src="/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/16C7EA57-ABD6-4E7F-8533-36BCA94C2A0D.png" width="50%" height="50%"><br>可以证明， <strong>最终value function会收敛到v*</strong><br>采用同步更新的方式，在新的时刻所有的state同时更新<br><img src="/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/D6214822-FC86-4C8C-BF90-429C21F6A836.png" width="70%" height="70%"><br>计算过程：</p>
<table><tr><td><img src="/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/55DFF675-66BA-4CD0-8878-39DBC1809814.png" width="100%" height="100%"></td><td><img src="/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/E887D5E7-5635-4601-A29A-C2D4CEA11F36.png" width="100%" height="100%"></td></tr></table>

<p>总结：value iteration的本质就是在每次迭代中每个state都选择optimal value，那么当所有的state都选择出了optimal value后，所对应的policy就是optimal policy。<br>对比policy iteration，是迭代到收敛后每个state才选择optimal value，然后这时候对应的policy就是improve后的policy pi’</p>
<p>有个value iteration的例子：需要安装java环境<br><a href="http://www.cs.ubc.ca/~poole/demos/mdp/vi.html" target="_blank" rel="external">http://www.cs.ubc.ca/~poole/demos/mdp/vi.html</a></p>
<p>下表是对DP的一个总结<br><img src="/images/2017-04-10-Reinforcement learning part 3 - Planning by Dynamic Programming/016A5E4D-2713-4FA7-BC4A-D15A43A0588E.png" width="80%" height="80%"></p>
<p>5、Extensions to Dynamic Programming<br>Asynchronous Dynamic Programming<br>In-Place Dynamic Programming<br>Prioritised Sweeping<br>Real-Time Dynamic Programming<br>Full-Width Backups<br>Approximate Dynamic Programming</p>
<p>6、Contraction Mapping</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://bluesmilery.github.io/blogs/b96003ba/" data-id="cj1c4pamp0006bcmvf3sc6huy" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/增强学习RL/">增强学习RL</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/blogs/e4dc3fbf/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">增强学习 Reinforcement learning part 2 - Markov Decision Process</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/增强学习RL/">增强学习RL</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/机器学习ML/">机器学习ML</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/增强学习RL/" style="font-size: 20px;">增强学习RL</a> <a href="/tags/机器学习ML/" style="font-size: 10px;">机器学习ML</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">April 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">March 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/blogs/b96003ba/">增强学习 Reinforcement learning part 3 - Planning by Dynamic Programming</a>
          </li>
        
          <li>
            <a href="/blogs/e4dc3fbf/">增强学习 Reinforcement learning part 2 - Markov Decision Process</a>
          </li>
        
          <li>
            <a href="/blogs/18a3f212/">机器学习 Machine learning part 1 - Linear Regression</a>
          </li>
        
          <li>
            <a href="/blogs/481fe3af/">增强学习 Reinforcement learning part 1 - Introduction</a>
          </li>
        
          <li>
            <a href="/blogs/9a018dfc/">机器学习系统环境配置指南 —— GTX 1080 + Ubuntu16.04 + CUDA8 + cuDNN5.1 + TensorFlow</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 Gai<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="http://apps.bdimg.com/libs/jquery/2.0.3/jquery.min.js"></script>
<script type="text/javascript">
//<![CDATA[
if (typeof jQuery == 'undefined') {
  document.write(unescape("%3Cscript src='/js/jquery-2.0.3.min.js' type='text/javascript'%3E%3C/script%3E"));
}
// ]]>
</script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>