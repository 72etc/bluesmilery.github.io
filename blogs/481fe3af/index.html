<!DOCTYPE html>
<html lang="">
  <head><meta name="generator" content="Hexo 3.8.0">
    
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">


<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">

<meta name="theme-color" content="#f8f5ec">
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">



  <meta name="description" content="增强学习 Reinforcement learning part 1 - Introduction">




  <meta name="keywords" content="Reinforcement Learning, Gai's Blog">





  <meta name="google-site-verification" content="SrSETrQ8JjhGYLB-qHgsT23NdViq-VEpvghPoNFdn2g">






  <link rel="alternate" href="/atom.xml" title="Gai's Blog">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=2.9.0">



<link rel="canonical" href="https://bluesmilery.github.io/blogs/481fe3af/">



  <link rel="stylesheet" type="text/css" href="/lib/fancybox/jquery.fancybox.css">




  <link rel="stylesheet" type="text/css" href="/lib/nprogress/nprogress.min.css">



<link rel="stylesheet" type="text/css" href="/css/style.css?v=2.9.0">



  
  <script id="baidu_analytics">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?a3f2fd48827f9ed28f8d9432317653de";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-121307266-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-121307266-1');
</script>


  <script id="baidu_push">
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>





  <script src="//cdn1.lncld.net/static/js/3.1.1/av-min.js"></script>
  <script id="leancloud">
    AV.init({
      appId: "BmUgD1geFKBFQpURVdGNzgl1-gzGzoHsz",
      appKey: "mOzxpdEEenjouuAKMFPazvnd"
    });
  </script>




<script>
  window.config = {"title":"Gai's Blog","subtitle":"A Zone for Knowledge","description":null,"author":"Gai","language":null,"timezone":"Europe/Lisbon","url":"https://bluesmilery.github.io","root":"/","permalink":"blogs/:abbrlink/","permalink_defaults":null,"source_dir":"source","public_dir":"public","tag_dir":"tags","archive_dir":"archives","category_dir":"categories","code_dir":"downloads/code","i18n_dir":":lang","skip_render":null,"new_post_name":":year-:month-:day-:title.md","default_layout":"post","titlecase":false,"external_link":true,"filename_case":0,"render_drafts":false,"post_asset_folder":false,"relative_link":false,"future":true,"highlight":{"enable":true,"auto_detect":false,"line_number":true,"tab_replace":null,"first_line_number":"always1"},"default_category":"uncategorized","category_map":null,"tag_map":null,"date_format":"YYYY-MM-DD","time_format":"HH:mm:ss","per_page":10,"pagination_dir":"page","theme":"even","deploy":[{"type":"git","repo":"git@github.com:bluesmilery/bluesmilery.github.io.git","branch":"master"},{"type":"baidu_url_submitter"}],"ignore":[],"keywords":null,"index_generator":{"per_page":10,"order_by":"-date","path":""},"sitemap":{"path":"sitemap.xml"},"baidusitemap":{"path":"baidusitemap.xml"},"abbrlink":{"alg":"crc32","rep":"hex"},"baidu_url_submit":{"count":1,"host":"bluesmilery.github.io","token":"cktimjuXHORRMXsM","path":"baidu_urls.txt"},"archive_generator":{"per_page":10,"yearly":true,"monthly":true,"daily":false},"category_generator":{"per_page":10},"feed":{"type":"atom","limit":20,"hub":"","content":true,"content_limit":140,"content_limit_delim":"","path":"atom.xml"},"tag_generator":{"per_page":10},"marked":{"gfm":true,"pedantic":false,"sanitize":false,"tables":true,"breaks":true,"smartLists":true,"smartypants":true,"modifyAnchors":"","autolink":true},"server":{"port":4000,"log":false,"compress":false,"header":true},"since":2017,"favicon":"/favicon.ico","rss":"default","menu":{"Home":"/","Archives":"/archives/","Tags":"/tags","Categories":"/categories"},"color":"Default","toc":true,"fancybox":true,"pjax":true,"copyright":{"enable":true,"license":"<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\" target=\"_blank\">知识共享署名-非商业性使用 4.0 国际许可协议</a>"},"reward":{"enable":false,"qrCode":{"wechat":null,"alipay":null}},"social":{"email":"plgaixd92498@gmail.com","stack-overflow":null,"twitter":null,"facebook":null,"linkedin":null,"google":null,"github":"https://github.com/bluesmilery","weibo":null,"zhihu":null,"douban":null,"pocket":null,"tumblr":null,"instagram":null},"leancloud":{"app_id":"BmUgD1geFKBFQpURVdGNzgl1-gzGzoHsz","app_key":"mOzxpdEEenjouuAKMFPazvnd"},"baidu_analytics":"a3f2fd48827f9ed28f8d9432317653de","baidu_verification":null,"google_analytics":"UA-121307266-1","google_verification":"SrSETrQ8JjhGYLB-qHgsT23NdViq-VEpvghPoNFdn2g","disqus_shortname":null,"changyan":{"appid":null,"appkey":null},"livere_datauid":"MTAyMC8zMzY4MC8xMDIzNQ","version":"2.9.0","word_count":true};
</script>

    <title> 增强学习 Reinforcement learning part 1 - Introduction - Gai's Blog </title>
  </head>

  <body><div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/." class="logo">Gai's Blog</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>

<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    
      <a href="/">
        <li class="mobile-menu-item">
          
          
            首页
          
        </li>
      </a>
    
      <a href="/archives/">
        <li class="mobile-menu-item">
          
          
            归档
          
        </li>
      </a>
    
      <a href="/tags">
        <li class="mobile-menu-item">
          
          
            标签
          
        </li>
      </a>
    
      <a href="/categories">
        <li class="mobile-menu-item">
          
          
            分类
          
        </li>
      </a>
    
  </ul>
</nav>

    <div class="container" id="mobile-panel">
      <header id="header" class="header"><div class="logo-wrapper">
  <a href="/." class="logo">Gai's Blog</a>
</div>

<nav class="site-navbar">
  
    <ul id="menu" class="menu">
      
        <li class="menu-item">
          <a class="menu-item-link" href="/">
            
            
              首页
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/archives/">
            
            
              归档
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/tags">
            
            
              标签
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/categories">
            
            
              分类
            
          </a>
        </li>
      
    </ul>
  
</nav>

      </header>

      <main id="main" class="main">
        <div class="content-wrapper">
          <div id="content" class="content">
            
  
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          增强学习 Reinforcement learning part 1 - Introduction
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2017-03-21
        </span>
        
          <div class="post-category">
            
              <a href="/categories/笔记/">笔记</a>
            
          </div>
        
        
        <div class="post-visits" data-url="/blogs/481fe3af/" data-title="增强学习 Reinforcement learning part 1 - Introduction">
            阅读次数 0
          </div>
        
        
        
        
          <span class="post-count">本文共1.5k字</span>
          <span class="post-count">阅读约6分钟</span>
        
      </div>
    </header>

    
    
  <div class="post-toc" id="post-toc">
    <h2 class="post-toc-title">文章目录</h2>
    <div class="post-toc-content">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1、About-Reinforcement-Learning"><span class="toc-text">1、About Reinforcement Learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2、The-Reinforcement-Learning-Problem"><span class="toc-text">2、The Reinforcement Learning Problem</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3、Inside-An-Reinforcement-Learning-Agent"><span class="toc-text">3、Inside An Reinforcement Learning Agent</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4、Problems-within-Reinforcement-Learning"><span class="toc-text">4、Problems within Reinforcement Learning</span></a></li></ol>
    </div>
  </div>



    <div class="post-content">
      
        <p>本文是在学习David Silver所教授的Reinforcement learning课程过程中所记录的笔记。因为个人知识的不足以及全程啃生肉，难免会有理解偏差的地方，欢迎一起交流。</p>
<p>课程资料：<a href="http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html" target="_blank" rel="noopener">http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html</a></p>
<p>关于 Reinforcement learning的两本参考：</p>
<p>An Introduction to Reinforcement Learning</p>
<p><a href="https://webdocs.cs.ualberta.ca/~sutton/book/the-book-1st.html" target="_blank" rel="noopener">https://webdocs.cs.ualberta.ca/~sutton/book/the-book-1st.html</a></p>
<p>Algorithms for Reinforcement Learning</p>
<p><a href="https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf" target="_blank" rel="noopener">https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf</a></p>
<a id="more"></a>
<h2 id="1、About-Reinforcement-Learning"><a href="#1、About-Reinforcement-Learning" class="headerlink" title="1、About Reinforcement Learning"></a>1、About Reinforcement Learning</h2><p>Many Faces of Reinforcement Learning</p>
<p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170321/rl1-1.png" alt=""></p>
<p>Machine Learningd的三个分支：Supervised Learning、Unsupervised Learning、Reinforcement Learning</p>
<p>RL与其他两种的区别：</p>
<ul>
<li>There is no supervisor, only <strong>a reward signal</strong></li>
<li><strong>Feedback is delayed</strong>, not instantaneous</li>
<li><strong>Time</strong> really matters (sequential, non i.i.d data)</li>
<li>Agent’s <strong>actions affect the subsequent data</strong> it receives</li>
</ul>
<h2 id="2、The-Reinforcement-Learning-Problem"><a href="#2、The-Reinforcement-Learning-Problem" class="headerlink" title="2、The Reinforcement Learning Problem"></a>2、The Reinforcement Learning Problem</h2><p>介绍三个概念：reward、environment、state</p>
<p>==<strong>reward</strong>==</p>
<p>用Rt来表示reward（标量，就是个’数’），衡量在第t步agent表现的好坏（收益），agent的目标就是最大化累计reward</p>
<p>Reinforcement learning is based on the <strong>reward hypothesis</strong>（All goals can be described by the maximisation of expected cumulative reward）</p>
<p>简而言之就是假设所有的目标都可以用最大化累计收益来表示</p>
<p>example：</p>
<p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170321/rl1-2.png" alt=""></p>
<p>Sequential Decision Making</p>
<p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170321/rl1-3.png" alt=""></p>
<p>==<strong>environment</strong>==</p>
<p>agent与environment的关系，agent执行action影响environment，environment给agent关于observation和reward的反馈</p>
<p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170321/rl1-4.png" alt=""></p>
<p>==<strong>state</strong>==</p>
<p><em>history</em>：在第t步之前的observation、reward、action。注意没有At，因为agent是基于observation和reward来选择action，在选择action之前的这个时间点，在此之前的都算是过去</p>
<p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170321/rl1-5.png" alt=""></p>
<p><em>state</em>：is the information used to determine what happens next。就是说我利用了history中某些信息来判断接下来会发生什么，所利用的这些信息就被称为state</p>
<p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170321/rl1-6.png" alt=""></p>
<p>这里what happens next分为两部分</p>
<ul>
<li>The agent selects actions。agent会选择什么action</li>
<li>The environment selects observations/rewards。environment会给出什么observation/reward</li>
</ul>
<p>state又分为environment state、agent state、information state</p>
<p><em>environment state</em>：Ste is the environment’s private representation。对于agent而言一般是invisible的，就算是visible，那也包含不相关的信息</p>
<p><em>agent state</em>：Sta is the agent’s internal representation。It can be any function of history</p>
<p><em>information state</em>：又被称为markov state。所以具有’The future is independent of the past given the present’</p>
<p>The environment state and the history are Markov</p>
<p>例子：每一行为一次过程，第一次灯亮灯亮，老鼠按下开关，然后铃响，结果老鼠遭到电击。第二次先铃响灯亮，然后老鼠两次按下开关，结果老鼠得到奶酪。第三次老鼠先按下开关，然后灯亮，然后老鼠又按下开关，然后铃响，猜测老鼠会得到什么？</p>
<p>分析：如果agent state是利用最后三个动作的顺序，那么老鼠会遭到电击。如果agent state是利用灯亮铃响按下开关的次数，那么老鼠会得到奶酪。如果agent state是利用整个序列，那我们也不知道会发生什么</p>
<p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170321/rl1-7.png" alt=""></p>
<p><em>Fully Observable Environments</em>：agent <strong>directly</strong> observes environment state。这种被称为<strong>Markov decision process</strong> (MDP)</p>
<p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170321/rl1-8.png" alt=""></p>
<p>agent state = environment state = information state</p>
<p><em>Partially Observable Environments</em>：agent <strong>indirectly</strong> observes environment。这种被称为<strong>partially observable Markov decision process</strong>（POMDP）</p>
<ul>
<li>A robot with camera vision isn’t told its absolute location</li>
<li>A trading agent only observes current prices</li>
<li>A poker playing agent only observes public cards</li>
</ul>
<p>agent state 不等于 environment state</p>
<p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170321/rl1-9.png" alt=""></p>
<h2 id="3、Inside-An-Reinforcement-Learning-Agent"><a href="#3、Inside-An-Reinforcement-Learning-Agent" class="headerlink" title="3、Inside An Reinforcement Learning Agent"></a>3、Inside An Reinforcement Learning Agent</h2><p>agent的三要素</p>
<ul>
<li>Policy：agent采取的行为策略（behaviour function）</li>
<li>Value Function：评估state/action的好坏</li>
<li>Model：agent对environment所构建的模型（在agent眼中environment的样子）</li>
</ul>
<p>==<strong>Policy</strong>==：agent的策略，也就是agent在某个状态会采取什么样的行动，所以policy is a map from state to action</p>
<p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170321/rl1-10.png" alt=""></p>
<p>==<strong>Value Function</strong>==：是对未来收益的一个预测，用来评估状态的好坏程度</p>
<p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170321/rl1-11.png" alt=""></p>
<p>其中gamma是discounted系数，表示了未来的收益对现在的影响，越远的影响越小。比如gamma是0.9，那么这个预测的时间跨度大约是未来三四十步</p>
<p>example：左上角那个是state value function，游戏画面上有一个紫色的，那个是mothership，击落的分数奖励更高，所以当mothership从右边出现后，对未来收益的预测增加，从而value function的值开始上升。当mothership从眼前过去后，不管打没打中，value function都会陡然下降，因为后面都是小兵，所以对未来收益的预期也就回到了一般水平。</p>
<p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170321/rl1-12.png" alt=""></p>
<p>还有个打砖块的例子，越靠上面的砖块分数越高，所以在游戏刚开始的时候value function比较平滑，当下面的打了好多以后，打到更深的砖块的概率上升，所以value function的波动增加了。</p>
<p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170321/rl1-13.png" alt=""></p>
<p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170321/rl1-14.png" alt=""></p>
<p>==<strong>Model</strong>==：agent对environment构建的模型，用来预测environment下一步会干什么（会跳转到哪个state，会给出什么reward）</p>
<p>P predicts the next state</p>
<p>R predicts the next (immediate) reward</p>
<p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170321/rl1-15.png" alt=""></p>
<p>基于上面三要素，RL agent有以下几种分类</p>
<p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170321/rl1-16.png" alt=""></p>
<h2 id="4、Problems-within-Reinforcement-Learning"><a href="#4、Problems-within-Reinforcement-Learning" class="headerlink" title="4、Problems within Reinforcement Learning"></a>4、Problems within Reinforcement Learning</h2><p>==<strong>Learning and Planning</strong>==</p>
<p>Two fundamental problems in sequential decision making</p>
<ul>
<li>Reinforcement Learning:<ul>
<li><strong>The environment is initially unknown</strong></li>
<li>The agent interacts with the environment</li>
<li>The agent improves its policy</li>
</ul>
</li>
<li>Planning:<ul>
<li><strong>A model of the environment is known</strong></li>
<li>The agent performs computations with its model (without any external interaction)</li>
<li>The agent improves its policy</li>
<li>a.k.a. deliberation, reasoning, introspection, pondering, thought, search</li>
</ul>
</li>
</ul>
<p>Reinforcement Learning的例子：游戏的机制不清楚，只能通过玩来学习，通过观察得分与游戏画面来选择下一步行动</p>
<p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170321/rl1-17.png" alt=""></p>
<p>Planning的例子：游戏机制很清楚，下一步是什么样子的都知道，有完整的策略（就像是玩游戏有攻略一样）</p>
<p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170321/rl1-18.png" alt=""></p>
<p>==<strong>Exploration and Exploitation</strong>==</p>
<p>Reinforcement learning is like trial-and-error learning</p>
<ul>
<li>Exploration：探索，更多的去探索environment的信息</li>
<li>Exploitation：利用，更多的利用已知的environment信息来最大化reward</li>
</ul>
<p>举个例子，吃饭选择餐厅，exploration是选择一个新餐厅，exploitation是选择自己平时最喜欢吃的餐厅</p>
<p>==<strong>Prediction and Control</strong>==</p>
<ul>
<li>Prediction：估计未来的收益，given a policy</li>
<li>Control：最优化未来的收益，find the best policy</li>
</ul>
<p>Gridworld Example，没看懂</p>
<p>后记：有了一些理解，如果移动到A的话那么就会跳转到A’，并且reward +10，如果移动到B的话那么就会跳转到B’，并且reward + 5</p>
<p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170321/rl1-19.png" alt=""></p>
<p><img src="https://tuchuang-1256478313.cos.ap-shanghai.myqcloud.com/20170321/rl1-20.png" alt=""></p>

      
    </div>

    
      
      

  <div class="post-copyright">
    <p class="copyright-item">
      <span>原文作者: </span>
      <a href="https://bluesmilery.github.io">Gai</a>
    </p>
    <p class="copyright-item">
      <span>原文链接: </span>
      <a href="https://bluesmilery.github.io/blogs/481fe3af/">https://bluesmilery.github.io/blogs/481fe3af/</a>
    </p>
    <p class="copyright-item">
      <span>许可协议: </span>
      
      <a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank">知识共享署名-非商业性使用 4.0 国际许可协议</a>
    </p>
  </div>



      
      
    

    
      <footer class="post-footer">
        
          <div class="post-tags">
            
              <a href="/tags/Reinforcement-Learning/">Reinforcement Learning</a>
            
          </div>
        
        
        
  <nav class="post-nav">
    
      <a class="prev" href="/blogs/e4dc3fbf/">
        <i class="iconfont icon-left"></i>
        <span class="prev-text nav-default">增强学习 Reinforcement learning part 2 - Markov Decision Process</span>
        <span class="prev-text nav-mobile">上一篇</span>
      </a>
    
    
      <a class="next" href="/blogs/18a3f212/">
        <span class="next-text nav-default">机器学习 Machine learning part 1 - Linear Regression</span>
        <span class="prev-text nav-mobile">下一篇</span>
        <i class="iconfont icon-right"></i>
      </a>
    
  </nav>

      </footer>
    

  </article>


          </div>
          
  <div class="comments" id="comments">
    
      <div id="lv-container" data-id="city" data-uid="MTAyMC8zMzY4MC8xMDIzNQ">
        <noscript>为正常使用来必力评论功能请激活JavaScript</noscript>
      </div>  
    
  </div>


        </div>
      </main>

      <footer id="footer" class="footer"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>


  <div class="social-links">
    
      
        
          <a href="mailto:plgaixd92498@gmail.com" class="iconfont icon-email" title="email"></a>
        
      
    
      
    
      
    
      
    
      
    
      
    
      
        
          <a href="https://github.com/bluesmilery" class="iconfont icon-github" title="github"></a>
        
      
    
      
    
      
    
      
    
      
    
      
    
      
    
    
    
      <a href="/atom.xml" class="iconfont icon-rss" title="rss"></a>
    
  </div>


<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://hexo.io/">Hexo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/ahonn/hexo-theme-even">Even</a>
  </span>

  
  <br>
  <span id="busuanzi_container_site_uv">
    被<span id="busuanzi_value_site_uv"></span>个小伙伴
  </span>
  <span id="busuanzi_container_site_pv">
    查看了<span id="busuanzi_value_site_pv"></span>次~
  </span>

  <span class="copyright-year">
    
    &copy; 
     
      2017 - 
    
    2018

    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">Gai</span>
  </span>
</div>

      </footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div>

    
  
  

  
   <script type="text/javascript">
	(function(d, s) {
       var j, e = d.getElementsByTagName(s)[0];

       if (typeof LivereTower === 'function') { return; }

       j = d.createElement(s);
       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
       j.async = true;

       e.parentNode.insertBefore(j, e);
   })(document, 'script');
  </script>




    
  



  
  





  
    <script type="text/javascript" src="/lib/jquery/jquery.min.js"></script>
  

  
    <script type="text/javascript" src="/lib/slideout/slideout.js"></script>
  

  
    <script type="text/javascript" src="/lib/fancybox/jquery.fancybox.pack.js"></script>
  

  
    <script type="text/javascript" src="/lib/pjax/jquery.pjax.min.js"></script>
  

  
    <script type="text/javascript" src="/lib/nprogress/nprogress.min.js"></script>
  


    <script type="text/javascript" src="/js/src/even.js?v=2.9.0"></script>

  </body>
</html>
